<!DOCTYPE html>
<html lang="en-US">

<head>

    <meta charset="UTF-8">
    <link rel="icon" type="image/png"  href="/img/favicon.jinja.png">
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>MobileNet Note | Yonji’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="MobileNet Note" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MobileNet Note MobileNet 是 Google 2017年的一个工作，提出了一个轻量级的神经网络模型架构。 arXiv链接：https://arxiv.org/abs/1704.04861 MobileNet 的主要工作可以概况为以下三个部分： 深度可分离卷积，将一层卷积分解为两层计算，以更少的参数和计算量产生同样格式的输出 引入超参 $\alpha$ ，减少模型中每一层的通道数 引入超参 $\rho$，缩小原始图片尺寸，则整个模型中产生的 feature map size 都会缩小 1 深度可分离卷积（depthwise separable convolution） 假设有一个卷积核 kernel_size = $F\times F$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$ 的卷积层 conv。 那么深度可分离卷积以如下两个卷积层来代替原始卷积。 第一个卷积层conv1： kernel_size = $F\times F$，输入通道数=输出通道数 = $C_{in}$，以分组卷积的形式产生 $C_{in}$ 个 feature map，分组数也为 $C_{in}$ ，即输入的每个通道单独做卷积； 第二个卷积层conv2： kernel_size = $1\times1$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$，将上一层得到的 $C_{in}$ 个 feature map 再 combine 为 $C_{out}$ 个。 1. 参数减少量 before: after: 则参数量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。 2. FLOPs 减少量 假设卷积层输出的 feature map 的size 为 $W_{out}\times H_{out}$，那么卷积层所需要的计算量 FLOPs 分别如下。 before: after: 则计算量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。 2 整体架构 网络层数 MobileNet 由1个普通的卷积层+13个可分离卷积层+1个全连接层构成，因此若将可分离卷积中的每一层都作为独立的一层，则MobileNet共有28层。 MobileNet 95%的计算量都用在 $1\times1$ 卷积上，卷积的本质是矩阵乘法，而 $1\times1$ 卷积的矩阵乘法是极其稀疏的，通过高度优化的GEMM来实现可以加速计算。 两个超参 取 $\alpha\in {1,0.75,0.5,0.25},\rho\in{224,192,160,128}$，在 ImageNet 上模型精度下降如下图所示。 训练细节 因为深度可分离卷积对比原始卷积已经大幅度减少了参数数量，极大地缓和了过拟合问题，所以不再使用L2正则项（即weight decay）。" />
<meta property="og:description" content="MobileNet Note MobileNet 是 Google 2017年的一个工作，提出了一个轻量级的神经网络模型架构。 arXiv链接：https://arxiv.org/abs/1704.04861 MobileNet 的主要工作可以概况为以下三个部分： 深度可分离卷积，将一层卷积分解为两层计算，以更少的参数和计算量产生同样格式的输出 引入超参 $\alpha$ ，减少模型中每一层的通道数 引入超参 $\rho$，缩小原始图片尺寸，则整个模型中产生的 feature map size 都会缩小 1 深度可分离卷积（depthwise separable convolution） 假设有一个卷积核 kernel_size = $F\times F$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$ 的卷积层 conv。 那么深度可分离卷积以如下两个卷积层来代替原始卷积。 第一个卷积层conv1： kernel_size = $F\times F$，输入通道数=输出通道数 = $C_{in}$，以分组卷积的形式产生 $C_{in}$ 个 feature map，分组数也为 $C_{in}$ ，即输入的每个通道单独做卷积； 第二个卷积层conv2： kernel_size = $1\times1$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$，将上一层得到的 $C_{in}$ 个 feature map 再 combine 为 $C_{out}$ 个。 1. 参数减少量 before: after: 则参数量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。 2. FLOPs 减少量 假设卷积层输出的 feature map 的size 为 $W_{out}\times H_{out}$，那么卷积层所需要的计算量 FLOPs 分别如下。 before: after: 则计算量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。 2 整体架构 网络层数 MobileNet 由1个普通的卷积层+13个可分离卷积层+1个全连接层构成，因此若将可分离卷积中的每一层都作为独立的一层，则MobileNet共有28层。 MobileNet 95%的计算量都用在 $1\times1$ 卷积上，卷积的本质是矩阵乘法，而 $1\times1$ 卷积的矩阵乘法是极其稀疏的，通过高度优化的GEMM来实现可以加速计算。 两个超参 取 $\alpha\in {1,0.75,0.5,0.25},\rho\in{224,192,160,128}$，在 ImageNet 上模型精度下降如下图所示。 训练细节 因为深度可分离卷积对比原始卷积已经大幅度减少了参数数量，极大地缓和了过拟合问题，所以不再使用L2正则项（即weight decay）。" />
<link rel="canonical" href="http://localhost:4000/2019/05/17/MobileNet.html" />
<meta property="og:url" content="http://localhost:4000/2019/05/17/MobileNet.html" />
<meta property="og:site_name" content="Yonji’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-17T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"MobileNet Note MobileNet 是 Google 2017年的一个工作，提出了一个轻量级的神经网络模型架构。 arXiv链接：https://arxiv.org/abs/1704.04861 MobileNet 的主要工作可以概况为以下三个部分： 深度可分离卷积，将一层卷积分解为两层计算，以更少的参数和计算量产生同样格式的输出 引入超参 $\\alpha$ ，减少模型中每一层的通道数 引入超参 $\\rho$，缩小原始图片尺寸，则整个模型中产生的 feature map size 都会缩小 1 深度可分离卷积（depthwise separable convolution） 假设有一个卷积核 kernel_size = $F\\times F$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$ 的卷积层 conv。 那么深度可分离卷积以如下两个卷积层来代替原始卷积。 第一个卷积层conv1： kernel_size = $F\\times F$，输入通道数=输出通道数 = $C_{in}$，以分组卷积的形式产生 $C_{in}$ 个 feature map，分组数也为 $C_{in}$ ，即输入的每个通道单独做卷积； 第二个卷积层conv2： kernel_size = $1\\times1$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$，将上一层得到的 $C_{in}$ 个 feature map 再 combine 为 $C_{out}$ 个。 1. 参数减少量 before: after: 则参数量减少为原来的 $\\cfrac{1}{C_{out}}+\\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\\cfrac{1}{9}$。 2. FLOPs 减少量 假设卷积层输出的 feature map 的size 为 $W_{out}\\times H_{out}$，那么卷积层所需要的计算量 FLOPs 分别如下。 before: after: 则计算量减少为原来的 $\\cfrac{1}{C_{out}}+\\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\\cfrac{1}{9}$。 2 整体架构 网络层数 MobileNet 由1个普通的卷积层+13个可分离卷积层+1个全连接层构成，因此若将可分离卷积中的每一层都作为独立的一层，则MobileNet共有28层。 MobileNet 95%的计算量都用在 $1\\times1$ 卷积上，卷积的本质是矩阵乘法，而 $1\\times1$ 卷积的矩阵乘法是极其稀疏的，通过高度优化的GEMM来实现可以加速计算。 两个超参 取 $\\alpha\\in {1,0.75,0.5,0.25},\\rho\\in{224,192,160,128}$，在 ImageNet 上模型精度下降如下图所示。 训练细节 因为深度可分离卷积对比原始卷积已经大幅度减少了参数数量，极大地缓和了过拟合问题，所以不再使用L2正则项（即weight decay）。","@type":"BlogPosting","url":"http://localhost:4000/2019/05/17/MobileNet.html","headline":"MobileNet Note","dateModified":"2019-05-17T00:00:00-07:00","datePublished":"2019-05-17T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/05/17/MobileNet.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=b0ca346b5b38b55e1001eb0920382c9d2e5aa454">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
</head>


  <body>
    <section class="page-header">
      <h1 class="project-name">Yonji's Blog|
      <a href="http://localhost:4000">Home</a></h1>
      <!--
      <h2 class="project-tagline">OMG! They killed Kenny!</h2>
      -->
      <a href="https://github.com/Amoko/amoko.github.io" class="btn">View on GitHub</a>
    
      
    </section>

    <section class="main-content">
      <h1 id="mobilenet-note">MobileNet Note</h1>

<p>MobileNet 是 Google 2017年的一个工作，提出了一个轻量级的神经网络模型架构。</p>

<p>arXiv链接：<a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></p>

<p>MobileNet 的主要工作可以概况为以下三个部分：</p>

<ul>
  <li>深度可分离卷积，将一层卷积分解为两层计算，以更少的参数和计算量产生同样格式的输出</li>
  <li>引入超参 $\alpha$ ，减少模型中每一层的通道数</li>
  <li>引入超参 $\rho$，缩小原始图片尺寸，则整个模型中产生的 feature map size 都会缩小</li>
</ul>

<h2 id="1-深度可分离卷积depthwise-separable-convolution">1 深度可分离卷积（depthwise separable convolution）</h2>

<p>假设有一个卷积核 kernel_size = $F\times F$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$ 的卷积层 conv。</p>

<p>那么深度可分离卷积以如下两个卷积层来代替原始卷积。</p>

<p><strong>第一个卷积层conv1</strong>：</p>

<p>kernel_size = $F\times F$，输入通道数=输出通道数 = $C_{in}$，以<strong>分组卷积</strong>的形式产生 $C_{in}$ 个 feature map，分组数也为 $C_{in}$ ，即输入的每个通道单独做卷积；</p>

<p><strong>第二个卷积层conv2</strong>：</p>

<p>kernel_size = $1\times1$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$，将上一层得到的 $C_{in}$ 个 feature map 再 combine 为 $C_{out}$ 个。</p>

<p><img src="/img/dw_conv.jpg" alt="此处应有图" /></p>

<p><em>1. 参数减少量</em></p>

<p>before:</p>

<script type="math/tex; mode=display">F\times F\times C_{in}\times C_{out}</script>

<p>after:</p>

<script type="math/tex; mode=display">F\times F\times C_{in}+ C_{in}\times C_{out}</script>

<p>则参数量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。</p>

<p><em>2. FLOPs 减少量</em></p>

<p>假设卷积层输出的 feature map 的size 为 $W_{out}\times H_{out}$，那么卷积层所需要的计算量 FLOPs 分别如下。</p>

<p>before:</p>

<script type="math/tex; mode=display">FLOPs_{conv}=2F^2\times W_{out}\times H_{out} \times C_{in}\times C_{out}</script>

<p>after:</p>

<script type="math/tex; mode=display">\begin{aligned}
FLOPs_{conv1+2}=(2F^2\times W_{out}\times H_{out} \times C_{in}\times 1) +
(2\times W_{out}\times H_{out} \times C_{in}\times C_{out})
\end{aligned}</script>

<p>则计算量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。</p>

<h2 id="2-整体架构">2 整体架构</h2>

<p><strong>网络层数</strong></p>

<p><img src="/img/dw_conv_block.PNG" alt="" /></p>

<p>MobileNet 由1个普通的卷积层+13个可分离卷积层+1个全连接层构成，因此若将可分离卷积中的每一层都作为独立的一层，则MobileNet共有28层。</p>

<p>MobileNet 95%的计算量都用在 $1\times1$ 卷积上，卷积的本质是矩阵乘法，而 $1\times1$ 卷积的矩阵乘法是极其稀疏的，通过高度优化的GEMM来实现可以加速计算。</p>

<p><strong>两个超参</strong></p>

<p>取 $\alpha\in {1,0.75,0.5,0.25},\rho\in{224,192,160,128}$，在 ImageNet 上模型精度下降如下图所示。</p>

<p><img src="/img/dw_hyper.png" alt="" /></p>

<p><strong>训练细节</strong></p>

<p>因为深度可分离卷积对比原始卷积已经大幅度减少了参数数量，极大地缓和了过拟合问题，所以不再使用L2正则项（即weight decay）。</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Amoko/amoko.github.io">amoko.github.io</a> is maintained by <a href="https://github.com/Amoko">Amoko</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
