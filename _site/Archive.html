<!DOCTYPE html>
<html lang="en-US">

<head>

    <meta charset="UTF-8">
    <link rel="icon" type="image/png"  href="/img/favicon.jinja.png">
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Archive | Yonji’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Archive" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="OMG! They killed Kenny!" />
<meta property="og:description" content="OMG! They killed Kenny!" />
<link rel="canonical" href="http://localhost:4000/Archive.html" />
<meta property="og:url" content="http://localhost:4000/Archive.html" />
<meta property="og:site_name" content="Yonji’s Blog" />
<script type="application/ld+json">
{"description":"OMG! They killed Kenny!","@type":"WebPage","url":"http://localhost:4000/Archive.html","headline":"Archive","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=b0ca346b5b38b55e1001eb0920382c9d2e5aa454">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
</head>


  <body>
    <section class="page-header">
      <h1 class="project-name">Yonji's Blog|
      <a href="http://localhost:4000">Home</a></h1>
      <!--
      <h2 class="project-tagline">OMG! They killed Kenny!</h2>
      -->
      <a href="https://github.com/Amoko/amoko.github.io" class="btn">View on GitHub</a>
    
      
    </section>

    <section class="main-content">
      <div class="posts">
  
    <article class="post">
      <h2><a href="/2019/05/17/MobileNet.html">MobileNet Note</a></h2>
      <!--
      <p><h1 id="mobilenet-note">MobileNet Note</h1>

<p>MobileNet 是 Google 2017年的一个工作，提出了一个轻量级的神经网络模型架构。</p>

<p>arXiv链接：<a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></p>

<p>MobileNet 的主要工作可以概况为以下三个部分：</p>

<ul>
  <li>深度可分离卷积，将一层卷积分解为两层计算，以更少的参数和计算量产生同样格式的输出</li>
  <li>引入超参 $\alpha$ ，减少模型中每一层的通道数</li>
  <li>引入超参 $\rho$，缩小原始图片尺寸，则整个模型中产生的 feature map size 都会缩小</li>
</ul>

<h2 id="1-深度可分离卷积depthwise-separable-convolution">1 深度可分离卷积（depthwise separable convolution）</h2>

<p>假设有一个卷积核 kernel_size = $F\times F$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$ 的卷积层 conv。</p>

<p>那么深度可分离卷积以如下两个卷积层来代替原始卷积。</p>

<p><strong>第一个卷积层conv1</strong>：</p>

<p>kernel_size = $F\times F$，输入通道数=输出通道数 = $C_{in}$，以<strong>分组卷积</strong>的形式产生 $C_{in}$ 个 feature map，分组数也为 $C_{in}$ ，即输入的每个通道单独做卷积；</p>

<p><strong>第二个卷积层conv2</strong>：</p>

<p>kernel_size = $1\times1$ ，输入通道数 = $C_{in}$ ，输出通道数 = $C_{out}$，将上一层得到的 $C_{in}$ 个 feature map 再 combine 为 $C_{out}$ 个。</p>

<p><img src="/img/dw_conv.jpg" alt="此处应有图" /></p>

<p><em>1. 参数减少量</em></p>

<p>before:</p>

<script type="math/tex; mode=display">F\times F\times C_{in}\times C_{out}</script>

<p>after:</p>

<script type="math/tex; mode=display">F\times F\times C_{in}+ C_{in}\times C_{out}</script>

<p>则参数量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。</p>

<p><em>2. FLOPs 减少量</em></p>

<p>假设卷积层输出的 feature map 的size 为 $W_{out}\times H_{out}$，那么卷积层所需要的计算量 FLOPs 分别如下。</p>

<p>before:</p>

<script type="math/tex; mode=display">FLOPs_{conv}=2F^2\times W_{out}\times H_{out} \times C_{in}\times C_{out}</script>

<p>after:</p>

<script type="math/tex; mode=display">\begin{aligned}
FLOPs_{conv1+2}=(2F^2\times W_{out}\times H_{out} \times C_{in}\times 1) +
(2\times W_{out}\times H_{out} \times C_{in}\times C_{out})
\end{aligned}</script>

<p>则计算量减少为原来的 $\cfrac{1}{C_{out}}+\cfrac{1}{F^2}$，若 $F=3$，则参数量减少为原来的 $\cfrac{1}{9}$。</p>

<h2 id="2-整体架构">2 整体架构</h2>

<p><strong>网络层数</strong></p>

<p><img src="/img/dw_conv_block.PNG" alt="" /></p>

<p>MobileNet 由1个普通的卷积层+13个可分离卷积层+1个全连接层构成，因此若将可分离卷积中的每一层都作为独立的一层，则MobileNet共有28层。</p>

<p>MobileNet 95%的计算量都用在 $1\times1$ 卷积上，卷积的本质是矩阵乘法，而 $1\times1$ 卷积的矩阵乘法是极其稀疏的，通过高度优化的GEMM来实现可以加速计算。</p>

<p><strong>两个超参</strong></p>

<p>取 $\alpha\in {1,0.75,0.5,0.25},\rho\in{224,192,160,128}$，在 ImageNet 上模型精度下降如下图所示。</p>

<p><img src="/img/dw_hyper.png" alt="" /></p>

<p><strong>训练细节</strong></p>

<p>因为深度可分离卷积对比原始卷积已经大幅度减少了参数数量，极大地缓和了过拟合问题，所以不再使用L2正则项（即weight decay）。</p>
</p>
      -->
      <span class="post-meta">May 17, 2019</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2019/04/24/Interpolation.html">Image Interpolation</a></h2>
      <!--
      <p><h1 id="image-interpolation">Image Interpolation</h1>

<h2 id="0-为什么需要插值interpolation">0 为什么需要插值（Interpolation）？</h2>

<p>一个简单的例子，若将一张图像绕原点旋转30度，旋转后的图像上点 $(1,1)$ 对应的原图像点 $(x,y)$ 是哪一个呢？</p>

<p>原坐标点 $(x,y)$ 可以通过计算出仿射变换矩阵后，解线性方程组求得。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
0.866 & 0.5 & 0\\
-0.5 & 0.866 & 0
\end{bmatrix}\cdot
\begin{bmatrix}
x\\y\\1
\end{bmatrix}=
\begin{bmatrix}
1\\1
\end{bmatrix} %]]></script>

<p>最终求得点 $x=0.366, y=1.366$ 。</p>

<p><img src="/img/30ang.png" alt="" /></p>

<p>然而图像坐标是离散的，点 $(0.366,1.366)$ 在原图像上并不存在，新坐标点 $(1,1)$ 也就无法找到在原图像上对应的坐标点。</p>

<p>在对图像进行缩放（resize）、旋转（rotation）等操作时常常出现这种情况。</p>

<p>此时需要根据其周围的点对未知点进行像素插值，常见的插值方法有以下几种。</p>

<h2 id="1-传统插值方法">1 传统插值方法</h2>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">resize</span><span class="p">(</span><span class="n">InputArray</span> <span class="n">src</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">dst</span><span class="p">,</span> <span class="n">Size</span> <span class="n">dsize</span><span class="p">,</span> <span class="kt">double</span> <span class="n">fx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="kt">double</span> <span class="n">fy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="kt">int</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">INTER_LINEAR</span><span class="p">)</span>
</code></pre></div></div>

<p>OpenCV中提供了五种插值（interpolation）方法。</p>

<ul>
  <li>INTER_NEAREST - 最近邻插值</li>
  <li>INTER_LINEAR - 双线性插值（<strong>缺省值</strong>）</li>
  <li>INTER_CUBIC - 双立方插值</li>
  <li>INTER_AREA - 区域插值</li>
  <li>INTER_LANCZOS4 - a Lanczos interpolation over 8x8 pixel neighborhood</li>
</ul>

<p>其中最常用的插值算法是最近邻、双线性、双立方三种，其原理如下图所示。</p>

<p><img src="/img/Comparison_of_1D_and_2D_interpolation.svg.png" alt="" /></p>

<h3 id="11-最近邻插值">1.1 最近邻插值</h3>

<p>以待插值点 $(x,y)$ 周围 $2\times2$ 区域内的4个点为依据进行插值。</p>

<p>最近邻插值不产生新的像素值，而是将与其欧式距离最近的点的像素直接赋值给点 $(x,y)$。</p>

<h3 id="12-双线性插值">1.2 双线性插值</h3>

<p>以待插值点 $(x,y)$ 周围 $2\times2$ 区域内的4个点为依据进行插值。</p>

<p>双线性插值是一种基于相对距离的加权平均。</p>

<p>假设待插值点 $(x,y)$ 周围4个点像素值分别为 $f(x_0,y_0),f(x_0,y_1),f(x_1,y_0),f(x_1,y_1)$，则先进行 $x$ 方向上插值，再进行 $y$ 方向插值，最终映射关系如下：</p>

<script type="math/tex; mode=display">f(x,y_0) = \cfrac{x_1-x}{x_1-x_0}f(x_0,y_0)+\cfrac{x-x_0}{x_1-x_0}f(x_1,y_0)\\
f(x,y_1) = \cfrac{x_1-x}{x_1-x_0}f(x_0,y_1)+\cfrac{x-x_0}{x_1-x_0}f(x_1,y_1)\\
f(x,y) = \cfrac{y_1-y}{y_1-y_0}f(x,y_0)+\cfrac{y-y_0}{y_1-y_0}f(x,y_1)</script>

<p>双线性插值与最近邻插值相比虽然计算量有所增加但插值效果更加平滑，因而是OpenCV中的默认插值方法。</p>

<h3 id="13-双立方插值">1.3 双立方插值</h3>

<p>以待插值点 $(x,y)$ 周围 $4\times4$ 区域内的16个点为依据进行插值。</p>

<p>利用了更多的像素点并采用更复杂的插值函数，所以效果会比双线性插值更平滑，计算代价也更大。</p>

<h2 id="2-转置卷积">2 转置卷积</h2>

<p><em>以上传统插值算法所使用的都是人工定义好的映射关系。</em></p>

<p><em>当然我们也能以机器学习的方式从数据中学习得到自适应的映射关系。</em></p>

<p><em>处理图像语义分割任务的 FCN 模型便使用了转置卷积（transposed conv）, 也有人称之为 deconv，来对图像进行上采样（upsampling），即图像放大。</em></p>

<p>在介绍转置卷积原理之前，我们首先以矩阵乘法的形式再来认识一下卷积（convolution）。</p>

<p><img src="/img/deconv.PNG" alt="" /></p>

<p><strong>定义卷积核尺寸 kernel_size=2，步长 stride=1，padding=0</strong>，对尺寸为 $3\times3$ 的图像 $X$ 进行卷积可以得到尺寸为 $2\times2$ 的图像 $Y$。将二维图像展开到一维，则卷积可以用矩阵乘法表示如下，矩阵 $W$ 即代表上述卷积操作：</p>

<script type="math/tex; mode=display">% <![CDATA[
W \cdot X =
\begin{bmatrix}
w_1 & w_2 & 0 & w_3 & w_4 & 0 & 0 & 0 & 0\\
0 & w_1 & w_2 & 0 & w_3 & w_4 & 0 & 0 & 0\\
0 & 0 & 0 & w_1 & w_2 & 0 & w_3 & w_4 & 0\\
0 & 0 & 0 & 0 & w_1 & w_2 & 0 & w_3 & w_4
\end{bmatrix}\cdot
\begin{bmatrix}
x_1\\x_2\\x_3\\
x_4\\x_5\\x_6\\
x_7\\x_8\\x_9
\end{bmatrix}=
\begin{bmatrix}
y_1\\y_2\\
y_3\\y_4
\end{bmatrix}
= Y\tag{1} %]]></script>

<p><strong>定义卷积核尺寸 kernel_size=2，步长 stride=1，padding=2</strong>，对图像 $Y$ 进行卷积操作可以得到与原图像 $X$ 尺寸相同的图像 $X’$。同样将二维图像展开到一维，则矩阵 $W’$ 代表上述卷积操作：</p>

<script type="math/tex; mode=display">% <![CDATA[
W' \cdot Y =
\begin{bmatrix}
w'_4 & 0 & 0 & 0\\
w'_3 & w'_4 & 0 & 0\\
0 & w'_3 & 0 & 0\\
w'_2 & 0 & w'_4 & 0\\
w'_1 & w'_2 & w'_3 & w'_4\\
0 & w'_1 & 0 & w'_3\\
0 & 0 & w'_2 & 0\\
0 & 0 & w'_1 & w'_2\\
0 & 0 & 0 & w'_1\\
\end{bmatrix}\cdot
\begin{bmatrix}
y_1\\y_2\\
y_3\\y_4
\end{bmatrix}=
\begin{bmatrix}
x'_1\\x'_2\\x'_3\\
x'_4\\x'_5\\x'_6\\
x'_7\\x'_8\\x'_9
\end{bmatrix}
= X'\tag{2} %]]></script>

<p>可以观察到，矩阵 $W$ 和矩阵 $W’$ 在形状上互为转置矩阵。</p>

<p><strong>转置卷积</strong>，即指对原图像进行卷积得到的小尺寸的 feature map <strong>再卷积上采样</strong>到原图像尺寸。</p>

<p>注意，矩阵 $W$ 和矩阵 $W’$ 参数并无对应关系。</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation">Wikipedia Bilinear interpolation</a></p>

<p>[2] <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=resize#resize">OpenCV Image Processing resize</a></p>

<p>[3] <a href="https://www.kancloud.cn/trent/imagesharp/100477">几种插值算法对比研究 - Trent1985</a></p>

<p>[4] <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">Fully convolutional networks for semantic segmentation. CVPR 2015</a></p>
</p>
      -->
      <span class="post-meta">Apr 24, 2019</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2019/04/12/BN.html">How Batch Normalization works?</a></h2>
      <!--
      <p><h1 id="how-batch-normalization-works">How Batch Normalization works?</h1>

<p>机器学习根据训练集得到模型在测试集上的结果通常会出现一定程度的下降，下降的多少取决于你对训练集的过拟合程度。</p>

<p>这是因为训练集的数据分布 $p(x)​$ 与测试集数据分布 $q(x)​$ 并不完全相同，那么根据训练集得到的参数模型 $p(y\vert x,\theta)​$ 与在测试集上所期望的的模型 $q(y\vert x)​$ 也会存在差异。对训练集进行过多的拟合导致这种差异性被无限放大。</p>

<p>这种由于数据集的分布差异 $p(x)\neq q(x)$，而造成的参数模型差异 $p(y\vert x,\theta) \neq q(y\vert x)$ 被 <ruby>下平<rt>しもだいら</rt></ruby> <ruby>英寿<rt>ひでとし</rt></ruby> 定义为 covariate shift，协变量偏移，并使用KL散度来衡量这种分布差异。</p>

<p>如何最大限度地消除不同数据集之间的差异是 domain adaption 所要解决的问题。</p>

<p>$\star$ 然而，covariate shift 不只是模型面对不同数据集时会出现。</p>

<p>对于多层神经网络，在训练阶段参数优化时，前层网络的参数更新会造成的后层网络的输入分布变化。</p>

<p>Batch Normalization 这篇文章将训练神经网络时所出现的这种，不同网络层之间数据分布差异定义为 internal covariate shift。</p>

<h2 id="1-作用机制">1 作用机制</h2>

<h3 id="11-normalization">1.1 Normalization</h3>

<p>如何处理不同网络层对数据改变所造成的 internal covariate shift？</p>

<p>答案是对数据做归一化（normalization），归一化即 scale + shift 两个操作的组合。</p>

<p>一般常用的Z-score归一化如下式：</p>

<script type="math/tex; mode=display">\hat{x}=\cfrac{x-\mu}{\sigma}</script>

<p>之所以称为Batch Normalization，是因为此处的<strong>均值 $\mu$ 与标准差 $\sigma$ 是在每次迭代时当前batch的数据上求得</strong>。</p>

<p>归一化可以减小 internal covariate shift 的影响，而且可以避免输入数据 $x$ 进入饱和区（saturated regime）从而加速模型的收敛。</p>

<p>但这样做的结果是抹去了前层网络所学习到的信息，即原始输入 $x$ 是上一层网络激活函数的输出，对其归一化后的 $\hat{x}$ 消除了上一层网络的激活作用。</p>

<p>因此为了保证归一化操作不会消除前层网络对数据的非线性变换，BN在归一化的基础上定义了两个可学习参数：再缩放参数 $\gamma$ 与再平移参数 $\beta$，来保证对 $x$ 的变换是恒等变换（identity transform）。</p>

<p>所以BN对原始数据 $x$ 的改变如下式：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
y&=\gamma \hat{x}+\beta\\
&=\gamma \cfrac{x-\mu}{\sigma}+\beta
\end{aligned} %]]></script>

<p>所以BN引入这样一来一回、两个相互抵消的操作优点在哪里呢？</p>

<p><a href="https://www.zhihu.com/people/junezth">Juliuszh</a> 的解释是，BN这样解除了不同网络层之间的复杂关联，而引入的两个新参数与模型中的其他参数一样可以通过梯度下降求解，从简化了神经网络的训练。</p>

<h3 id="12-regularization">1.2 Regularization</h3>

<p>与SGD求梯度时使用子样本代替全体样本引入随机噪声类似，BN使用mini-batch代替全体样本求均值 $\mu$ 与方差 $\sigma$ 同样引入了随机噪声，一定程度上也对模型起到了正则化作用。从而可以舍弃掉为了防止过拟合而采用的dropout操作。</p>

<h2 id="2-参数更新">2 参数更新</h2>

<p>BN所引入的两个可学习参数 $\gamma$ 与 $\beta$ 求解方式与神经网络中的普通参数相同，所以只需要求梯度。</p>

<p>那么通过链式法则就可以推出 $\gamma$ 与 $\beta$ 相对于损失函数 $l$ 的梯度。</p>

<script type="math/tex; mode=display">\cfrac{\partial l}{\partial \gamma}=\cfrac{\partial l}{\partial y}\cdot \hat{x}\\

\cfrac{\partial l}{\partial \beta}=\cfrac{\partial l}{\partial y}​$</script>

<h2 id="3-tbc">3 TBC</h2>

<p>これで今夜もくつろいで熟睡できるな。</p>

<h2 id="4-reference">4 Reference</h2>

<p>[1] Improving predictive inference under covariate shift by weighting the log-likelihood function, JSPI2000</p>

<p>[2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv2015</p>

<p>[3] <a href="https://zhuanlan.zhihu.com/p/33173246">详解深度学习中的Normalization，BN/LN/WN</a></p>
</p>
      -->
      <span class="post-meta">Apr 12, 2019</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2019/02/19/ArcFace.html">Softmax loss から、ArcFace まで</a></h2>
      <!--
      <p><h1 id="softmax-loss-からarcface-まで">Softmax loss から、ArcFace まで</h1>

<h2 id="1-背景">1 背景</h2>

<p>人脸识别，作为图像识别的一个子领域，本质上还是一个多分类任务。</p>

<p>神经网络中处理多分类任务（$n​$类）的模式是固定的：</p>

<ol>
  <li>使用多层神经网络对输入进行特征提取，最终得到一个 $n​$ 维向量 $\vec{z}​$ ；</li>
  <li>使用Softmax函数对 $n​$ 维向量进行归一化 $\vec{z} \rightarrow \vec{a}​$ ，将 $\vec{a}​$ 与对应的label求得交叉熵损失；</li>
  <li>将交叉熵损失BP回去更新网络参数、优化所提取到的分类特征。</li>
</ol>

<p>在这个固定模式下，可以在三个方向上进行优化：数据、网络结构、损失函数。</p>

<p><strong>本篇涉及到的所有文章都是对softmax损失函数的改进。</strong></p>

<p><img src="/img/the_last_fc_layer1.jpg" alt="" /></p>

<h3 id="11-softmax交叉熵损失">1.1 Softmax+交叉熵损失</h3>

<p>对 $n$ 维向量进行归一化 $\vec{z} \rightarrow \vec{a}$ ，Softmax函数形式如下：</p>

<script type="math/tex; mode=display">a_i(\vec{z})=\cfrac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}，{\rm for}\quad i=1,\dots,n</script>

<p>而交叉熵损失（CrossEntropyLoss）定义如下：</p>

<script type="math/tex; mode=display">L=-\sum_i^n y_ilna_i</script>

<p>$\vec{y}$ 向量为输入样本对应label的one-hot编码，所以 $\vec{y}$ 只有一个维度的值为1，其他维度的值皆为0。</p>

<p>那么假设 $y_i=1$，将Softmax函数带入后的交叉熵损失形式如下：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
L_s &= -lna_i \\
&=-ln\cfrac{e^{z_i}}{\sum^n_{j=1} e^{z_j}}
\end{aligned} %]]></script>

<p>我们的目标是 $L_s=0$，即让 $a_i=1$，那么最终的优化目标则是让 $z_i$ 取值尽可能的大。</p>

<h3 id="12-类内距离损失">1.2 类内距离损失</h3>

<p>这个方向上的开坑之作，Center loss (ECCV2016)，首先对基于Softmax损失函数得到的特征进行了可视化。</p>

<p>这里可视化的特征是<strong>倒数第二层神经元</strong>，即最后一层全连接的输入 $\vec{x}$ 。</p>

<p>将最后一层全连接用线性变换表示，$z_i=W^T_i \cdot x$，那么Softmax损失函数可以改写成如下形式：</p>

<script type="math/tex; mode=display">L_S=-ln\cfrac{e^{W^T_ix}}{\sum^n_{j=1} e^{W^T_jx}}</script>

<p><img src="/img/the_last_fc_layer2.jpg" alt="" /></p>

<p>下图是论文中给出的可视化结果，将倒数第二层神经元个数置为2，得到可视化的二维特征。</p>

<p>我的复现结果也与之类似。</p>

<p><img src="/img/softmax_loss.PNG" alt="" /></p>

<p>观察Softmax损失函数得到的特征，每一类的类内距离明显是可以减小的。</p>

<p>所以作者在Softmax损失函数的基础上，增加了一个惩罚项 $L’$ 来减小特征间的类内距离。因此，<strong>此方法仅适用于每一类有多个样本的数据集。</strong></p>

<script type="math/tex; mode=display">L = L_S + \lambda L'\tag{1}</script>

<p>Center loss基于欧式距离所定义的类内距离损失 $L’$ 如下：</p>

<script type="math/tex; mode=display">L_C = \cfrac{1}{2}\sum^m_{i=1}\Vert x_i-c_{y_i}\Vert^2_2</script>

<p>Softmax损失函数加入Center loss约束后，学习到的特征可视化如下：</p>

<p><img src="/img/softmax_center_loss.PNG" alt="" /></p>

<p>总结一下，Center loss这篇文章的核心贡献有两个：</p>

<p>1、发现了可以对类内距离损失 $L’​$ 进行约束；</p>

<p>2、并提出了一个基于欧式距离的类内距离损失。</p>

<p>下面介绍的三篇文章，都是在Center loss思路的基础上，基于余弦距离或角度距离来重新定义类内距离损失 $L’​$ 。</p>

<ul>
  <li>L-Softmax loss, ICML2016</li>
  <li>A-Softmax loss, CVPR2017</li>
  <li>ArcFace, arXiv2018</li>
</ul>

<h2 id="2-l-softmax-loss">2 L-Softmax loss</h2>

<p>使用欧式距离对类内距离损失进行约束是合适的吗？</p>

<p>L-Softmax loss (ICML2016)，提出用<strong>余弦距离</strong>代替欧式距离，然后<strong>增大学习的难度</strong>。</p>

<p>对于二分类，Softmax的决策边界是 $W^T_1x&gt;W^T_2x​$，而向量点积可以用几何定义来表示：$\vec{a} \cdot \vec{b}=\Vert a\Vert \Vert b\Vert \cos\theta​$。因此，Softmax的决策边界可以转化为如下形式：</p>

<script type="math/tex; mode=display">\Vert W_1\Vert \Vert x\Vert \cos\theta_1>\Vert W_2\Vert \Vert x\Vert \cos\theta_2</script>

<p>然后引入一个参数 $m​$ 来增大学习的难度，因为cos函数在 $[0, \pi]​$ 范围内单调递减，所以 $m​$ 取值范围为正整数。相应的决策边界为：</p>

<script type="math/tex; mode=display">\Vert W_1\Vert \Vert x\Vert \cos m\theta_1>\Vert W_2\Vert \Vert x\Vert \cos\theta_2</script>

<p>最终，L-Softmax loss的定义如下：</p>

<script type="math/tex; mode=display">L=-ln\cfrac{e^{\Vert W_i\Vert \Vert x\Vert \cos m\theta_i}}{e^{\Vert W_i\Vert \Vert x\Vert \cos m\theta_i} + \sum^n_{j=1,j\neq i} e^{\Vert W_j\Vert \Vert x\Vert \cos \theta_j}}\tag{2}</script>

<p>基于 L-Softmax  loss 学习到的特征可视化如下：</p>

<p><img src="/img/L_Softmax_loss.PNG" alt="" /></p>

<h2 id="3-a-softmax-loss">3 A-Softmax loss</h2>

<p>A-Softmax loss 是 SphereFace (CVPR2017) 提出的一个方法。</p>

<p>因为余弦距离衡量的是空间中两个向量的夹角，与向量长度无关。</p>

<p>所以在 L-Softmax loss 的基础上，A-Softmax loss 索性对参数向量 $\Vert W_i\Vert$ 进行了<strong>L2归一化</strong>。</p>

<p>其决策边界为如下形式：</p>

<script type="math/tex; mode=display">\Vert x\Vert(\cos m\theta_1-\cos\theta_2)=0</script>

<p>相应的，L-Softmax loss 定义如下：</p>

<script type="math/tex; mode=display">L=-ln\cfrac{e^{\Vert x\Vert \cos m\theta_i}}{e^{\Vert x\Vert \cos m\theta_i} + \sum^n_{j=1,j\neq i} e^{\Vert x\Vert \cos \theta_j}}\tag{3}</script>

<p>基于 A-Softmax  loss 学习到的三维特征可视化如下：</p>

<p><img src="/img/A_Softmax_loss.PNG" alt="" /></p>

<h2 id="4-arcface">4 ArcFace</h2>

<p>终于到 ArcFace 了。</p>

<p>ArcFace 的改进是，在 SphereFace 对参数向量 $\Vert W_i\Vert$ 进行归一化的基础上，对特征 $\vec{x}$ 也做了<strong>L2归一化</strong>。</p>

<p>因此，决策边界变成了如下形式：</p>

<script type="math/tex; mode=display">s(\cos(\theta_1+m)-\cos \theta_2)=0</script>

<p>相应的，L-Softmax loss 定义如下：</p>

<script type="math/tex; mode=display">L=-ln\cfrac{e^{s(\cos (\theta_i+m))}}{e^{s(\cos (\theta_i+m))} + \sum^n_{j=1,j\neq i} e^{s(\cos \theta_j)}}\tag{4}</script>

<p><strong>关于超参数</strong></p>

<p>关于超参数 $m$ 的取值，作者基于实验给出的建议是 0.2~0.5（弧度值），取值大于0.5时会过拟合。</p>

<p><strong>关于数据集</strong></p>

<table>
  <thead>
    <tr>
      <th>Dataset</th>
      <th>identities #</th>
      <th>images #</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>VGG2</td>
      <td>9,131</td>
      <td>3.31M</td>
    </tr>
    <tr>
      <td>MS-Celeb-1M</td>
      <td>100k</td>
      <td>10M</td>
    </tr>
    <tr>
      <td>LFW</td>
      <td>5,749</td>
      <td>13.23k</td>
    </tr>
    <tr>
      <td>CFP</td>
      <td>500</td>
      <td>7k</td>
    </tr>
    <tr>
      <td>AgeDB</td>
      <td>440</td>
      <td>12.24k</td>
    </tr>
    <tr>
      <td>MegaFace</td>
      <td>690k</td>
      <td>1M</td>
    </tr>
  </tbody>
</table>

<p>训练集：VGG2, MS-Celeb-1M；验证集：LFW, CFP, AgeDB；测试集：MegaFace。</p>

<p>（验证集的作用是选择超参数。）</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://zhuanlan.zhihu.com/p/25723112">详解softmax函数以及相关求导过程</a></p>

<p>[2] A Discriminative Feature Learning Approach for Deep Face Recognition, ECCV2016</p>

<p>[3] Large-Margin Softmax Loss for Convolutional Neural Networks, ICML2016</p>

<p>[4] SphereFace: Deep Hypersphere Embedding for Face Recognition, CVPR2017</p>

<p>[5] ArcFace: Additive Angular Margin Loss for Deep Face Recognition, arXiv2018</p>
</p>
      -->
      <span class="post-meta">Feb 19, 2019</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2019/02/15/OpenPyXL.html">OpenPyXL Notes</a></h2>
      <!--
      <p><h1 id="openpyxl-notes">OpenPyXL Notes</h1>

<p>OpenPyXL 是一个用于读写 excel 文件的 Python 库，简单记录一下使用方法。</p>

<p><strong>写入のdemo</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">openpyxl</span> <span class="kn">import</span> <span class="n">Workbook</span>
<span class="kn">from</span> <span class="nn">openpyxl.styles</span> <span class="kn">import</span> <span class="n">PatternFill</span>
<span class="n">wb</span> <span class="o">=</span> <span class="n">Workbook</span><span class="p">()</span>
<span class="n">ws</span> <span class="o">=</span> <span class="n">wb</span><span class="o">.</span><span class="n">active</span>

<span class="n">ws</span><span class="p">[</span><span class="s">"A1"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span> <span class="c"># assign 1</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="s">"A2"</span><span class="p">]</span> <span class="c"># assign 2</span>
<span class="n">cell</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">1994</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">ws</span><span class="o">.</span><span class="n">cell</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">column</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c"># assign 3</span>
<span class="n">cell</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="s">"Amoko"</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="s">"B1"</span><span class="p">]</span> <span class="c"># assign 4</span>
<span class="n">cell</span><span class="o">.</span><span class="n">fill</span> <span class="o">=</span> <span class="n">PatternFill</span><span class="p">(</span><span class="s">"solid"</span><span class="p">,</span> <span class="n">fgColor</span><span class="o">=</span><span class="s">"ff0000"</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="s">"B2"</span><span class="p">]</span> <span class="c"># assign 5</span>
<span class="n">cell</span><span class="o">.</span><span class="n">fill</span> <span class="o">=</span> <span class="n">PatternFill</span><span class="p">(</span><span class="s">"solid"</span><span class="p">,</span> <span class="n">fgColor</span><span class="o">=</span><span class="s">"00ff00"</span><span class="p">)</span>
<span class="n">wb</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">"0215.xlsx"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>读取のdemo</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">openpyxl</span> <span class="kn">import</span> <span class="n">load_workbook</span>
<span class="n">wb</span> <span class="o">=</span> <span class="n">load_workbook</span><span class="p">(</span><span class="s">"0215.xlsx"</span><span class="p">)</span>
<span class="n">ws</span> <span class="o">=</span> <span class="n">wb</span><span class="p">[</span><span class="n">wb</span><span class="o">.</span><span class="n">sheetnames</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

<span class="k">print</span><span class="p">(</span><span class="n">ws</span><span class="o">.</span><span class="n">max_row</span><span class="p">,</span> <span class="n">ws</span><span class="o">.</span><span class="n">max_column</span><span class="p">)</span>
<span class="n">cell</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="s">"B1"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">fill</span><span class="o">.</span><span class="n">start_color</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">ws</span><span class="o">.</span><span class="n">iter_rows</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">row</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">row</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">col_idx</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://openpyxl.readthedocs.io/en/stable/">OpenPyXL 官网文档</a></p>

</p>
      -->
      <span class="post-meta">Feb 15, 2019</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2019/02/13/Affine-Perspective.html">Affine, Perspective Transformation</a></h2>
      <!--
      <p><h1 id="affine-perspective-transformation">Affine, Perspective Transformation</h1>

<p><img src="/img/ap.png" alt="" /></p>

<h2 id="1-仿射变换">1 仿射变换</h2>

<p>仿射变换是在二维空间上对图像进行平移、缩放、旋转、shear 和镜像5个操作的组合。</p>

<p>仿射变换后，相交线之间的角度可能发生变化（shear），但平行线之间的关系保持不变。</p>

<p>因为仿射变换是在二维空间中进行的，所以至少需要3个点才能构造一个 $2\times3$ 的仿射变换矩阵。</p>

<p>仿射变换矩阵中各个元素的作用见下图：</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/2D_affine_transformation_matrix.svg/500px-2D_affine_transformation_matrix.svg.png" alt="" /></p>

<p>求出仿射变换矩阵 $M$ 后，与点 $P(x, y)$ 进行矩阵乘法，就得到仿射变换后的点 $P’(x’,y’)$。</p>

<script type="math/tex; mode=display">M \cdot
\begin{bmatrix}
x\\y\\1
\end{bmatrix}=
\begin{bmatrix}
x'\\y'
\end{bmatrix}</script>

<ul>
  <li>将点 $p(0,0)$ 通过仿射变换矩阵在 $x,y$ 方向上分别平移1，2个单位。</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
1 & 0 & 1\\
0 & 1 & 2
\end{bmatrix}\cdot
\begin{bmatrix}
0\\0\\1
\end{bmatrix}=
\begin{bmatrix}
1\\2
\end{bmatrix} %]]></script>

<ul>
  <li>将点 $p(0,1)$ 通过仿射变换矩阵绕原点旋转$90^{\circ}$。</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
0 & 1 & 0\\
-1 & 0 & 0
\end{bmatrix}\cdot
\begin{bmatrix}
0\\1\\1
\end{bmatrix}=
\begin{bmatrix}
1\\0
\end{bmatrix} %]]></script>

<h2 id="2-透视变换">2 透视变换</h2>

<p><em>“一个简单的例子，你用手电筒往墙上打光。</em></p>

<p><em>如果手电筒和墙面垂直，打出来的光是圆，有倾斜角度就是椭圆。离墙面近光环就小点，离得远光环就大点。</em></p>

<p><em>但手电筒本身是固定不变的，只是不同的透视变换有不同的结果。”</em></p>

<p>透视变换就是通过投影的方式，把当前平面上的图像映射到另外一个平面。</p>

<p>因为透视变换是在三维空间中进行的，所以至少需要4个点才能构造一个一个 $3\times3$ 的透视变换矩阵。</p>

<p>透视变换矩阵没有直观的理解，就不做解释辽。</p>

<h2 id="3-opencv中调用">3 OpenCV中调用</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span> <span class="k">as</span> <span class="n">cv</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'circle.jpg'</span><span class="p">)</span>
<span class="c"># affine</span>
<span class="n">pts1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3200</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3200</span><span class="p">]])</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">2400</span><span class="p">,</span><span class="mi">200</span><span class="p">],</span> <span class="p">[</span><span class="mi">3200</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3200</span><span class="p">]])</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">getAffineTransform</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="mi">3200</span><span class="p">,</span><span class="mi">3200</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c"># perspective</span>
<span class="n">pts1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3200</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3200</span><span class="p">],</span> <span class="p">[</span><span class="mi">3200</span><span class="p">,</span> <span class="mi">3200</span><span class="p">]])</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">800</span><span class="p">,</span><span class="mi">200</span><span class="p">],</span> <span class="p">[</span><span class="mi">3200</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3200</span><span class="p">],</span> <span class="p">[</span><span class="mi">3000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">]])</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">getPerspectiveTransform</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">warpPerspective</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="mi">3200</span><span class="p">,</span><span class="mi">3200</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Affine_transformation">Wikipedia Affine transformation</a></p>

<p>[2] <a href="https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html">OpenCV Geometric Transformations of Images</a></p>

<p>[2] <a href="https://www.cnblogs.com/houkai/p/6660272.html">仿射变换与投影变换 - houkai</a></p>
</p>
      -->
      <span class="post-meta">Feb 13, 2019</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/12/24/Integral-Image.html">Integral Image</a></h2>
      <!--
      <p><h2 id="1-积分图">1 积分图</h2>

<p>对于需要多次计算图像子区域像素值之和的算法，都可以采用积分图进行加速。</p>

<p>积分图上每一点的值 $sum(x,y)$ 是原图对应位置左上角矩形的像素和。</p>

<p>而关于这个矩形是否包含边缘，并没有一个统一的标准，因此积分图的定义有两种格式。</p>

<p><strong>Wikipedia 的定义</strong>：</p>

<script type="math/tex; mode=display">sum(x,y)=\sum_{x'\leq x,y'\leq y}i(x',y')</script>

<p>inclusive，最终积分图尺寸为 $W \times H$。</p>

<p><strong>OpenCV  的定义</strong>：</p>

<script type="math/tex; mode=display">% <![CDATA[
sum(x,y)=\sum_{x'<x,y'<y}i(x',y') %]]></script>

<p>exclusive，最终积分图尺寸 $(W+1) \times (H+1)$。</p>

<h2 id="2-opencv中的积分图">2 OpenCV中的积分图</h2>

<h3 id="21-python">2.1 python</h3>

<p>从skimage库中读取一张RGB图片，转化为灰度图后，求积分图。</p>

<p>灰度图每个像素所占存储空间为8位，而积分图需要32位。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">cv2</span> <span class="k">as</span> <span class="n">cv</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">astronaut</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">grayImg</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img_integral</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">integral</span><span class="p">(</span><span class="n">grayImg</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">grayImg</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img_integral</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">513</span><span class="p">,</span> <span class="mi">513</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="n">grayImg</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">'</span><span class="nc">numpy</span><span class="o">.</span><span class="n">uint8</span><span class="s">'&gt;</span><span class="err">
</span><span class="s">&gt;&gt;&gt; type(img_integral[0][0])</span><span class="err">
</span><span class="s">&lt;class '</span><span class="n">numpy</span><span class="o">.</span><span class="n">int32</span><span class="s">'&gt;</span><span class="err">
</span></code></pre></div></div>

<h3 id="22-c">2.2 C++</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">integral</span><span class="p">(</span><span class="n">grayImg</span><span class="p">,</span> <span class="n">inteImg</span><span class="p">);</span>
</code></pre></div></div>

<p>输入、输出的数据类型都为Mat，Mat.data的数据类型为 unsigned char*</p>

<p>灰度图、积分图的每个元素的数据类型分别为CV_8U、CV_32S</p>

<p><strong>tips</strong></p>

<p>当使用一些第三方库对积分图进行加速优化时，需要特别注意 <strong>积分图定义方式</strong>，<strong>步长*数据类型长度</strong>。</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Summed-area_table">Wikipedia Integral image</a></p>

<p>[2] <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#integral">OpenCV docs Integral image</a></p>

<p>[3] <a href="https://algorithm.joho.info/image-processing/integral-image/">積分画像の原理・計算式・高速化</a></p>

<p>[4] <a href="https://www.cnblogs.com/Imageshop/p/6219990.html">13行代码实现最快速最高效的积分图像算法</a></p>
</p>
      -->
      <span class="post-meta">Dec 24, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/12/20/Jekyll.html">Jekyll Configuration On Ubuntu</a></h2>
      <!--
      <p><h1 id="jekyll-configuration-on-ubuntu">Jekyll Configuration On Ubuntu</h1>

<p>Jekyll 是一个静态网页生成器，Github Pages 支持以 Jekyll 来组织你的博客。</p>

<p>首先，你无需本地安装 Jekyll，就可以直接在 Github 编辑你的博客仓库。</p>

<p>但是，当你想要对博客进行大量改动时，本地修改通过后再推送到 Github 是更好的方式。</p>

<p>此篇博客是在 Ubuntu 上使用 Jekyll 的一个记录。</p>

<h2 id="1-installation">1 Installation</h2>

<h3 id="11-安装-ruby23">1.1 安装 Ruby2.3</h3>

<p>在安装 Jekyll 前，需要先解决 Ruby 依赖。</p>

<p>Ruby 2.3 安装命令如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-add-repository ppa:brightbox/ruby-ng
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get install ruby2.3 ruby2.3-dev
</code></pre></div></div>

<p>验证 Ruby 版本</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ruby <span class="nt">-v</span>
<span class="c"># ruby 2.3.8p459 (2018-10-18 revision 65136) [x86_64-linux-gnu]</span>
</code></pre></div></div>

<h3 id="12-安装-jekyll--bundler">1.2 安装 Jekyll &amp; Bundler</h3>

<p>命令如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>gem install jekyll bundler
</code></pre></div></div>

<p>OK，完了。</p>

<h2 id="2-starts">2 Starts</h2>

<h3 id="21-新建-jekyll-项目">2.1 新建 Jekyll 项目</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jekyll new myblog
</code></pre></div></div>

<h3 id="22-启动服务">2.2 启动服务</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>myblog
bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<p>然后就可以访问 Jekyll 项目网页了，<a href="http://localhost:4000/">http://localhost:4000</a></p>

<h2 id="3-conect-with-github-pages">3 Conect with Github Pages</h2>

<h3 id="31-克隆仓库到本地">3.1 克隆仓库到本地</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/Amoko/amoko.github.io
</code></pre></div></div>

<h3 id="32-新建gemfile文件文件内容如下">3.2 新建Gemfile文件，文件内容如下</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source 'https://rubygems.org'

gem 'github-pages', group: :jekyll_plugins
</code></pre></div></div>

<h3 id="33-运行命令">3.3 运行命令</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle install
</code></pre></div></div>

<p>然后我在这里遇到一个问题，nokogiri 依赖安装失败；</p>

<p>查看日志发现缺少”zlib.h”文件，需要安装 zlib1g-dev</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get install zlib1g-dev
</code></pre></div></div>

<p>然而又遇到如下问题，“zlib1g-dev : 依赖: zlib1g (= 1:1.2.3.4.dfsg-3ubuntu4) 但是 1:1.2.8.dfsg-1ubuntu1 已安装”，解决方法是将 zlib1g 降级；</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt install zlib1g=1:1.2.3.4.dfsg-3ubuntu4
</code></pre></div></div>

<p>问题解决，不过第二个问题仅在 Ubuntu 14.04上出现过，切换到 Ubuntu 16.04没有出现该问题。</p>

<h3 id="34-启动服务">3.4 启动服务</h3>

<p>同じ</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<h2 id="4-liquid语法">4 Liquid语法</h2>

<p>待补充</p>

<h2 id="5-tbc">5 TBC</h2>

<p>では、十分近づくがよい！</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://jekyllrb.com/docs/">Quickstart Jekyll</a></p>

<p>[2] <a href="https://www.brightbox.com/blog/2016/01/06/ruby-2-3-ubuntu-packages/">Ruby 2.3 Ubuntu packages</a></p>

<p>[3] <a href="https://help.github.com/articles/setting-up-your-github-pages-site-locally-with-jekyll/">Setting up your GitHub Pages site locally with Jekyll</a></p>
</p>
      -->
      <span class="post-meta">Dec 20, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/12/08/Otsu-threshold.html">Otsu の method</a></h2>
      <!--
      <p><h1 id="otsu-の-method">Otsu の method</h1>

<p>大津（Otsu）算法是图像领域一个基础的二值化方法，由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/38px-Flag_of_Japan.svg.png" alt="" />日本人 <ruby>大津展之<rt>おおつのぶゆき</rt></ruby> 于1979年提出。</p>

<p>大津算法的输入为灰度图，在灰度图上求得一个自适应阈值，以此阈值为界将灰度图像二值化。</p>

<h2 id="1-灰度图--二值图">1 灰度图 &amp; 二值图</h2>

<p>对于灰度图，一个像素的存储空间为 8 bit，即取值空间为256，从黑到白共有256种亮度变化（0~255）。</p>

<p>对于二值图，一个像素的存储空间为 1 bit，即取值空间为2，只有黑、白两种取值。</p>

<p>下图为灰度图、二值图对比：</p>

<p><img src="/img/8bit_1bit.png" alt="" /></p>

<p>所以将灰度图二值化，也就是在灰色地带选一个阈值，将低于此值的归入黑色类，将高于此值的归入白色类。</p>

<p>Id est, from quantity to quality.</p>

<p>那么，这个阈值要怎么取才能更加公允呢？</p>

<h2 id="2-大津算法">2 大津算法</h2>

<p>算法步骤很简单，<strong>遍历所有灰度值，找到使类间方差最大的灰度值作为二值化的阈值</strong>，完了。</p>

<p>现在问题只有一个，类间方差是什么？</p>

<p>大津算法所使用的类间方差定义，与Fisher线性判别（LDA）相同。</p>

<p><strong>类间方差</strong></p>

<p>阈值为 $t$ 时，类间方差 $\delta_t$ 定义如下：
<script type="math/tex">% <![CDATA[
\begin{aligned}
\delta_t^2 &= w_0(\mu_0-\mu)^2 + w_1(\mu_1-\mu)^2 \\
&= w_0w_1(\mu_0-\mu_1)^2
\end{aligned}\tag{1} %]]></script>
$p_i$，灰度值 $i$ 占比；$\mu=\sum_0^{255} ip_i$，全图灰度均值。</p>

<p>$w_0=\sum_0^{t}p_i$，黑色像素占比；$\mu_0=\cfrac{\sum_0^tip_i}{w_0}$，黑色像素灰度均值。</p>

<p>$w_1=\sum_{t+1}^{255}p_i$，白色像素占比；$\mu_1=\cfrac{\sum_{t+1}^{255}ip_i}{w_1}$，白色像素灰度均值。</p>

<p>大津法类间方差的计算，基于每个灰度值的占比。</p>

<p>因此计算灰度直方图后，对灰度直方图进行查表统计，就可以得到每个阈值下的类间方差。</p>

<p><img src="/img/gray_histogram.png" alt="" /></p>

<h2 id="3-numpy实现">3 NumPy实现</h2>

<p>笔者基于 NumPy 对大津法的一个简单实现：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">cv2</span> <span class="k">as</span> <span class="n">cv</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">get_otsu_value</span><span class="p">(</span><span class="n">grayImg</span><span class="p">):</span>
    <span class="c"># get hist</span>
    <span class="n">ravel</span> <span class="o">=</span> <span class="n">grayImg</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">hist</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">ravel</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">257</span><span class="p">))</span>
    <span class="n">sum_value</span> <span class="o">=</span> <span class="n">hist</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">hist_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">/</span><span class="n">sum_value</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">hist</span><span class="p">]</span>
    <span class="n">weight_p</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">):</span>
        <span class="n">weight_p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">hist_p</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">hist_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hist_p</span><span class="p">)</span>
    <span class="n">weight_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weight_p</span><span class="p">)</span>
    
    <span class="c"># search</span>
    <span class="n">the_max</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">weight_p</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"weighted mean:"</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">):</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">hist_p</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">w1</span> <span class="o">=</span> <span class="n">hist_p</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">w0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">w1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">mu0</span> <span class="o">=</span> <span class="n">weight_p</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">w0</span>
        <span class="n">mu1</span> <span class="o">=</span> <span class="n">weight_p</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">w1</span>
     
        <span class="n">delta</span> <span class="o">=</span> <span class="n">w0</span><span class="o">*</span><span class="p">(</span><span class="n">mu0</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">w1</span><span class="o">*</span><span class="p">(</span><span class="n">mu1</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="c">#delta2 = w0*w1*(mu0-mu1)**2</span>
        <span class="c">#print(delta, delta2)</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">the_max</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">the_max</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">delta</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"adjust thres, loss:"</span><span class="p">,</span> <span class="n">the_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">the_max</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">img</span>  <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"bear.jpg"</span><span class="p">)</span>
<span class="n">grayImg</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">thres</span> <span class="o">=</span> <span class="n">get_otsu_value</span><span class="p">(</span><span class="n">grayImg</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">thresImg</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">grayImg</span><span class="p">,</span> <span class="n">thres</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="4-opencv中直接调用">4 OpenCV中直接调用</h2>

<p>在 OpenCV 中使用大津法进行二值化，只需在二值化函数中加入 <code>THRESH_OTSU</code> 参数即可。</p>

<p>可以查看 OpenCV 源代码，在二值化函数中加入 <code>THRESH_OTSU</code> 参数，实质为在二值化前调用大津算法求自适应阈值，<strong>替换参数原阈值后</strong>再进行二值化。</p>

<h3 id="41-python">4.1 Python</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span> <span class="k">as</span> <span class="n">cv</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span>  <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"bear.jpg"</span><span class="p">)</span>
<span class="n">grayImg</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="c"># threshold with otsu</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">thresImg</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">grayImg</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">thresImg</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="42-c">4.2 C++</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;opencv2/opencv.hpp&gt;
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">cv</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="n">Mat</span> <span class="n">img</span><span class="p">;</span>
	<span class="n">img</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="s">"bear.jpg"</span><span class="p">);</span>
	<span class="n">Mat</span> <span class="n">grayImg</span><span class="p">;</span>
	<span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">grayImg</span><span class="p">,</span> <span class="n">CV_BGR2GRAY</span><span class="p">);</span>
	<span class="n">Mat</span> <span class="n">biImg</span><span class="p">;</span>
	<span class="c1">// threshold with otsu
</span>	<span class="n">threshold</span><span class="p">(</span><span class="n">grayImg</span><span class="p">,</span> <span class="n">biImg</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">THRESH_BINARY</span> <span class="o">|</span> <span class="n">THRESH_OTSU</span><span class="p">);</span>
	<span class="n">imshow</span><span class="p">(</span><span class="s">"biImg"</span><span class="p">,</span> <span class="n">biImg</span><span class="p">);</span>
	<span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p><strong>tips</strong></p>

<p>据笔者在工程中的体验，大津算法的时间瓶颈在于统计灰度直方图步骤，而非阈值的遍历搜索步骤。</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Grayscale">Wikipedia Grayscale</a></p>

<p>[2] <a href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%B4%A5%E7%AE%97%E6%B3%95">Wikipedia 大津算法</a></p>

<p>[3] <a href="https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html">OpenCV thresholding turorial</a></p>

</p>
      -->
      <span class="post-meta">Dec 8, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/11/17/OpenCV-Config.html">OpenCV Configuration, Python &amp; C++</a></h2>
      <!--
      <p><h1 id="opencv-configuration-python--c">OpenCV Configuration, Python &amp; C++</h1>

<h2 id="1-linux--python">1 Linux + Python</h2>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1 not using cv.imshow()</span>
pip install opencv-python
<span class="c"># 2 using cv.imshow(), but can't read video files</span>
conda remove opencv
conda install <span class="nt">-c</span> menpo opencv
</code></pre></div></div>

<h2 id="2-win-10--python">2 Win 10 + Python</h2>

<h3 id="21-安装">2.1 安装</h3>
<blockquote>
  <p>版本：Win 10 + Python 3.7 + OpenCV 3.4.3</p>

  <p>时间：2018.11.17</p>
</blockquote>

<ol>
  <li>安装Anaconda3，<a href="https://www.anaconda.com/download/">官网下载页面</a>；</li>
  <li>安装OpenCV，<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/">UCI下载页面</a>，选择opencv_python-3.4.3-cp37-cp37m-win_amd64.whl下载，使用pip本地安装即可。</li>
</ol>

<h3 id="22-canny边缘检测demo">2.2 Canny边缘检测demo</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'tower.jpg'</span><span class="p">)</span>
<span class="n">edge</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Canny</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Original Image'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Edge Image'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="3-win-10--c">3 Win 10 + C++</h2>

<h3 id="31-安装">3.1 安装</h3>

<blockquote>
  <p>版本：Win 10 + VS 2013 + OpenCV 2.4.13</p>

  <p>时间：2018.11.17</p>
</blockquote>

<p>OpenCV 发布页面，<a href="https://opencv.org/releases.html">OpenCV releases</a>，选择2.4.13- winpack下载，双击解压即可。</p>

<h3 id="32-配置项目">3.2 配置项目</h3>

<p>注意，我的 opencv-2.4.13.exe 解压目录为 <code>D:\opencv</code>，请根据你的目录修改路径。</p>

<ol>
  <li>
    <p>添加环境变量</p>

    <p>path中添加 <code> D:\opencv\build\x86\vc12\bin;</code></p>
  </li>
  <li>
    <p>配置项目目录</p>

    <p>选择 Project - Properties - VC++ Directories</p>

    <p>Include Directories 中添加 <code>D:\opencv\build\include;</code></p>

    <p>Library Directories 中添加 <code>D:\opencv\build\x86\vc12\lib;</code></p>
  </li>
  <li>
    <p>配置链接</p>

    <p>选择 Project - Properties - Linker - Input</p>

    <p>Additional Dependencies 中添加下列内容。（默认是debug模式，）</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>opencv_ml2413d.lib
opencv_calib3d2413d.lib
opencv_contrib2413d.lib
opencv_core2413d.lib
opencv_features2d2413d.lib
opencv_flann2413d.lib
opencv_gpu2413d.lib
opencv_highgui2413d.lib
opencv_imgproc2413d.lib
opencv_legacy2413d.lib
opencv_objdetect2413d.lib
opencv_ts2413d.lib
opencv_video2413d.lib
opencv_nonfree2413d.lib
opencv_ocl2413d.lib
opencv_photo2413d.lib
opencv_stitching2413d.lib
opencv_superres2413d.lib
opencv_videostab2413d.lib
</code></pre></div>    </div>
  </li>
</ol>

<p><strong>tips</strong></p>

<ul>
  <li>
    <p>项目编译通过，Debug阶段报错，“应用程序无法正常启动(0xc000007b)”。</p>

    <p>x86/x64平台问题，检查环境变量与项目配置中，所使用的是哪一个版本。</p>
  </li>
</ul>

<h3 id="33-canny边缘检测demo">3.3 Canny边缘检测demo</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;opencv2/core/core.hpp&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;
#include &lt;opencv2/imgproc/imgproc.hpp&gt;
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">cv</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="n">Mat</span> <span class="n">src</span><span class="p">,</span> <span class="n">edge</span><span class="p">;</span>
	<span class="n">src</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="s">"1.jpg"</span><span class="p">);</span>
    <span class="c1">// Canny edge
</span>    <span class="n">Canny</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">edge</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">);</span>
	<span class="n">imshow</span><span class="p">(</span><span class="s">"Canny edge"</span><span class="p">,</span> <span class="n">edge</span><span class="p">);</span>
	<span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

</p>
      -->
      <span class="post-meta">Nov 17, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/11/05/Cycle-GAN.html">Cycle GAN Notes</a></h2>
      <!--
      <p><h2 id="1-gan--cycle-gan">1 GAN &amp; Cycle GAN</h2>

<p>Ian Goodfellow 在2014年提出了原始的GAN模型，我这篇博客有初步介绍 $\rightarrow$ <a href="https://amoko.github.io/2018/10/31/GAN.html">GAN Notes</a>。</p>

<p>Cycle GAN是2017年ICCV上的一篇文章，以GAN为基础来实现图像的风格迁移，表现非常惊艳。</p>

<p>论文标题：Unpaired image-to-image translation using cycle-consistent adversarial networks.</p>

<p>Github项目主页：<a href="https://junyanz.github.io/CycleGAN/">Cycle GAN</a>。</p>

<h2 id="2-motivation">2 Motivation</h2>

<p>原始GAN学习到的是<strong>随机噪声</strong>分布$Z$到<strong>真实数据</strong>分布$X$的映射：$G(z)\approx x$。</p>

<p>但GAN模型所生成的数据是不可控的，以MINIST数据集为例，GAN无法生成指定label的数字图片。</p>

<p>那么，如果将label作为条件信息也加入模型进行训练呢？这个思路就是 conditional GAN。</p>

<p>具体到图像风格迁移，基于conditional GAN的pix2pix已经可以完成这项任务，但代价是训练样本必须是两两配对的。</p>

<p><strong>Cycle GAN对pix2pix的改进是解决了对训练样本的限制，即不需要两个domain的样本是两两配对的。</strong></p>

<p>以上就是Cycle GAN这个工作的意义。</p>

<h2 id="3-how">3 How?</h2>

<h3 id="31-基础架构">3.1 基础架构</h3>

<ul>
  <li>
    <p>$X​$：来自domain $X​$的数据；</p>
  </li>
  <li>
    <p>$Y​$：来自domain $Y​$的数据；</p>
  </li>
  <li>
    <p>$G(X)$：生成器，用来自$X$的数据仿造$Y$；</p>
  </li>
  <li>
    <p>$D_Y$：判别器，判定数据来自$Y​$的概率；</p>
  </li>
  <li>
    <p>$F(Y)$：生成器，用来自$Y$的数据仿造$X$；</p>
  </li>
  <li>
    <p>$D_X$：判别器，判定数据来自$X$的概率；</p>
  </li>
</ul>

<p>需要注意，原始GAN以随机噪声作为生成器的输入。</p>

<p>而Cycle GAN生成器的输入是另一个域的数据，因此<strong>不再需要随机噪声</strong>。</p>

<h3 id="32-思路">3.2 思路</h3>

<p>Cycle GAN的解决方案很简单，一个GAN不行，我两个GAN行不行？</p>

<p>直观上，Cycle GAN就是训练两个GAN模型构成一个循环。</p>

<p>第一个GAN的生成器负责学习 $X\rightarrow Y$，第二个GAN的生成器负责学习 $Y\rightarrow X$。</p>

<p>然后保证数据在这个循环前后的一致性：$x\rightarrow G(x)\rightarrow F(G(x))\approx x$，$y\rightarrow F(y)\rightarrow G(F(y))\approx y$。</p>

<h3 id="33-损失函数">3.3 损失函数</h3>

<p><img src="/img/cycle_gan.PNG" alt="" /></p>

<p>对两个domain的数据${{X,Y}}$，学习过程分别是对称的两个循环：$F(G(x))\approx x$，$G(F(y))\approx y$。</p>

<p>这两个循环的损失函数定义为 cycle consistency loss，即循环一致性损失：</p>

<script type="math/tex; mode=display">L_{cyc}(G,F)=E_{x\sim p_{data}(x)}[\Vert F(G(x))-x\Vert_1]+E_{y\sim p_{data}(y)}[\Vert G(F(y))-y\Vert_1]\tag{1}</script>

<p>所以模型最终的Loss是两个GAN的Loss + 两个循环的Loss。
<script type="math/tex">L(G,F,D_X,D_Y)=L_{GAN}(G,D_Y)+L_{GAN}(F,D_X)+L_{cyc}(G,F)\tag{2}</script></p>

<h3 id="34-two-details">3.4 two details</h3>

<ul>
  <li>
    <p>GAN的损失函数，用OLS取代MLE（即假设了误差服从高斯分布）。</p>

    <p>原始GAN的优化目标：
<script type="math/tex">\min_G \max_DV(D,G)=E_{x\sim p_r}[\log D(x)]+E_{x\sim p_g}[\log(1-D(x)]\tag{3}</script>
Cycle GAN的MSE loss：
<script type="math/tex">L_{GAN}(G,D_Y)=E_{x\sim p_r}[(1-D(x))^2]+E_{x\sim p_g}[D(x)^2]\tag{4}</script></p>
  </li>
  <li>
    <p>使用有时延的$G(x)$来更新判别器。</p>
  </li>
</ul>

<h2 id="4-cycle-gan-in-pytorch">4 Cycle GAN in PyTorch</h2>

<p>作者公布了Cycle GAN的PyTorch源码，Github项目主页在此 $\rightarrow$ の<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">Pytorch Cycle GAN</a>。</p>

<p>需要安装的两个额外库依赖</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install dominate
pip install visdom
</code></pre></div></div>

<p>下载数据集</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bash ./datasets/download_cyclegan_dataset.sh vangogh2photo
</code></pre></div></div>

<p>train &amp; test，参数设定及说明见 options 文件夹</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># train</span>
python <span class="nt">-m</span> visdom.server
python train.py <span class="nt">--dataroot</span> ./datasets/vangogh2photo <span class="nt">--name</span> vangoph_cyclegan <span class="nt">--model</span> cycle_gan <span class="nt">--gpu_ids</span> 0,1 <span class="nt">--batch_size</span> 8 <span class="nt">--display_id</span> <span class="nt">-1</span>
<span class="c"># test</span>
python test.py <span class="nt">--dataroot</span> ./datasets/vangogh2photo <span class="nt">--name</span> vangoph_cyclegan <span class="nt">--model</span> cycle_gan <span class="nt">--gpu_ids</span> 0,1
</code></pre></div></div>

<p>TBC</p>

<h2 id="5-無駄無駄">5 無駄無駄</h2>

<p>このジョルノ・ジョバァーナには夢がある！</p>
</p>
      -->
      <span class="post-meta">Nov 5, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/10/31/GAN.html">GAN Notes</a></h2>
      <!--
      <p><h2 id="1-gan是什么">1 GAN是什么？</h2>

<p>Generative adversarial nets，生成对抗式网络，<ruby>Ian Goodfellow<rt>イアン・グッドフェロー</rt></ruby> 2014年提出的模型。</p>

<p>简单来讲就是定义两个神经网络模型，一个是验钞机、一个假币生成机，以对抗博弈的方式来彼此学习进步，最终达到生命的大和谐（误）。</p>

<p><strong>基础架构</strong></p>

<p>$r$：真实数据；</p>

<p>$z$：随机噪声，服从分布$p_z(z)$；</p>

<p>$G(z)$：生成器，用神经网络去生成仿造数据；</p>

<p>$D(x)$：判别器，判定$x$来自真实数据的概率。</p>

<p><strong>最终目标</strong></p>

<p>$G(z)$ 学到的分布和真实数据分布相同，$p_g=p_r$，让 $D(x)$ 无法区分 $G(z)$ 与真实数据，即 $D(x)=\frac{1}{2}$。</p>

<h2 id="2-如何优化">2 如何优化？</h2>

<h3 id="21-gan的优化目标">2.1 GAN的优化目标</h3>

<p><script type="math/tex">\min_G \max_DV(D,G)=E_{x\sim p_r}[\log D(x)]+E_{x\sim p_g}[\log(1-D(x)]\tag{1}</script></p>
<h3 id="22-minimax算法">2.2 minimax算法</h3>

<p>问题背景是零和博弈。</p>

<p>假设有A，B两个玩家，双方利益之和为零或一个常数，那么任一方的获利必然意味着对方的损失。</p>

<p>令A玩家得分为$V$，那么A玩家目标是最大化$V$。对于博弈问题，在A玩家做出最优解的同时，也要假设对手是有智商的，所以B玩家目标是最小化$V$。</p>

<script type="math/tex; mode=display">\min_B \max_A V</script>

<p>Minimax算法就是在A的回合最大化$V$，在B的回合最小化$V$，以一定深度交替计算求出最优解。</p>

<p>所以GAN的优化目标，式$(1)$，也是分解为两个部分来进行。</p>

<h3 id="23-两个子优化目标">2.3 两个子优化目标</h3>

<p>1 判别器：最大化$D(x)$的正确分类率
<script type="math/tex">\max_D V(D,G)=E_{x\sim p_r}[\log D(x)]+E_{x\sim p_g}[\log(1-D(x)]\tag{2}</script>
2 生成器：最小化$D(x)$的正确分类率
<script type="math/tex">% <![CDATA[
\begin{aligned}
\min_G V(D,G)&= E_{x\sim p_g}[\log(1-D(x)]\\
&= E_{z\sim p_z(z)}[\log(1-D(G(z))]
\end{aligned}\tag{3} %]]></script></p>

<h2 id="3-pytorch实现">3 PyTorch实现</h2>

<blockquote>
  <p><em>实现代码基于莫烦的 GAN 教程，有改动。</em></p>
</blockquote>

<p>$r​$：以介于一定范围的二次函数作为真实数据。$y=ax^2+a-1, a\sim U[1,3]​$，并从 $[-2,2]​$ 均匀取 DATA_COMPONENTS 个点，作为一个函数的抽样；</p>

<p>$z$：$z\sim N[0,1]$，因为直接使用 torch.randn() 生成随机数，而这个函数使用标准正态分布（<a href="https://pytorch.org/docs/stable/torch.html#torch.randn">见文档</a>）；</p>

<p>$G(z)​$：两层神经网络；</p>

<p>$D(x)$：两层神经网络。</p>

<p>完整代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># -*- coding: utf-8 -*-</span>
<span class="s">"""
Created on Tue Oct 30 20:00:14 2018
@author: Yonji
"""</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">LR_G</span><span class="p">,</span> <span class="n">LR_D</span>  <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0001</span> <span class="c"># learning rate for G, D</span>
<span class="n">NOISE_COMPONENTS</span> <span class="o">=</span> <span class="mi">5</span>         <span class="c"># G的输入维度、即噪声维度</span>
<span class="n">DATA_COMPONENTS</span> <span class="o">=</span> <span class="mi">15</span>         <span class="c"># G的输出维度、同时也是D的输入维度、即真实数据维度</span>

<span class="c"># x轴坐标点、[-1, 1]之间等分、再复制batch_size个</span>
<span class="n">PAINT_POINTS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">DATA_COMPONENTS</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)])</span>

<span class="c"># real data</span>
<span class="k">def</span> <span class="nf">artist_works</span><span class="p">():</span>
    <span class="c"># lower &amp; upper bound</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">paintings</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">paintings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">paintings</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">paintings</span>

<span class="k">class</span> <span class="nc">GNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">NOISE_COMPONENTS</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">DATA_COMPONENTS</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">DNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">DATA_COMPONENTS</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">D</span><span class="p">,</span> <span class="n">G</span> <span class="o">=</span> <span class="n">DNet</span><span class="p">(),</span> <span class="n">GNet</span><span class="p">()</span>
    <span class="n">opt_D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR_D</span><span class="p">)</span>
    <span class="n">opt_G</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR_G</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">401</span><span class="p">):</span>
        <span class="n">artist_paintings</span> <span class="o">=</span> <span class="n">artist_works</span><span class="p">()</span>                   <span class="c"># real data</span>
        <span class="n">G_ideas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NOISE_COMPONENTS</span><span class="p">)</span> <span class="c"># z</span>
        <span class="n">G_paintings</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="n">G_ideas</span><span class="p">)</span>                            <span class="c"># G(z)</span>
    
        <span class="c"># 论文公式、MLE loss</span>
        <span class="n">prob_real</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">artist_paintings</span><span class="p">)</span>
        <span class="n">prob_fake</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">G_paintings</span><span class="p">)</span>
        <span class="n">D_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_real</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">prob_fake</span><span class="p">))</span>
        <span class="n">G_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">prob_fake</span><span class="p">))</span>
        
        <span class="c"># Loss BP、更新参数 </span>
        <span class="n">opt_D</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">D_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>      <span class="c"># reusing computational graph</span>
        <span class="n">opt_D</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
        <span class="n">opt_G</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">G_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_G</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c"># plotting</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                     <span class="s">"--"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'#000066'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'upper bound of data'</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">0</span><span class="p">,</span>
                     <span class="s">"--"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'#4AD631'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'lower bound of data'</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">artist_paintings</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
                     <span class="n">c</span><span class="o">=</span><span class="s">'#0066ff'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'a real data'</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">PAINT_POINTS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">G_paintings</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
                     <span class="n">c</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'G(z)'</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'epoch = </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="s">'D_accuracy = </span><span class="si">%.2</span><span class="s">f'</span> <span class="o">%</span> 
                     <span class="n">prob_real</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="s">'D_loss = </span><span class="si">%.2</span><span class="s">f'</span> <span class="o">%</span> 
                     <span class="n">D_loss</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">draw</span><span class="p">();</span><span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>结果如下图：</p>

<p><img src="/img/gan_morvan.png" alt="" /></p>

<h2 id="4-このディオだ">4 このディオだ！</h2>

<p>WRYYYYYYYYYY！</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://www.msra.cn/zh-cn/news/features/gan-20170511">MSRA 到底什么是生成式对抗网络GAN？</a></p>

<p>[2] <a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/4-06-GAN/">莫烦 GAN with Pytorch 教程</a></p>
</p>
      -->
      <span class="post-meta">Oct 31, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/10/30/Terminal.html">Linux, Git, etc.</a></h2>
      <!--
      <p><blockquote>
  <p>Updated on Jun 10, 2019</p>
</blockquote>

<h2 id="1-linux">1 Linux</h2>

<h3 id="11-切换用户">1.1 切换用户</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># root</span>
<span class="nb">sudo</span> <span class="nt">-i</span>
<span class="c"># 个人用户</span>
su username
</code></pre></div></div>

<h3 id="12-进程相关">1.2 进程相关</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 查看全部进程</span>
ps <span class="nt">-ef</span>
<span class="c"># 查看指定用户进程</span>
ps <span class="nt">-u</span> username
<span class="c"># 杀死进程</span>
<span class="nb">kill</span> <span class="nt">-9</span> pid
<span class="c"># 后台启动进程，且关闭终端后此进程不会被杀死</span>
nohup <span class="nb">command</span> &amp;
</code></pre></div></div>

<h3 id="13-文件相关">1.3 文件相关</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 该变文件权限</span>
chmod 777 xx
<span class="c"># 改变文件夹所有者</span>
chown <span class="nt">-R</span> java:java dir
<span class="c"># 查看文本文件</span>
<span class="nb">cat </span>1.txt | <span class="nb">grep </span>xxx
head <span class="nt">-n</span> 10 xx
tail <span class="nt">-n</span> 10 xx
tail <span class="nt">-f</span> xx
<span class="c"># 新建空文件、更新时间戳</span>
touch 1.txt
<span class="c"># 解压文件</span>
<span class="nb">tar</span> <span class="nt">-xvf</span> xx.tar
unzip xx.zip
<span class="c"># 压缩文件</span>
<span class="nb">tar</span> <span class="nt">-cvf</span> xx.tar /xxx 
<span class="c"># 从文件列表压缩文件</span>
<span class="nb">cat </span>list.txt | xargs <span class="nb">tar</span> <span class="nt">-cvfz</span> list.tar.gz
</code></pre></div></div>

<h3 id="14-软装安装">1.4 软装安装</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># ubuntu local</span>
<span class="nb">sudo </span>dpkg <span class="nt">-i</span> xxx.deb
<span class="c"># ubuntu remote</span>
<span class="nb">sudo </span>apt-get install xxx
<span class="c"># centos remote</span>
yum <span class="nt">-y</span> install vim<span class="k">*</span>
</code></pre></div></div>

<h3 id="15-编译动态链接库">1.5 编译动态链接库</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-fPIC</span> <span class="nt">-shared</span> <span class="nt">-o</span> ligsig.so signature.cpp
</code></pre></div></div>

<h3 id="16-etc">1.6 etc.</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 系统版本</span>
uname <span class="nt">-v</span>
uname <span class="nt">-m</span> <span class="o">&amp;&amp;</span> <span class="nb">cat</span> /etc/<span class="k">*</span>release
<span class="c"># 统计硬盘</span>
df <span class="nt">-h</span>
<span class="c"># 统计文件夹</span>
du <span class="nt">-h</span> <span class="nt">--max-depth</span><span class="o">=</span>1
<span class="c"># 统计文件个数</span>
<span class="nb">ls</span> | wc <span class="nt">-l</span>
<span class="c"># GPU</span>
watch <span class="nt">-n</span> 2 nvidia-smi
<span class="c"># ip</span>
ifconfig | <span class="nb">grep </span>inet
<span class="c"># cpu #</span>
<span class="nb">cat</span> /proc/cpuinfo| <span class="nb">grep</span> <span class="s2">"processor"</span>| wc <span class="nt">-l</span>
<span class="c"># net</span>
netstat <span class="nt">-lntp</span>
<span class="c"># 查找可执行文件位置</span>
which python
</code></pre></div></div>

<h2 id="2-git">2 Git</h2>

<p><a href="https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git">官方在线文档</a></p>

<p>设置用户信息</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config <span class="nt">--global</span> user.name <span class="s2">"xxx"</span>
git config <span class="nt">--global</span> user.email yyy@zzz.com
</code></pre></div></div>

<p>创建本地仓库</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 从远程获取</span>
git clone https://github.com/Amoko/CBA.git
<span class="c"># 或者本地新建</span>
git init
</code></pre></div></div>
<p>基本操作</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 素质三连</span>
git add README.md
git commit <span class="nt">-m</span> <span class="s2">"add README"</span>
git push origin master

<span class="c"># 仅移除跟踪</span>
git rm <span class="nt">--cached</span> 1.txt
<span class="c"># 彻底移除文件</span>
git rm 1.txt
<span class="c"># 重命名</span>
git mv 1.txt 2.txt

<span class="c"># 撤销commit</span>
git log <span class="c"># 找到需要回滚到的commitid</span>
git reset commitid <span class="c"># 回滚</span>

<span class="c"># 将远程仓库同步到本地</span>
git pull origin master
</code></pre></div></div>

<p>添加、删除远程仓库</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git remote add origin https://github.com/Amoko/CBA.git
git remote rm origin
</code></pre></div></div>
</p>
      -->
      <span class="post-meta">Oct 30, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/10/29/Python.html">Python Notes</a></h2>
      <!--
      <p><h1 class="no_toc" id="python-notes">Python Notes</h1>

<blockquote>
  <p>Updated on Jan 16, 2019</p>
</blockquote>

<ul id="markdown-toc">
  <li><a href="#1-syntax" id="markdown-toc-1-syntax">1 Syntax</a>    <ul>
      <li><a href="#11-格式化输出" id="markdown-toc-11-格式化输出">1.1 格式化输出</a></li>
      <li><a href="#12-对象复制" id="markdown-toc-12-对象复制">1.2 对象复制</a></li>
      <li><a href="#13-fun-with-true--1" id="markdown-toc-13-fun-with-true--1">1.3 fun with True &amp; 1</a></li>
      <li><a href="#14-list-sort" id="markdown-toc-14-list-sort">1.4 list sort</a></li>
      <li><a href="#15-iterate-through-tow-list" id="markdown-toc-15-iterate-through-tow-list">1.5 iterate through tow list</a></li>
      <li><a href="#15-inf" id="markdown-toc-15-inf">1.5 inf</a></li>
    </ul>
  </li>
  <li><a href="#2-io" id="markdown-toc-2-io">2 IO</a>    <ul>
      <li><a href="#21-读写文本文件" id="markdown-toc-21-读写文本文件">2.1 读写文本文件</a></li>
      <li><a href="#22-文件批处理三连" id="markdown-toc-22-文件批处理三连">2.2 文件批处理三连</a></li>
    </ul>
  </li>
  <li><a href="#3-etc" id="markdown-toc-3-etc">3 etc.</a>    <ul>
      <li><a href="#31-execute" id="markdown-toc-31-execute">3.1 Execute</a></li>
      <li><a href="#32-生成随机01比特串" id="markdown-toc-32-生成随机01比特串">3.2 生成随机01比特串</a></li>
    </ul>
  </li>
  <li><a href="#4-tbc" id="markdown-toc-4-tbc">4 TBC</a></li>
</ul>

<h2 id="1-syntax">1 Syntax</h2>

<h3 id="11-格式化输出">1.1 格式化输出</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">no</span> <span class="o">=</span> <span class="mi">74</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">year</span> <span class="o">=</span> <span class="mi">1984</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s</span> <span class="o">=</span> <span class="s">"Soldier </span><span class="si">%04</span><span class="s">d, welcome to </span><span class="si">%</span><span class="s">d."</span> <span class="o">%</span> <span class="p">(</span><span class="n">no</span><span class="p">,</span> <span class="n">year</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s</span>
<span class="s">'Soldier 0074, welcome to 1984.'</span>
<span class="c"># d代表十进制数据，4是最小输出字符数，用0填充。</span>
</code></pre></div></div>

<h3 id="12-对象复制">1.2 对象复制</h3>

<p><code>int</code> , <code>str</code> 数据类型可以直接使用 <code>=</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">s1</span> <span class="o">=</span> <span class="s">"nice"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">s1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s1</span> <span class="o">+=</span> <span class="s">" tits"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s2</span> <span class="o">+=</span> <span class="s">" boobs"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s1</span>
<span class="s">'nice tits'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s2</span>
<span class="s">'nice boobs'</span>
</code></pre></div></div>

<p><code>list</code> 数据类型则不能使用 <code>=</code>，要使用 copy 模块</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">copy</span>
<span class="c"># 一维数组可以使用list()或copy()</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="c"># 高维数组必须使用deepcopy()</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="13-fun-with-true--1">1.3 fun with True &amp; 1</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">'</span><span class="nc">bool</span><span class="s">'&gt;</span><span class="err">
</span><span class="s">&gt;&gt;&gt; type(1)</span><span class="err">
</span><span class="s">&lt;class '</span><span class="nb">int</span><span class="s">'&gt;</span><span class="err">
</span><span class="s">&gt;&gt;&gt; 1 == True</span><span class="err">
</span><span class="s">True</span><span class="err">
</span><span class="s">&gt;&gt;&gt; 1 ^ True</span><span class="err">
</span><span class="s">0</span><span class="err">
</span><span class="s">&gt;&gt;&gt; 0 ^ True</span><span class="err">
</span><span class="s">1</span><span class="err">
</span></code></pre></div></div>

<h3 id="14-list-sort">1.4 list sort</h3>

<p>对 <code>list</code> 进行排序</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">li</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">6.6</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">li</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">li</span>
<span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">6.6</span><span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">li</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">li</span>
<span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">6.6</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
<span class="c"># sort 默认升序排序，reverse 逆序</span>
</code></pre></div></div>

<h3 id="15-iterate-through-tow-list">1.5 iterate through tow list</h3>

<p>同步遍历两个 list</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">la</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lb</span> <span class="o">=</span> <span class="p">[</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">e1</span><span class="p">,</span> <span class="n">e2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">la</span><span class="p">,</span> <span class="n">lb</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">print</span><span class="p">(</span><span class="n">e1</span><span class="p">,</span> <span class="n">e2</span><span class="p">)</span>
<span class="mi">1</span> <span class="n">a</span>
<span class="mi">2</span> <span class="n">b</span>
<span class="mi">3</span> <span class="n">c</span>
</code></pre></div></div>

<h3 id="15-inf">1.5 inf</h3>

<p>Python 中的无穷小与无穷大</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">999</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">999</span> <span class="o">&lt;</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>
<span class="bp">False</span>
</code></pre></div></div>

<h2 id="2-io">2 IO</h2>

<h3 id="21-读写文本文件">2.1 读写文本文件</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">li</span> <span class="o">=</span> <span class="p">[</span><span class="s">"The Beatles"</span><span class="p">,</span> <span class="s">"David Bowie"</span><span class="p">,</span> <span class="s">"Radiohead"</span><span class="p">,</span> <span class="s">"Blur"</span><span class="p">]</span>
<span class="c"># write</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"1.txt"</span><span class="p">,</span> <span class="s">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
	<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">li</span><span class="p">:</span>
		<span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
		<span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">e</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
        
<span class="c"># read</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"1.txt"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</code></pre></div></div>

<h3 id="22-文件批处理三连">2.2 文件批处理三连</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>
    
<span class="n">dirname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
<span class="n">filenamelist</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dirname</span><span class="p">)</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">filenamelist</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s">".jpg"</span><span class="p">:</span>
    	<span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s">".jpg"</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    	<span class="n">shutil</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="s">"./small/"</span><span class="p">)</span> 
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<h2 id="3-etc">3 etc.</h2>

<h3 id="31-execute">3.1 Execute</h3>

<p>Python脚本可以在终端直接运行</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">xxx</span><span class="o">.</span><span class="n">py</span>
</code></pre></div></div>

<p>Python某些内置的模块（如SimpleHTTPServer）也能够以脚本方式直接运行，需添加参数-m。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> SimpleHTTPServer
</code></pre></div></div>

<h3 id="32-生成随机01比特串">3.2 生成随机01比特串</h3>

<p>不限定01比例</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># A 使用 numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">li</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="c"># B 使用 random.sample</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">li</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>
<span class="n">li</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">li</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<p>限定01比例</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 使用 random.shuffle</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">l0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">l1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span>
<span class="n">li</span> <span class="o">=</span> <span class="n">l0</span> <span class="o">+</span> <span class="n">l1</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">li</span><span class="p">)</span> 
</code></pre></div></div>

<h2 id="4-tbc">4 TBC</h2>

<p>ゴールド・エクスペリエンス！</p>
</p>
      -->
      <span class="post-meta">Oct 29, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/10/29/C.html">C/C++ Notes</a></h2>
      <!--
      <p><h1 class="no_toc" id="cc-notes">C/C++ Notes</h1>

<blockquote>
  <p>Updated on Jan 05, 2019</p>
</blockquote>

<ul id="markdown-toc">
  <li><a href="#1-syntax" id="markdown-toc-1-syntax">1 Syntax</a>    <ul>
      <li><a href="#11-预编译注释语句" id="markdown-toc-11-预编译注释语句">1.1 预编译注释语句</a></li>
      <li><a href="#12-查看数据类型" id="markdown-toc-12-查看数据类型">1.2 查看数据类型</a></li>
      <li><a href="#13-use-template" id="markdown-toc-13-use-template">1.3 Use Template</a></li>
      <li><a href="#14-键排序" id="markdown-toc-14-键排序">1.4 键排序</a></li>
      <li><a href="#15-iterate-over-array" id="markdown-toc-15-iterate-over-array">1.5 iterate over array</a></li>
    </ul>
  </li>
  <li><a href="#2-io" id="markdown-toc-2-io">2 IO</a>    <ul>
      <li><a href="#21-读写文本文件" id="markdown-toc-21-读写文本文件">2.1 读写文本文件</a></li>
      <li><a href="#21-读写二进制文件" id="markdown-toc-21-读写二进制文件">2.1 读写二进制文件</a></li>
    </ul>
  </li>
  <li><a href="#3-字符串操作" id="markdown-toc-3-字符串操作">3 字符串操作</a>    <ul>
      <li><a href="#31-整型转字符串-int-to-string" id="markdown-toc-31-整型转字符串-int-to-string">3.1 整型转字符串 int to string</a></li>
      <li><a href="#32-单引号--双引号" id="markdown-toc-32-单引号--双引号">3.2 单引号 &amp; 双引号</a></li>
      <li><a href="#33-字符串拼接" id="markdown-toc-33-字符串拼接">3.3 字符串拼接</a></li>
      <li><a href="#34-字符复制多个" id="markdown-toc-34-字符复制多个">3.4 字符复制多个</a></li>
    </ul>
  </li>
  <li><a href="#4-vs" id="markdown-toc-4-vs">4 VS</a>    <ul>
      <li><a href="#41-快捷键" id="markdown-toc-41-快捷键">4.1 快捷键</a></li>
      <li><a href="#42-error-solution" id="markdown-toc-42-error-solution">4.2 error solution</a></li>
      <li><a href="#43-etc" id="markdown-toc-43-etc">4.3 etc.</a></li>
    </ul>
  </li>
  <li><a href="#5-tbc" id="markdown-toc-5-tbc">5 TBC</a></li>
</ul>

<h2 id="1-syntax">1 Syntax</h2>

<h3 id="11-预编译注释语句">1.1 预编译注释语句</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#if 0
	cout &lt;&lt; "Hi there." &lt;&lt; endl;
	cin.get();
#endif
</span></code></pre></div></div>

<h3 id="12-查看数据类型">1.2 查看数据类型</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="k">typeid</span><span class="p">(</span><span class="mi">2018</span><span class="p">).</span><span class="n">name</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="13-use-template">1.3 Use Template</h3>

<p>使用 typename 可以接触对C++对函数参数及返回值数据类型的严格限制，在实例化后才确定数据类型。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;iostream&gt;
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
<span class="n">T</span> <span class="n">get_max</span><span class="p">(</span><span class="n">T</span> <span class="n">a</span><span class="p">,</span> <span class="n">T</span> <span class="n">b</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">a</span><span class="o">&gt;</span><span class="n">b</span> <span class="o">?</span> <span class="n">a</span> <span class="o">:</span> <span class="n">b</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="n">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
	<span class="kt">double</span> <span class="n">c</span> <span class="o">=</span> <span class="mf">3.14</span><span class="p">;</span>
	<span class="kt">double</span> <span class="n">d</span> <span class="o">=</span> <span class="mf">9.9</span><span class="p">;</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">get_max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">get_max</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>

	<span class="n">cin</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="14-键排序">1.4 键排序</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="kt">bool</span> <span class="nf">cmp_descending</span><span class="p">(</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">double</span><span class="o">&gt;</span> <span class="n">a</span><span class="p">,</span> <span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">double</span><span class="o">&gt;</span> <span class="n">b</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">a</span><span class="p">.</span><span class="n">second</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">bool</span> <span class="nf">cmp_ascending</span><span class="p">(</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">double</span><span class="o">&gt;</span> <span class="n">a</span><span class="p">,</span> <span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">double</span><span class="o">&gt;</span> <span class="n">b</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">return</span> <span class="n">a</span><span class="p">.</span><span class="n">second</span> <span class="o">&lt;</span> <span class="n">b</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="n">vector</span><span class="o">&lt;</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">double</span><span class="o">&gt;&gt;</span> <span class="n">ratio</span><span class="p">;</span>
	<span class="n">ratio</span> <span class="o">=</span> <span class="p">{</span> <span class="p">{</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">3.14</span> <span class="p">},</span> <span class="p">{</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.618</span> <span class="p">}</span> <span class="p">};</span>
	<span class="n">sort</span><span class="p">(</span><span class="n">ratio</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">ratio</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">cmp_ascending</span><span class="p">);</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">first</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
	<span class="n">sort</span><span class="p">(</span><span class="n">ratio</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">ratio</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">cmp_descending</span><span class="p">);</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">first</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
	<span class="n">cin</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="15-iterate-over-array">1.5 iterate over array</h3>

<p>c++也有类似于Python中遍历list元素的方式：</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kt">int</span> <span class="n">arr</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span> <span class="p">};</span>
	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">e</span> <span class="o">:</span> <span class="n">arr</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">e</span> <span class="o">&lt;&lt;</span> <span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">;</span>
	<span class="p">}</span>
</code></pre></div></div>

<h2 id="2-io">2 IO</h2>

<h3 id="21-读写文本文件">2.1 读写文本文件</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;fstream&gt;
#include &lt;iostream&gt;
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="n">std</span><span class="o">::</span><span class="n">fstream</span> <span class="n">fp</span><span class="p">;</span>
	<span class="n">fp</span><span class="p">.</span><span class="n">open</span><span class="p">(</span><span class="s">"./1.txt"</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">out</span><span class="p">);</span>
	<span class="n">fp</span> <span class="o">&lt;&lt;</span> <span class="mf">3.14</span><span class="p">;</span>
	<span class="n">fp</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>

	<span class="kt">double</span> <span class="n">pi</span><span class="p">;</span>
	<span class="n">fp</span><span class="p">.</span><span class="n">open</span><span class="p">(</span><span class="s">"./1.txt"</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">in</span><span class="p">);</span>
	<span class="n">fp</span> <span class="o">&gt;&gt;</span> <span class="n">pi</span><span class="p">;</span>
	<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">pi</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
	<span class="n">fp</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>
	<span class="n">std</span><span class="o">::</span><span class="n">cin</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="21-读写二进制文件">2.1 读写二进制文件</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// tbc
</span></code></pre></div></div>

<h2 id="3-字符串操作">3 字符串操作</h2>

<h3 id="31-整型转字符串-int-to-string">3.1 整型转字符串 int to string</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include &lt;iostream&gt;
#include &lt;string&gt;
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1984</span><span class="p">;</span>
	<span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">s</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">s</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
	<span class="n">cin</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="32-单引号--双引号">3.2 单引号 &amp; 双引号</h3>

<p>C++中，单引号中只能是单个字符；</p>

<p>而双引号中是字符串，并且会在末尾追加’\0’，例如”7” = ‘7’ + ‘\0’。</p>

<p>可以查看其存储空间占用字节数，下段代码的结果是：1，2，4。</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="k">sizeof</span><span class="p">(</span><span class="sc">'7'</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="k">sizeof</span><span class="p">(</span><span class="s">"7"</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="k">sizeof</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="33-字符串拼接">3.3 字符串拼接</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">s</span> <span class="o">=</span> <span class="s">"Kira Yoshikage"</span><span class="p">;</span>
	<span class="n">s</span> <span class="o">=</span> <span class="s">"I'm "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s">"."</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="34-字符复制多个">3.4 字符复制多个</h3>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">s7</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="sc">'*'</span><span class="p">);</span>
</code></pre></div></div>

<h2 id="4-vs">4 VS</h2>

<h3 id="41-快捷键">4.1 快捷键</h3>

<p><strong>多行注释：</strong> Ctrl - K &amp; Ctrl - C</p>

<p><strong>取消多行注释：</strong> Ctrl - K + Ctrl - U</p>

<h3 id="42-error-solution">4.2 error solution</h3>

<p><em>error C4996: ‘fopen’: This function or variable may be unsafe. Consider using fopen_s instead.</em></p>

<p>选择 Project - Properties - C/C++ - Preprocessor</p>

<p>Preprocessor Definitions 中添加  <code>_CRT_SECURE_NO_WARNINGS;</code></p>

<h3 id="43-etc">4.3 etc.</h3>

<p>输出重定向</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="o">&lt;&lt;</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span><span class="n">endl</span><span class="p">;</span>
</code></pre></div></div>

<p>选择 Project - Properties - Debugging</p>

<p>Command Arguments 中添加  <code>&gt;nul</code></p>

<h2 id="5-tbc">5 TBC</h2>

<p>ザ・ワールド！時よ止まれ！</p>
</p>
      -->
      <span class="post-meta">Oct 29, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/08/17/PCA.html">PCA, The Classic</a></h2>
      <!--
      <p><h1 id="pca">PCA</h1>

<p>主成分分析（Principal Component Analysis）是机器学习里一个基础的降维方法，由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/30px-Flag_of_the_United_Kingdom.svg.png" alt="" />英国人Karl Pearson于1901年提出。其实现原理是通过正交化线性变换，将数据从原来的坐标系转换到新的坐标系，<strong>我自己的理解PCA本质是坐标系旋转</strong>。</p>

<p>新坐标系由数据分布所决定。第一个新坐标轴是原始数据中方差最大的方向；第二个新坐标轴选择与第一个坐标轴正交且方差最大的方向；重复此过程直至新坐标与原坐标维度（特征）数目相同。<strong>新坐标轴的重要程度依次递减，因此能够将降维的信息损失最小化</strong>。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/480px-GaussianScatterPCA.svg.png" alt="" /></p>

<h2 id="1-pca算法">1 PCA算法</h2>

<p>以上是PCA的直观理解，具体到算法实现，如下4个步骤：</p>

<p><strong>输入</strong>：数据矩阵$A$，矩阵形状$M\times N$，代表$M$个样本、$N$个维度。</p>

<ol>
  <li>计算转置矩阵$A^T$的协方差矩阵$\sum$（因为目标是<strong>特征</strong>而非<strong>样本</strong>，所以这里是矩阵$A$的转置）；</li>
  <li>对协方差矩阵进行特征分解，$\sum=Q\Lambda Q^{-1}$；</li>
  <li>将$\Lambda​$中的特征值降序排序，再将$Q​$中每列对应的特征向量重新排列后得到矩阵$P​$；</li>
  <li>将数据转换到新的坐标系，$A’=AP$。</li>
</ol>

<p><strong>输出</strong>：新数据矩阵$A’$，矩阵形状$M\times N$，$N$个维度的主成分依次递减。</p>

<p>$\clubsuit$ 在NumPy中，如何计算<strong>协方差矩阵</strong>、进行<strong>方阵的特征值分解</strong>，参见我这篇笔记。$\rightarrow$ <a href="https://amoko.github.io/2018/08/02/NumPy-Notes.html">NumPy Notes</a></p>

<p>以上就是PCA算法的计算过程，非常简单。</p>

<p>从直观理解到算法实现之间的公式推导，待补充。</p>

<h2 id="2-python实现">2 Python实现</h2>

<h3 id="21-调包淆">2.1 调包淆</h3>

<p>以iris数据集为例，使用sklearn中的PCA函数对比变换前后的数据维度。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># -*- coding: utf-8 -*-</span>
<span class="s">"""
Created on Sun Aug 19 11:13:34 2018
@author: Yonji
"""</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c"># data</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span>

<span class="c"># model</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">X_r</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c"># show result</span>
<span class="k">print</span><span class="p">(</span><span class="s">'explained variance ratio (all 4 components): </span><span class="si">%</span><span class="s">s'</span>
      <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'navy'</span><span class="p">,</span> <span class="s">'turquoise'</span><span class="p">,</span> <span class="s">'darkorange'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">color</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colors</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">target_names</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_r</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_r</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'PCA axis 0 and 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'navy'</span><span class="p">,</span> <span class="s">'turquoise'</span><span class="p">,</span> <span class="s">'darkorange'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">color</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colors</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">target_names</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Original axis 0 and 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>结果如下图，左图是PCA的前两个主成分，右图是原始数据的前两个维度。</p>

<p><img src="/img/PCA.before.after.png" alt="" /></p>

<h3 id="22-numpy手动实现">2.2 NumPy手动实现</h3>

<p>以下是我自己写的PCA实现</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">def</span> <span class="nf">PCA_Yonji</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c"># 1</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c"># 2</span>
    <span class="n">va</span><span class="p">,</span> <span class="n">ve</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="c"># 3</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">va</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">ve</span><span class="p">[:,</span><span class="n">order</span><span class="p">]</span>
    <span class="c"># 4</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
    <span class="c"># zero-center data</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">-=</span> <span class="n">m</span>
    <span class="k">return</span> <span class="n">X_s</span>
</code></pre></div></div>

<p>同样使用上面的iris数据集，验证的结果如下图。</p>

<p>对比sklearn中的PCA函数，我们自己写的这个版本，<strong>某些维度发生了符号反转</strong>，这是因为sklearn所使用的是奇异值分解（SVD），而非特征值分解（ED）。</p>

<p><img src="/img/PCA.SVD.ED.png" alt="" /></p>

<h2 id="3-related-works">3 Related Works</h2>

<h3 id="31-lda">3.1 LDA</h3>

<p>线性判别分析（Linear Discriminant Analysis），<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/30px-Flag_of_the_United_Kingdom.svg.png" alt="" />英国人Ronald Fisher于1936年提出的一个经典分类方法。</p>

<p>LDA和PCA的相同点在于，算法本质都是线性降维。不同的是PCA是为了降维而降维，而LDA的目标是分类。经过线性变换后，让数据在低维空间上的类内方差最小、类间方差最大。</p>

<h3 id="32-ae">3.2 AE</h3>

<p>自动编码器（AutoEncoder），利用神经网络来进行<strong>非线性降维</strong>。设定隐层神经元少于输入维度，再将输入作为隐层神经元输出的拟合目标。それでは、编码过程は$Input \rightarrow HiddenLayer$です；解码过程は$HiddenLayer \rightarrow Output$です。</p>

<h2 id="4-后记">4 后记</h2>

<p>Pearson, Fisher, Hinton，一种传承<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/30px-Flag_of_the_United_Kingdom.svg.png" alt="" />，天不生大英，万古如长夜。</p>

<h2 id="reference">Reference</h2>

<p>[1] Peter Harrington（2013）机器学习实战. 人民邮电出版社. 北京</p>

<p>[2] <a href="https://ja.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">Wikipedia 主成分分析</a></p>

<p>[3] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">scikit-learn PCA</a></p>

<p>[4] <a href="https://cloud.tencent.com/developer/article/1185398">Hinton是如何理解PCA？-史春奇</a></p>

<p>[5] <a href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90">Wikipedia 线性判别分析</a></p>

<p>[6] <a href="https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80">Wikipedia オートエンコーダ</a></p>

</p>
      -->
      <span class="post-meta">Aug 17, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/08/02/NumPy-Notes.html">NumPy Notes</a></h2>
      <!--
      <p><h1 class="no_toc" id="numpy-notes">NumPy Notes</h1>

<blockquote>
  <p>一些NumPy笔记 ，Updated on Jun 12, 2019</p>
</blockquote>

<ul id="markdown-toc">
  <li><a href="#1-切片变形" id="markdown-toc-1-切片变形">1 切片&amp;变形</a></li>
  <li><a href="#2-特征值特征向量" id="markdown-toc-2-特征值特征向量">2 特征值&amp;特征向量</a></li>
  <li><a href="#3-均值标准差" id="markdown-toc-3-均值标准差">3 均值&amp;标准差</a></li>
  <li><a href="#4-协方差矩阵" id="markdown-toc-4-协方差矩阵">4 协方差矩阵</a>    <ul>
      <li><a href="#41-协方差" id="markdown-toc-41-协方差">4.1 协方差</a></li>
      <li><a href="#42-协方差矩阵" id="markdown-toc-42-协方差矩阵">4.2 协方差矩阵</a></li>
    </ul>
  </li>
  <li><a href="#5-矩阵性质" id="markdown-toc-5-矩阵性质">5 矩阵性质</a>    <ul>
      <li><a href="#51-矩阵的转置transpose" id="markdown-toc-51-矩阵的转置transpose">5.1 矩阵的转置（Transpose）</a></li>
      <li><a href="#52-矩阵的秩rank" id="markdown-toc-52-矩阵的秩rank">5.2 矩阵的秩（Rank）</a></li>
      <li><a href="#53-矩阵的逆inverse" id="markdown-toc-53-矩阵的逆inverse">5.3 矩阵的逆（Inverse）</a></li>
      <li><a href="#54-矩阵的行列式determinant" id="markdown-toc-54-矩阵的行列式determinant">5.4 矩阵的行列式（Determinant）</a></li>
    </ul>
  </li>
  <li><a href="#6-解线性方程组" id="markdown-toc-6-解线性方程组">6 解线性方程组</a></li>
  <li><a href="#7-计算两点距离" id="markdown-toc-7-计算两点距离">7 计算两点距离</a></li>
  <li><a href="#8-tbd" id="markdown-toc-8-tbd">8 TBD</a></li>
</ul>

<h2 id="1-切片变形">1 切片&amp;变形</h2>

<p>取 numpy 矩阵的子行列</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
</code></pre></div></div>

<p>改变 numpy 矩阵 shape</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">3</span><span class="p">,)</span>
</code></pre></div></div>

<h2 id="2-特征值特征向量">2 特征值&amp;特征向量</h2>

<p>对一个$n$阶方阵$A$，其一组特征值$\lambda$和特征向量$v$满足以下等式：$Av=\lambda v$。</p>

<p>在NumPy中计算方阵的特征值、特征向量的函数为:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># linear algebra eigen</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<p>返回值是两个array，依次为特征值，特征向量（<strong>注意特征向量在返回值中以列存储</strong>）。参见下面这段代码。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">values</span><span class="p">,</span> <span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">values</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">3.24642492e+01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.46424920e+00</span><span class="p">,</span>  <span class="mf">1.92979794e-15</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.09576009e-16</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vectors</span>
<span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.11417645</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7327781</span> <span class="p">,</span>  <span class="mf">0.54500164</span><span class="p">,</span>  <span class="mf">0.00135151</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.3300046</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.28974835</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.68602671</span><span class="p">,</span>  <span class="mf">0.40644504</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.54583275</span><span class="p">,</span>  <span class="mf">0.15328139</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2629515</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.8169446</span> <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.76166089</span><span class="p">,</span>  <span class="mf">0.59631113</span><span class="p">,</span>  <span class="mf">0.40397657</span><span class="p">,</span>  <span class="mf">0.40914805</span><span class="p">]])</span>
<span class="c"># check equaion</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">A_Ve</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">vectors</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">A_Ve</span>
<span class="n">array</span><span class="p">([</span> <span class="o">-</span><span class="mf">3.70665277</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.71335153</span><span class="p">,</span> <span class="o">-</span><span class="mf">17.72005028</span><span class="p">,</span> <span class="o">-</span><span class="mf">24.72674904</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lam_Ve</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vectors</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">lam_Ve</span>
<span class="n">array</span><span class="p">([</span> <span class="o">-</span><span class="mf">3.70665277</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.71335153</span><span class="p">,</span> <span class="o">-</span><span class="mf">17.72005028</span><span class="p">,</span> <span class="o">-</span><span class="mf">24.72674904</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="3-均值标准差">3 均值&amp;标准差</h2>

<p>计算均值 mean 与标准差 standard deviation。</p>

<p>均值，$\mu=\cfrac{1}{n}\sum_{i=1}^nx_i$</p>

<p>标准差，$\sigma = \sqrt{\cfrac{1}{n-1}\sum_{i=1}^n(x_i-\mu)^2}$</p>

<p>$\star$ 注意 np.std() <strong>默认使用有偏估计 ，ddof=0，即自由度为 $n$</strong>；</p>

<p>若需要计算无偏估计，即自由度为 $n-1$，则要设定参数 ddof=1。</p>

<p>$\star$ 注意方差/标准差衡量的是变量的离散程度，所以会受到数据 scale 的影响。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="mf">2.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="mf">0.5</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="mf">2.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="mf">2.8284271247461903</span>
</code></pre></div></div>

<h2 id="4-协方差矩阵">4 协方差矩阵</h2>

<h3 id="41-协方差">4.1 协方差</h3>

<p>协方差（Covariance）用于衡量两个变量之间的总体误差，反映两变量间线性相关程度。</p>

<p>对随机变量$X={x_1,x_2,\cdots,x_n}$ 和 $Y={y_1,y_2,\cdots,y_n}$，协方差 ${\rm Cov}(X,Y)$ 定义如下：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
{\rm Cov}(X,Y)&={\rm E}[(X-{\rm E}[X])(Y-{\rm E}[Y])]=E[XY]-{\rm E}[X]{\rm E}[Y]\\
&=\frac{1}{n}\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)
\end{aligned} %]]></script>

<h3 id="42-协方差矩阵">4.2 协方差矩阵</h3>

<p>协方差矩阵（Covariance matrix）记录的是是多个变量，两两之间的协方差。协方差矩阵以符号 $\sum$ 表示。</p>

<p>其$i,j$位置的元素，即是第 $i$ 个变量与第 $j$ 个变量之间的协方差。公式定义如下：$\sum_{ij}=Cov(X_i,X_j)$。</p>

<p>因此<strong>对角线上的元素是方差</strong>，对角线以外的元素才是狭义上的协方差。</p>

<p>在计算协方差矩阵时，<strong>如果你所关注的变量是属性而不是样本，则需要对矩阵进行转置</strong>。</p>

<p>在NumPy中计算协方差矩阵的函数为：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">fweights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>参数 ddof 是 delta degree of freedom的缩写，即自由度差。</p>

<p>$\star$ 注意 np.cov() <strong>默认使用无偏估计 ，ddof=1，即自由度为 $n-1$</strong>；</p>

<p>若需要计算无偏估计，即自由度为 $n-1$，则要设定参数 ddof=1。</p>

<p>参见下面这段代码。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.</span> <span class="p">],</span>
       <span class="p">[</span><span class="mf">2.</span> <span class="p">,</span> <span class="mf">8.</span> <span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">]],</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.</span>  <span class="p">],</span>
       <span class="p">[</span><span class="mf">1.</span>  <span class="p">,</span> <span class="mf">4.</span>  <span class="p">]])</span>
</code></pre></div></div>

<h2 id="5-矩阵性质">5 矩阵性质</h2>

<h3 id="51-矩阵的转置transpose">5.1 矩阵的转置（Transpose）</h3>

<p>计算矩阵矩阵 $A$ 的转置矩阵 $A^T$。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">aT</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">T</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">aT</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
</code></pre></div></div>

<h3 id="52-矩阵的秩rank">5.2 矩阵的秩（Rank）</h3>

<p>矩阵 $A$ 的秩 $rank(A)$ 代表矩阵中线性无关向量的个数，行秩与列秩相等。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">r1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
<span class="c"># 1</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">r2</span><span class="p">)</span>
<span class="c"># 2</span>
</code></pre></div></div>

<h3 id="53-矩阵的逆inverse">5.3 矩阵的逆（Inverse）</h3>

<p>可逆矩阵是针对<strong>方阵</strong>而言的。</p>

<p>对一个 $n$ 阶方阵 $A$，其逆矩阵 $A^{-1}$ 满足以下等式：$AA^{-1}=A^{-1}A=I_n$，$I_n$ 为 $n$ 阶单位矩阵。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">aI</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">aI</span>
<span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">aI</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
</code></pre></div></div>

<h3 id="54-矩阵的行列式determinant">5.4 矩阵的行列式（Determinant）</h3>

<p>矩阵的行列式是针对<strong>方阵</strong>而言的。</p>

<p>对一个 $n$ 阶方阵 $A$，以下叙述等价：</p>

<ul>
  <li>矩阵 $A$ 的行列式 $\vert A \vert\neq0$</li>
  <li>矩阵 $A$ 可逆</li>
  <li>矩阵 $A$ 满秩，即 $rank(A)=n$</li>
  <li>矩阵 $A$ 为非奇异矩阵（nonsingular matrix）</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">-</span><span class="mf">2.0</span>
</code></pre></div></div>

<h2 id="6-解线性方程组">6 解线性方程组</h2>

<p>使用Numpy解线性方程组 $AX=b$。</p>

<p>限制条件是矩阵 $A$ 必须为方阵，且为非奇异矩阵。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c">#[-1.  1.]</span>
</code></pre></div></div>

<h2 id="7-计算两点距离">7 计算两点距离</h2>

<p>两点构成一个向量，可以通过向量的范数来计算两点距离。</p>

<p>在NumPy中计算矩阵、向量范数的函数为：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>默认值</strong> ord=2，对应L2范数，即常用的欧式距离；ord=1 对应L1范数，即曼哈顿距离；ord=0 对应L0范数。</p>

<p>参见下面示例。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span>
<span class="mf">5.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="mf">2.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="mf">7.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="mf">5.0</span>
</code></pre></div></div>

<h2 id="8-tbd">8 TBD</h2>

<p>つづく</p>
</p>
      -->
      <span class="post-meta">Aug 2, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/08/01/PyTorch-Notes.html">PyTorch Notes</a></h2>
      <!--
      <p><h1 id="pytorch-notes">PyTorch Notes</h1>

<blockquote>
  <p>一些PyTorch笔记 ，Updated on Mar 22，2019</p>
</blockquote>

<h2 id="1-crossentropyloss--softmax">1 CrossEntropyLoss &amp; Softmax</h2>

<p>因为PyTorch在CrossEntropyLoss损失函数中整合了Softmax激活函数，所以对于多分类神经网络，最后一层不需要添加激活函数，只要设定神经元个数为类别个数即可。</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c"># the last layer needs no activation function</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
        
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c"># calculate loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="c"># predict</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="2-数据归一化">2 数据归一化</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
            <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])</span>
</code></pre></div></div>

<p>PyTorch在数据预处理时使用参数 transform 来定义对数据的归一化方式，将原始值域为 $[0,255]$ 的numpy数据矩阵转换为值域为 $[0,1]$ 的Tensor。</p>

<p>这里面有两个操作，第一个<code>transforms.ToTensor()</code>是 Min-Max 归一化；</p>

<p>第二个<code>transforms.Normalize()</code>则是 Z-score 归一化，两组参数均值与标准差需要计算后赋值，因为定义的是单通道图像所以均值与标准差各只有1个。</p>

<p>参见下面这段代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">255</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c"># x' = x / 255</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="s">'''
tensor([[[ 0.0000,  0.1255],
         [ 0.5020,  1.0000]]])
'''</span>
<span class="c"># x' = (x - mean) / std</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.407</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.389</span><span class="p">,))</span> 
<span class="k">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="s">'''
tensor([[[-1.0463, -0.7237],
         [ 0.2441,  1.5244]]])
'''</span>
</code></pre></div></div>

<h2 id="3-参数dim">3 参数dim</h2>

<p>PyTorch中的参数dim，就是NumPy中的参数axis，参考下面两个函数定义：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pytorch</span>
<span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># numpy</span>
<span class="n">numpy</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>
<p>对于二维矩阵，dim = 0 则以行（0维度）为轴，对各列进行“压缩”；dim = 1 则以列（1维度）为轴对各行进行“压缩”。参见下面这段代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">9.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>

<h2 id="4-tbc">4 TBC</h2>

<p>人は運命にはさからえませんから。</p>
</p>
      -->
      <span class="post-meta">Aug 1, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/07/15/Parameter-Optimization-in-Neural-Networks.html">Deep learning その 2、Parameter Optimization</a></h2>
      <!--
      <p><h1 id="deep-learning-その-2-parameter-optimization">Deep learning その 2: Parameter Optimization</h1>

<blockquote>
  <p>机器学习的一般框架是：建立模型，确定目标（损失）函数，使用最优化算法进行参数求解。</p>

  <p>那么，神经网络当然也遵循这个基本的流程。</p>
</blockquote>

<p>不同模型的损失函数评判标准不一。</p>

<p>例如感知机、SVM的损失函数仅由误分类点决定，即正确分类的点对参数更新没有帮助。</p>

<p>而Logistics回归的极大似然损失、神经网络常用的交叉熵损失则是概率模型，即使正确分类，若预测概率值与目标不相等，损失函数值仍不为零。</p>

<p>那么对于神经网络，初始化网络权重，根据具体的损失函数得到当前的loss后，要如何把loss最小化，即要如何进行网络中全部参数的求解呢？</p>

<p><strong>答案是基于梯度的迭代方法，每一层网络参数的梯度都可以通过BP算法计算得到，那么有了梯度方向，就可以使用经典的SGD进行参数的迭代更新。</strong></p>

<h2 id="1-梯度计算">1 梯度计算</h2>

<p>BP算法（Error Backpropagation），我的理解就是链式法则，不过因为前层网络的梯度对后层网络存在依赖关系，所以从后往前计算，避免重复计算。</p>

<p>假设输入为2维特征，回归问题的损失函数为MSE损失，对下面的神经网络简单推导如下。</p>

<p><img src="/img/bp.jpg" alt="" /></p>

<p>那么因为BP中的求导是基于链式法则，当网络层数过深时，为小数的梯度值不断累乘，会造成梯度弥散。</p>

<p>针对梯度弥散，解决方案有以下两个：ReLU、BN。</p>

<h3 id="relu">ReLU</h3>

<p>S型函数如Sigmoid、Tanh函数在输入过小或过大时，梯度值变得非常小，那么将其替换为ReLU族函数则不存在这个问题，虽然新的问题是ReLU易坏死。</p>

<h3 id="bn">BN</h3>

<p>也可以从scale的角度入手，将输入值拉伸到合适的范围，避免出现过大或过小值。</p>

<h2 id="2-参数を更新">2 参数を更新</h2>

<h3 id="21-sgd">2.1 SGD</h3>

<p>神经网络普遍采用SGD，即<strong>随机</strong>梯度下降（<strong>Stochastic</strong> Gradient Descent），作为最优化方法。</p>

<p>SGD是GD的变体，关于梯度下降法GD，我在这篇文章里有介绍。$\rightarrow$ <a href="https://amoko.github.io/2018/03/28/Logistic-Regression.html">Logistic Regression</a></p>

<p>GD是沿着整个训练集的梯度方向下降，而SGD则是随机挑选小批量数据进行梯度下降。</p>

<p>那么SGD这样改进的优点在哪里呢？</p>

<ul>
  <li>速度快</li>
  <li>引入随机性，一定程度上可以避免局部最小值</li>
</ul>

<p><strong>在算法实现上，有别于原始SGD，主流框架都是通过epoch来实现这种随机性。</strong>即设定一个小批量数据的数目batch_size，依次去遍历训练集，经过数次迭代后将训练集全部遍历一遍就是一个epoch。</p>

<h3 id="22-sgd-with-momentum">2.2 SGD with Momentum</h3>

<p>SGD-M的下降方向不再仅由此时的梯度决定，而是由历史梯度 $g_{t-1}$ 和此时梯度 $g_t$ 构成的动量 $m_t$ 共同决定。</p>

<p>简单讲历史梯度就是一个惯性，而当前梯度负责对这个惯性做微调。</p>

<p>定义：待优化参数 $\theta​$，目标函数 $f(\theta)​$，学习率 $\alpha​$，动量衰减系数 $\beta_1​$。</p>

<blockquote>
  <p>超参数 $\beta_1​$ 一般取值0.9</p>
</blockquote>

<p>那么更新策略如下，在每次迭代 $t$：</p>

<ol>
  <li>计算当前梯度：$g_t=\nabla f(\theta_t)​$</li>
  <li>计算当前动量（下降方向）：$m_t=\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t$</li>
  <li>更新参数：$\theta_{t+1}=\theta_t-\alpha \cdot m_t$</li>
</ol>

<h3 id="23-adagrad">2.3 AdaGrad</h3>

<p>Ada是Adaptive的缩写，AdaGrad对SGD的改进在学习率上。</p>

<p>AdaGrad引入了二阶动量以自适应不同参数的学习率，所谓二阶动量就是该参数所有历史梯度值的平方和$V_t=\sum_{i=1}^t g_i^2​$，学习率反比于此二阶动量。其作用是在参数空间中越平缓的倾斜方向，使用越大的学习率。（Deep Learning Book 原文：Greater progress in the more gently sloped directions of parameter space.）</p>

<ul>
  <li>
    <p>Q：为什么要在平缓的倾斜方向上使用大学习率？</p>

    <p>A：我的理解是加入此二阶动量后，对那些历史梯度较大的参数，降低其学习率以防止震荡；而历史梯度较小的参数即 gently sloped directions，仍可以使用大学习率进行更新。</p>

    <p>（无论优化的结果是好是坏，随着历史梯度的积累到一定程度，对应参数的更新都是趋于停滞的。简单讲就是配额制，步长配额用完的参数就停止更新。）</p>
  </li>
</ul>

<p>定义：待优化参数$\theta$，目标函数$f(\theta)$，学习率$\alpha$。</p>

<blockquote>
  <p>未引入额外超参数。</p>
</blockquote>

<p>那么更新策略如下，在每次迭代$t$：</p>

<ol>
  <li>
    <p>计算当前梯度：$g_t=\nabla f(\theta_t)$</p>
  </li>
  <li>
    <p>更新参数：$\theta_{t+1}=\theta_t-\cfrac{\alpha}{\sqrt V_t} \cdot g_t$</p>
  </li>
</ol>

<h3 id="24-adam">2.4 Adam</h3>

<p>同时引入一阶动量和二阶动量，小白用这个就行了。</p>

<p>待补充。</p>

<blockquote>
  <p>超参数 $(\beta_1, \beta_2)$ 一般取值(0.9, 0.999)</p>
</blockquote>

<h2 id="3-权重衰减">3 权重衰减</h2>

<p>权重衰减（weight decay）是优化器中的可选项，其等价于在损失函数中加入$L_2$范数作为正则项。关于正则项，我在这篇文章里有介绍。$\rightarrow$ <a href="https://amoko.github.io/2018/04/29/Generalization-in-Machine-Learning.html">Generalization in Machine Learning</a></p>

<p>为什么优化器中的权重衰减等价于$L_2​$范数正则项？</p>

<p>首先$L_2$范数的公式为$\lVert x\rVert_2=\sqrt{\sum_i x_i^2}$。</p>

<p>假设原始损失函数为 $f(\theta)$，那么加入$L_2$正则项（为方便计算对$L_2$范数做了一些变形）后损失函数为$g(\theta)=f(\theta)+\cfrac{\lambda}{2\alpha}\lVert\theta\rVert_2^2​$。</p>

<p>相应在SGD优化过程中，迭代公式变为：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\theta_{t+1} &= \theta_t-\alpha \cdot \nabla g(\theta_t)\\
&= \theta_t-\alpha\cdot(\nabla f(\theta_t)+\frac{\lambda}{\alpha}\theta_t)\\
&= \theta_t-\alpha\cdot\nabla f(\theta_t)-\lambda\theta_t
\end{align}\tag{あ} %]]></script>

<p>可以看出，加入 $L_2$ 正则项后，迭代公式的变化是多了一项 $-\lambda\theta_t$，因此将其称为权重衰减，超参数$\lambda$即是权重衰减系数。</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://github.com/exacity/deeplearningbook-chinese">Github 深度学习中文版</a></p>

<p>[2] <a href="https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95">Wikipedia 確率的勾配降下法</a></p>

<p>[3] <a href="https://pytorch.org/docs/stable/optim.html#">PyTorch Optimizer</a></p>

<p>[3] <a href="https://zhuanlan.zhihu.com/p/28060786">为什么我们更宠爱“随机”梯度下降？- 非凸优化学习之路</a></p>

<p>[4] <a href="https://zhuanlan.zhihu.com/p/32230623">Adam那么棒，为什么还对SGD念念不忘 - Juliuszh</a></p>

<p>[5] <a href="https://zh.gluon.ai/chapter_deep-learning-basics/weight-decay.html">权重衰减-动手学深度学习</a></p>

</p>
      -->
      <span class="post-meta">Jul 15, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/07/08/CNN.html">CNN Notes</a></h2>
      <!--
      <p><h1 id="cnn-notes">CNN Notes</h1>

<blockquote>
  <p>Updated in Mar 01, 2019.</p>
</blockquote>

<p>卷积神经网络（Convolutional Neural Networks），深度学习三巨头之一<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/33px-Flag_of_France.svg.png" alt="" />法国人 Yann LeCun 的杰作，在图像识别领域具有统治级的优秀表现。</p>

<p>卷积神经网络不断推陈出新，从 AlexNet (2012)、VGGNet (2014)、InceptionV2 (2015)、ResNet(2015)，现在已经趋于稳定。</p>

<p>这篇笔记简单介绍一下这些网络结构的两个共同基础，卷积层和池化层。</p>

<h2 id="1-卷积层convolution-layer">1 卷积层（Convolution Layer）</h2>

<p>$\star$ <em>使用多层神经网络的优势是可以提取输入数据（例如图像）的高维特征。</em></p>

<p>在介绍卷积层之前，我们首先来看使用普通的全连接层会怎样呢，对一张 $224\times224$ 的单通道灰度图像，一个神经元需要学习的参数量（省略偏置项）为 50176。</p>

<p>而对于卷积层，一个神经元的参数量仅为一个卷积核的大小，通常为 $3\times3,5\times5, 7\times7$，那么参数量就下降了三个数量级。</p>

<p><strong>那么卷积核是什么呢？</strong></p>

<p>卷积核本质是一个线性滤波（filter），例如均值滤波、高斯滤波等，使用卷积核以一定的步长在整个图像上滑动，就得到了图像的一个feature。因为是以同一个卷积核对整个图像的不同部分进行扫描，所以对于整个图像来说不同部分之间是<strong>权值共享</strong>的。</p>

<p>那么使用多个卷积核就能得到多个feature，再使用ReLU等非线性激活函数来达到提取图像各种高维特征的目的。</p>

<p>以下是 Stanford CS231n所给出的卷积层计算过程的例子：</p>

<p><img src="/img/conv.PNG" alt="" /></p>

<p>输入为 $5\times5\times3$ 的RBG三通道图像矩阵 $x​$。</p>

<p>卷积层有两个神经元，其卷积核分别为 $w_0$ 和 $w_1$，每个卷积核都是一个 $3\times3\times3$ 的滤波矩阵。（卷积核最后一个维度值和输入图片通道数相等，相当于每个通道上 $5\times5$ 的图片都有一个 $3\times3$ 的滤波。）</p>

<p>当然<strong>一般不同通道间是使用相同的卷积核</strong>，但无论不同通道间是否共享卷积核，一个神经元对应位置的输出值都是此卷积核在所有输入信道上的卷积值之和。</p>

<p>除了卷积核的尺寸外，对图像进行扫描还需要额外定义的两个参数：</p>

<ul>
  <li>stride，卷积核在原始图像上扫描时的步长。</li>
  <li>padding，边缘填充，在原始图像边缘填充的个数。</li>
</ul>

<p><strong>PyTorch中的卷积层</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>以上是PyTorch中一个2D的卷积层定义，必须指定的参数有三个：in_channels, out_channels, kernel_size。</p>

<ul>
  <li>in_channels，输入信道，上一层神经元的个数。若为第一层，则对于RGB三通道图像数据，这个值为3。</li>
  <li>out_channels，输出信道，此卷积层神经元的个数，即卷积核的数目。</li>
  <li>kernel_size，卷积核尺寸。</li>
</ul>

<p>假设输入图像为 $C_{in}\times W_1\times H_1​$；</p>

<p>定义卷积层神经元个数为 $C_{out}$，每个神经元卷积核尺寸为$F\times F$，填充位数padding为$P$，步长stride为$S$；</p>

<p>则此卷积层的输出维度为$W_2\times H_2\times K​$，其中$W_2=\cfrac{W_1-F+2P}{S}+1, H_2=\cfrac{H_1-F+2P}{S}+1​$。</p>

<p>那么如果想让卷积后的图片尺寸保持不变，一般设$S=1,P=\cfrac{F-1}{2}​$。</p>

<h2 id="2--池化层pooling-layer">2  池化层（Pooling Layer）</h2>

<p>池化层的作用是降维，池化层并没有需要学习的参数，因此一般讲并不计算在网络层数中。</p>

<p>降维对于神经网络这样参数量较大的模型优点有两个：一是使优化更容易，二是提升模型的泛化能力。</p>

<p>池化的方式有很多，常用的是 max pooling，即选择区域内最大的元素值作为对应位置的输出。</p>

<p><img src="/img/pool.PNG" alt="" /></p>

<p><strong>PyTorch中的池化层</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>以上是PyTorch中一个2D的最大值池化层定义，必须指定的参数有两个： kernel_size, stride。</p>

<ul>
  <li>kernel_size，卷积核尺寸。</li>
  <li>kernel_size，步长。</li>
</ul>

<p>$\star$ 注意，如果池化后的维度数不是整数，会向下取整。</p>

<p>例如 <code>nn.MaxPool2d(2, 2)</code>，尺寸 $7\times7$ 的图像池化后变为 $3\times3$ 。</p>

<h2 id="3-pytorch-demo">3 PyTorch demo</h2>

<p>下段代码定义了一个拥有3个卷积层、3个全连接层，总深度为6的神经网络。</p>

<p><strong>目的是展示一下，如果不使用padding的话，计算feature map的size是一件多么让人捉急的事。</strong></p>

<p>输入数据为$32\times32$的三通道图像，即输入维度$3\times32\times32$；多分类标签有10个，即输出维度为10。</p>

<p>我们需要计算最后一层卷积层的输出维度$C\times H\times W​$，来给全连接层的输入维度赋值，以下为此维度的计算过程。</p>

<ul>
  <li>Step1，第1层卷积神经元个数为6，卷积核为$3\times3$，则此层输出维度为$6\times30\times30$；</li>
  <li>Step2，第2层卷积神经元个数为12，卷积核为$3\times3$，则此层输出维度为$12\times28\times28$；</li>
  <li>Step3，池化层池化核为$2\times2$，步长为2，则此层输出维度为$12\times14\times14$；</li>
  <li>Step4，第3层卷积神经元个数为24，卷积核为$3\times3$，则此层输出维度为$24\times12\times12$；</li>
  <li>Step5，池化层池化核为$2\times2$，步长为2，则此层输出维度为$24\times6\times6$。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c"># C*H*W</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">24</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c"># 1-2</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c"># 3</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c"># reshape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">6</span><span class="p">)</span>
        <span class="c"># fully connected        </span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h2 id="reference">Reference</h2>

<p>[1] 李航 (2012) 统计学习方法. 清华大学出版社, 北京.</p>

<p>[2] <a href="http://cs231n.github.io/convolutional-networks/">CS231n: CNN for Visual Recognition </a></p>

<p>[3] <a href="https://pytorch.org/docs/stable/nn.html#conv2d">PyTorch nn.Conv2d</a></p>

<p>[4] <a href="https://pytorch.org/docs/stable/nn.html#maxpool2d">PyTorch nn.MaxPool2d</a></p>
</p>
      -->
      <span class="post-meta">Jul 8, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/07/03/Code-Editor-Config.html">Sublime Text, Spyder Configuration</a></h2>
      <!--
      <p><h2 id="1-sublime-text-3">1 Sublime Text 3</h2>

<h3 id="11-添加新的build-system">1.1 添加新的Build System</h3>

<p>Tools - Build System- New Build System</p>

<p>粘贴以下配置（注意python安装路径）</p>

<p><strong>Linux</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"shell_cmd"</span><span class="p">:</span> <span class="s">"/home/you/anaconda3/bin/python -u </span><span class="se">\"</span><span class="s">$file</span><span class="se">\"</span><span class="s">"</span><span class="p">,</span>
    <span class="s">"selector"</span><span class="p">:</span> <span class="s">"source.python"</span><span class="p">,</span>
    <span class="s">"file_regex"</span><span class="p">:</span> <span class="s">"^[ ]*File </span><span class="se">\"</span><span class="s">(...*?)</span><span class="se">\"</span><span class="s">, line ([0-9]*)"</span>
<span class="p">}</span>
</code></pre></div></div>
<p><strong>Windows</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"cmd"</span><span class="p">:</span> <span class="p">[</span><span class="s">"C:/Users/you/Anaconda3/python"</span><span class="p">,</span> <span class="s">"-u"</span><span class="p">,</span> <span class="s">"$file"</span><span class="p">],</span>
    <span class="s">"selector"</span><span class="p">:</span> <span class="s">"source.python"</span><span class="p">,</span>
    <span class="s">"file_regex"</span><span class="p">:</span> <span class="s">"^</span><span class="se">\\</span><span class="s">s*File </span><span class="se">\"</span><span class="s">(...*?)</span><span class="se">\"</span><span class="s">, line ([0-9]*)"</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="12-添加sublimerge插件">1.2 添加sublimerge插件</h3>

<p>Preferences - Package Control - Install Package</p>

<p>搜索 sublimerge 安装。</p>

<h3 id="13-查看文件编码">1.3 查看文件编码</h3>

<p>Preference - Settings</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s2">"show_encoding"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
</span></code></pre></div></div>

<h2 id="2-spyder">2 Spyder</h2>

<h3 id="21-添加模块自动补全">2.1 添加模块自动补全</h3>

<p>~\Anaconda3\Lib\site-packages\spyder\utils\introspection\module_completion.py</p>

<p>将需要自动补全的模块名添加到mods变量中。</p>
</p>
      -->
      <span class="post-meta">Jul 3, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/06/20/Jung-&amp;-Freud.html">Jung &amp; Freud</a></h2>
      <!--
      <p><h1 id="jung--freud">Jung &amp; Freud</h1>

<blockquote>
  <p>西格蒙德·弗洛伊德 Sigmund Freud （1856-1939），心理学家，出生于<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Flag_of_the_Habsburg_Monarchy.svg/38px-Flag_of_the_Habsburg_Monarchy.svg.png" alt="" />奥地利。</p>
</blockquote>

<p>弗洛伊德的大名，相信每个现代人都有所耳闻。他所提出的一些理论，也通过参差的信息源有简单的了解。</p>

<p>如潜意识，弗洛伊德认为人无法全然意识到自我，人的意识就像一座冰山，露出水面的只是一小部分，水面下无法被认知却真实存在的就是潜意识。</p>

<p>诸如本我（id），自我（ego），超我（superego）人格构成的三个概念。</p>

<p>对于梦的解析，弗洛伊德认为梦是一种被压抑的愿望的隐晦表达 ，是对潜意识的一种欺骗性满足。</p>

<p>当然弗洛伊德最著名的是他对于性的理解，他认为性欲是人最原始的驱动力。</p>

<blockquote>
  <p>卡尔·荣格 Carl Jung （1875-1961），心理学家，出生于<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/30px-Flag_of_Switzerland.svg.png" alt="" />瑞士。</p>
</blockquote>

<p>而对于荣格，此前仅仅是听过他的名字。偶然对荣格有了进一步的认识，是前段时间看了 Jordan B. Peterson的课程视频。</p>

<p><a href="https://www.bilibili.com/video/av21066202">人格与其转变（Personality and its Transformations）——Jordan Peterson 2017</a></p>

<p>荣格提出了集体潜意识，原型（Archetype）开创了分析心理学。</p>

<p>荣格早期追随弗洛伊德的学说，但后来由于理念上的不同与之决裂。那么造成最终决裂的分歧在哪里呢？这也是今天这个博文的写作初衷。</p>

<p>荣格的不满，在于弗洛伊德片面的因果论方法。</p>

<p>荣格认为弗洛伊德因为还原论和因果论的倾向，导致他完全无视一切心理事件中目的性指向明确的做法。</p>

<p>荣格对于人类的看法包含目的论和因果论，他认为人类行为不仅由个人及种族的历史，更由对生活的期望所形成。也就是说，一个人的行为同时受过去和未来的影响。为了正确理解一个人的行为，除了需要考虑过去的事件以外，也还需要考虑到他未来的目标。</p>

<p>相应地荣格认为梦是集体潜意识的一种表达，对未来有着启示和帮助作用，而不是对个体潜意识的欺骗性满足。</p>
</p>
      -->
      <span class="post-meta">Jun 20, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/06/16/Activation-Function-in-Neural-Networks.html">Deep learning その 1、Activation Function</a></h2>
      <!--
      <p><h1 id="deep-learning-その-1-activation-function">Deep learning その 1: Activation Function</h1>

<blockquote>
  <p>神经网络中，一个节点的<strong>激活函数</strong>定义了此节点输入与输出之间的映射关系。</p>
</blockquote>

<h3 id="0-perception">0 Perception</h3>

<p>神经网络（Neural Networks）与感知机（Perceptron）很大的区别，或者说改进就在于<strong>激活函数</strong>的不同。</p>

<p>简单介绍一下，感知机由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/38px-Flag_of_the_United_States.svg.png" alt="" />美国心理学家Frank Rosenblatt 于1958年提出，是一种处理二分类问题的线性模型，同时也是SVM、Logistics回归和神经网络的基础。模型如下：</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Perceptron.svg/750px-Perceptron.svg.png" alt="" /></p>

<p>对于线性不可分的数据，感知机模型无法收敛。因为感知机的激活函数$f(x)$是Sign函数是线性的。Sign函数，也称单位跃迁函数，常用形式如下：
<script type="math/tex">f(x)={\rm sign}(x)=\begin{cases}+1,x>0\\-1,x\leq0\end{cases}\tag{1}</script>
而神经网络常使用如Sigmoid、Tanh、ReLU等非线性函数作为神经元的激活函数。</p>

<p>为什么要使用非线性的激活函数，因为<strong>若激活函数为线性，则无论神经网络有多少层，输出都是输入的线性组合，多层没有任何意义</strong>。因此引入非线性的激活函数，神经网络才有理论上拟合任意函数的能力。</p>

<h3 id="1-sigmoid函数">1 Sigmoid函数</h3>

<p>Sigmoid函数，也称S型函数，形式如下：</p>

<script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}\tag{2}</script>

<p>Sigmoid函数将任意范围内的输入映射在[0, 1]区间内。优点是求导简单，$f’(x)=f(x)(1-f(x))$。</p>

<p>但缺点是因为S型函数的关系，在输入值较大或较小时梯度值过小。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sg</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'x={:2d}, g={:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">sg</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="s">'''
x= 0, g=0.2500
x= 2, g=0.1050
x= 4, g=0.0177
x= 6, g=0.0025
x= 8, g=0.0003
x=10, g=0.0000
'''</span>
</code></pre></div></div>

<h3 id="2-tanh函数">2 Tanh函数</h3>

<p>Tanh函数，即双曲正切函数，形式如下：
<script type="math/tex">f(x)=\tanh(x)=\frac{\sinh(x)}{\cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}\tag{3}</script></p>

<p>Tanh函数与Sigmoid函数同为S型函数，区别是其将输出映射在[-1, 1]区间内。</p>

<h3 id="3-relu函数">3 ReLU函数</h3>

<p>线性整流函数（Rectified Linear Unit）,也称斜坡函数，形式如下：
<script type="math/tex">f(x)=\max(0,x)\tag{4}</script></p>

<p>现在广泛使用的是ReLU函数，其优点是收敛速度更快，避免了梯度消失，计算简单，缺点是比较脆弱。</p>

<h3 id="4-softmax函数">4 Softmax函数</h3>

<p>Softmax函数是Sigmoid函数的泛化，常用于多分类神经网络的最后一层。</p>

<p>Softmax函数的作用是将输出向量归一化，并且让归一化后的值大的更大、小的更小。</p>

<p>将一个元素为任意实数的$n$维向量$\vec{z}$压缩为一个同为$n$维的向量$\vec{a}$，向量$\vec{a}$中每一个元素范围为(0, 1)，且所有元素和为1。常用形式如下：</p>

<script type="math/tex; mode=display">a_i(\vec{z})=\cfrac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}，{\rm for}\quad i=1,\dots,n\tag{5}</script>

<h3 id="reference">Reference</h3>

<p>[1] <a href="https://en.wikipedia.org/wiki/Activation_function">Wikipedia Activation Function</a></p>

<p>[2] <a href="https://www.jiqizhixin.com/articles/2018-01-15-2">从感知机到深度神经网络-机器之心</a></p>

<p>[3] <a href="http://www.shuang0420.com/2017/01/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/">深度学习-从线性到非线性-徐阿衡</a></p>
</p>
      -->
      <span class="post-meta">Jun 16, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/05/25/Intro-to-CI.html">An Intro to Causal Inference &amp; do-Calculus</a></h2>
      <!--
      <p><h1 id="超越机器学习因果推断与do-算子介绍">超越机器学习：因果推断与do-算子介绍</h1>

<blockquote>
  <p>本文翻译自剑桥大学博士Ferenc Huszár的博文，原文标题为：ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus，<a href="http://www.inference.vc/untitled/">原文链接</a>。</p>
</blockquote>

<p>你或许偶然看到了Judea Pearl的新书（The Book of Why，2018年5月出版），并且与之相关的采访在社交网站上被点赞之交们广泛分享。Pearl在采访里把我们在机器学习上的大多数工作贬为仅仅在做曲线拟合。虽然我相信这是夸大其词的说法，但大多数有意义的辩论常常是由富有争议性或者自大的言论引起的。最近一个很好的例子就是将机器学习比作炼金术。在读过文章后，我决定再次研究一下Pearl著名的“do-算子”和因果推断。</p>

<p>之所以说再一次，是因为我之前已经接触过很多次了。第一次学到“do-算子”是在本科时期一次贝叶斯网络的课上，这门课不怎么受欢迎但是很高级就行了。自此，我每隔2到3年都要在不同的领域接触到“do-算子”，但我从来没有真正对此感兴趣过。我的想法仅仅是“这玩意太难了吧，也许还没什么卵用”，最终都是与之擦身而过。直到现在，我才意识到这个东西是有多么的基础。</p>

<p>p3：我信了，我入教。</p>

<p>p4：不但入教，我还要传教。</p>

<h2 id="1-基础知识">1 基础知识</h2>

<p>首先，causal calculus 差别对待两种看似一样的条件分布。太长不看版：机器学习中，我们通常只关注其中一个，但一些应用场景下我们实际上应该试着去估计另一个，或者说我们必须这么做。</p>

<p>

假设我们有从$p(x,y,z,\dots)$得到的独立同分布的数据抽样，我们有大量的数据和最好的工具（深度学习）来充分估计它们的联合分布或者是条件分布、边缘分布这样的属性，简单讲就是$p$已知且易处理。我们感兴趣的是在给定$x$的条件下，变量$y$的表现。高级一点的说法，你可以以两种方式问这个问题：$p(y|x)$，$p(y|do(x))$。

观察型 $p(y|x)$：观察到变量$X$取值为$x$时$Y$的概率分布。对于有监督机器学习，我们通常是这么做的。这是一个基于$p(x,y,z,\dots)$可以被计算的条件分布，即两个边缘分布的比值，$p(y|x)=\cfrac{p(x,y)}{p(x)}$。我们对这个分布和计算方法非常熟悉。

干预型 $p(y|do(x))$：将变量$X$强行设置为$x$时$Y$的概率分布。这个描述的是如果我强行干预$X$的取值为$x$，模拟其他变量按照原始机制产生相应数据时$Y$的分布。（注意这里的数据生成部分和$p(x,y,z,\dots)$是不同的，这一点非常重要）。

</p>

<h2 id="2-这两个有什么不一样">2 这两个有什么不一样？</h2>

<p>

不，很不一样。你可以通过做几组实验来验证这一点。假设$Y$是我咖啡机水壶内的压力，取值范围为0到1.1，取决于运行时间。$X$是咖啡机内置气压计的显示值。在随机时间同时观察$X$，$Y$。合适的气压计函数$p(y|x)$应当是一个均值为$x$的单峰（unimodal）分布，随机性由测量误差体现。然而，$p(y|do(x))$实际上并不依赖于$x$的取值，其大体上等于水壶压力的边缘分布$p(y)$。因为我们人工设置了气压计的显示值（比如将指针固定），这并不会改变水壶内真实的压力值。

</p>

<p>总结一下，$y$和$x$是相关的或者说统计上是有依赖关系的，因此可以由观察到的$x$去预测$y$。但$y$的值并不是由$x$决定的，所以设定$x$的值并不会对$y$的分布造成影响。这个例子只是冰山一角，在多个变量存在复杂交互影响时，观察型条件性和干预型条件性的差别可以是更加细微和难以描述的。</p>

<h2 id="3-我想要哪一个">3 我想要哪一个？</h2>

<p>

取决于你想要解决的问题，你应该设法去估计其中的一个。如果你的终极目标是诊断或者预测（即以观察到的自然产生的$x$来推测$y$的值），那么你需要的就是$p(y|x)$。这就是我们在有监督学习中所做的事，也就是 Juda Pearl 所说的曲线拟合。这适用于一系列重要应用：分类、图像分割、高分辨率成像、语音识别和机器翻译等等。



如果你的终极目标是基于预计的条件分布来控制或者选择$x$的值，那么你应该使用$p(y|do(x))$。例如，如果$x$是药物治疗，$y$是治疗结果，那么你并不仅满足于观察自然产生的治疗$x$和结果预测。我们想要的是在$x$会影响$y$的假设下，来主动选择$x$。相似的应用场景有系统识别、控制和在线推荐系统。

</p>

<h2 id="4-pydox到底是什么">4 $p(y|do(x))$到底是什么？</h2>

<p>

这或许是我以前没有理解的主要概念。$p(y|do(x))$实际上是一个香草（vanilla）条件分布，但并不基于$p(x,y,z,\dots)$计算，相反是一个不同的分布$p_{do(X=x)}(x,y,z,\dots)$。$p_{do(X=x)}$就是假如我们采取干预后得到的数据分布。$p(y|do(x))$就是我们在随机控制实验中控制变量$x$后得到的条件分布。需要注意的是真的去采取干预或者随机实验或许是不可能的，或者说至少在实践层面或者伦理层面是是不可能的。即使你不能在随机实验上直接估计$p(y|do(x))$，但这个问题依然存在。因果推断和“do-算子”的重心就是：如果我不能通过随机控制实验测量$p(y|do(x))$，那么我可以从无干预的观察数据上去估计它吗？

</p>

<h2 id="5-所有这些是怎么联系到一起的">5 所有这些是怎么联系到一起的？</h2>

<p>

让我们从这副图开始，如果我们只关心$p(y|x)$，即下面这个简单的有监督学习例子：

</p>

<p><img src="http://www.inference.vc/content/images/2018/05/Causality-0_-just-observational.png" alt="" /></p>

<p>

假设我们观察到3个变量$x,y,z$，观察数据是联合分布的独立同分布抽样。我们的关注点是以$x$预测$y$，$z$变量同样可以测量但不进行操作（考虑到完整性）。观察到的条件概率$p(y|x)$可以从联合分布计算得到。具体可以使用深度学习来最小化交叉熵或者其他方法，从训练数据构建一个模型$p(y|x,\theta)$来近似这个条件概率。



如果我们关心的是$p(y|do(x))$，如下图

</p>

<p><img src="http://www.inference.vc/content/images/2018/05/Causality-2_-two-distros.png" alt="" /></p>

<p>

我们仍有观察到的联合分布（蓝色所示），数据同样是此联合分布的抽样。然而我们希望估计的是右下角的干预概率$p(y|do(x))$。这取决于上方的因子图（红色所示）。这是一个源自$p(x,y,z,\dots)$但与之并不相同的联合分布。如果我们可以从这个红色分布上抽样（即以$x$为控制变量做一次随机控制实验），那么这个问题也可以被有监督学习解决。然而，这不现实，我们仅有从蓝色的分布上抽样的数据。我们不得不去尝试从蓝色的分布上来估计条件概率$p(y|do(x))$。

</p>

<h3 id="因果模型">因果模型</h3>

<p>如果我们想要在蓝色分布与红色分布间建立联系，必须对数据产生机制的因果结构引入一些假设。只有在知道变量间因果关系的前提下，才能对干预后的分布做出预测。从联合分布来对因果关系建模是不够的，必须引入表达能力更强的工具。如下所示：</p>

<p><img src="http://www.inference.vc/content/images/2018/05/Causality_-building-a-bridge--1-.png" alt="" /></p>

<p>除联合分布外，我们现有还有一个因果模型（左上）。因果模型包含联合分布更多的细节：不仅知道压力和气压计显示值是由依赖关系的，而且知道这个依赖关系的确切结构。模型中的箭头意味着因果关系的方向，如果变量之间不存在箭头，则无因果关系。因果图和联合分布之间的映射是多对一关系：几个不同的因果结构可能具有相同的联合分布。因此，仅从观察数据获得确定唯一的因果结构是不可能的。</p>

<p>

构建一个因果模型，我们必须考虑关于世界如何运行，什么造成了什么这样的假设。一旦我们有了一个因果图，我们可以通过改动因果图（删去所有指向$do$操作节点的边）对干预的结果进行仿真，如上图（中上）所示。绿色因子图就是这个改动过的因果图所对应的联合分布。相应的条件概率是$\tilde p(y|do(x))$，作为$p(y|do(x))$的近似。如果我们的因果结构是正确的，那么$\tilde p(y|do(x))=p(y|do(x))$。错误的因果结构假设会导致错误的近似结果。

</p>

<p>为了得到绿色的部分来建立观察数据与干预结果之间的桥梁，我们不得不引入额外的假设，即先验知识。仅仅有数据是不够的。</p>

<h3 id="do-算子">do-算子</h3>

<p>现在的问题是，我们在仅有蓝色分布数据的状况下，怎么有资格对绿色分布BB什么呢。比起之前将两者relating起来的因果模型，我们现在的状况已经好多了。长话短说，这就是“do-算子”的意义所在。在有蓝色分布的边缘、条件、期望等属性的情况，“do-算子”允许我们对绿色分布做个马杀鸡。”do-算子“通过引入4个额外的规则扩展了我们处理条件概率分布的工具箱。这些规则和因果图的属性相关，这篇博文无法将其全部阐明，但你可以读一下这篇<a href="https://arxiv.org/abs/1305.5506">入门论文</a>。</p>

<p>

理想情况下，“do-算子”的结果是一个公式$\tilde p(y|do(x))$，内部没有任何do操作，你可以在观察数据上单独估计它。这是$\tilde p(y|do(x))$可识别的情况。那么相反的，如果这是不可行的，无论我们怎么去尝试，也无法得到可识别的结果，这意味着我们无法从既有数据上去估计它。下面的图总结了因果推断机制 in its full glory。

</p>

<p><img src="http://www.inference.vc/content/images/2018/05/Causality_-do-calculus-estimand--1-.png" alt="" /></p>

<p>

注意，如果你只关心$p(y|x)$ 那么变量$z$是完全是无关变量，你仍可以做有监督学习。但在因果推断中，如果我们不能观察到$z$，那么将不能够处理$p(y|do(x))$。

</p>

<h2 id="6-我怎么知道我的因果模型对不对">6 我怎么知道我的因果模型对不对？</h2>

<p>你永远也无法从观测数据来充分验证一个因果模型的正确性和完整性。然而因果模型的一些aspects是可以通过实验验证的。具体来讲，因果模型蕴含着变量集之间的条件独立性和依赖关系。这些独立和依赖关系是可以被实验验证的，如果说你模型中的这些关系与数据相悖，说明你的因果模型是错的。带着这个观点你可以试着去做完整的因果发现：尝试从数据来推断因果模型，或者至少因果模型的一些aspects。</p>

<p>但底线是：a full causal model is a form of prior knowledge that you have to add to your analysis in order to get answers to causal questions without actually carrying out interventions.  单单依靠数据推理不能使你达到这一步。和贝叶斯推断不同，其先验知识是锦上添花并且可以提升数据效果的，而因果图对于因果推断是必须的。</p>

<h2 id="7-总结">7 总结</h2>

<p>因果推断毫无疑问是很基础的东西。它可以让我回答”what-if-we-did-x”这样的问题，而这通常需要随即控制实验和精准的干预。 And I haven’t even touched on counterfactuals which are even more powerful.在一些情况下，你可以不关系因果推断。但存在一些应用场景，你必须使用因果推断才能解决问题。</p>

<p>我再次强调这篇博文不是关于你应该做因果推断还是深度学习。你可以，并且应该两者都做。 因果推断和“do-算子”可以让你理解问题并基于一些假设从数据中建立一个因果图模型。在实践中，你仍需要强大的工具（深度学习、SGD、变分等等）来帮助你完成工作。</p>
</p>
      -->
      <span class="post-meta">May 25, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/04/29/Generalization-in-Machine-Learning.html">Generalization in Machine Learning</a></h2>
      <!--
      <p><h1 id="generalization-in-machine-learning">Generalization in Machine Learning</h1>

<blockquote>
  <p>机器学习中的泛化能力，指模型对未知数据（测试集）的拟合能力。</p>
</blockquote>

<p>从统计学习的角度出发，具体的学习策略有两种方案：经验风险最小（ERM）和结构风险最小（SRM）。</p>

<ul>
  <li>经验风险最小 Empirical Risk Minimization， 仅考虑模型在已知数据（训练集）上的表现，经验风险最小的模型就是最优的模型。在数据量足够大的，ERM能够保证有很好的学习效果；但是当数据量不足时，ERM学习出来的模型往往会出现过拟合的现象，即仅在训练集上表现好，而在测试集上表现差。</li>
  <li>结构风险最小 Structural Risk Minimization，是为了防止过拟合而提出的策略，在ERM的基础上加入正则项（惩罚项）对模型的复杂度进行约束，以提升模型的泛化能力。</li>
</ul>

<p>以下图的二分类问题为例，以ERM的标准，倾向于绿色曲线；而以SRM的标准，黑色曲线则是最优选择。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/450px-Overfitting.svg.png" alt="" /></p>

<p>以下我将简单介绍在机器学习的不同阶段，评估/提升模型泛化能力的方法。</p>

<h3 id="1-模型训练">1 模型训练</h3>

<p>模型训练阶段，从SRM的角度考虑，需要对模型加一个正则项。正则项的选取，常用的有L1、L2范数。</p>

<p>L1范数，也称为曼哈顿范数，为参数向量元素绝对值之和。以L1范数作为正则项，目的是使模型稀疏化，防止模型参数过多。例如 lasso regression 就是在线性回归的基础上加L1范数作为正则项。</p>

<script type="math/tex; mode=display">\lVert x \rVert_1 = \sum_i \vert x_i \rvert \tag{1}</script>

<p>L2范数，也称为欧几里得范数，为参数向量元素绝对值平方和再开方。以L2范数作为正则项，防止参数过拟合到某个特征上。例如 ridge regression 就是在线性回归的基础上加L2范数作为正则项。
<script type="math/tex">\lVert x \rVert_2 =\sqrt{ \sum_i x_i^2}\tag{2}</script></p>

<h3 id="2-模型选择">2 模型选择</h3>

<p>模型选择考虑的是不同的模型之间的优劣。</p>

<p>从ERM的角度考虑，拟合数据最好的模型，即似然函数值$\hat L$最大的模型是最好的。那么从SRM的角度考虑，是寻求一个模型复杂度与拟合数据之间的最佳平衡。以SRM策略为目标的评分准则，常用的有AIC、BIC评分。</p>

<p>Akaike Information Criterion 由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/38px-Flag_of_Japan.svg.png" alt="" />日本人<ruby>赤池<rt>あかいけ</rt></ruby>（ Akaike）于1974年提出，$k$是模型参数的个数，$\hat L$是似然函数值，AIC评分越小模型越好。
<script type="math/tex">{\rm AIC}=2k-2\ln(\hat L)\tag{3}</script>
Bayesian Information Criterion 由Schwarz 于1978年提出，$k$是模型参数的个数，$n$是样本数量，$\hat L$是似然函数值，BIC评分越小模型越好。对比AIC，BIC的改进是考虑了样本数量$n$，大数据量时，惩罚项更大。
<script type="math/tex">{\rm BIC}=k\ln(n)-2\ln(\hat L)\tag{4}</script></p>

<h3 id="3-bagging">3 Bagging</h3>

<p>Bagging是集成学习（ensemble learning）策略的一种。</p>

<p>集成学习的思想是利用多个弱分类器组合一个强分类器，主要有bagging和boosting两种策略。Boosting的目标是提升分类器拟合能力，而bagging则是为了提升分类器的泛化能力。</p>

<p>Bagging策略的做法是对样本均匀随机重采样，对每一份重采样得到的子样本训练一个模型，利用得到的N个模型进行最终的结果预测。</p>

<h3 id="reference">Reference</h3>

<p>[1] 李航 (2012) 统计学习方法. 清华大学出版社, 北京.</p>

<p>[2] <a href="https://en.wikipedia.org/wiki/Overfitting">Wikipedia Overfitting</a></p>
</p>
      -->
      <span class="post-meta">Apr 29, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/04/27/LR-AR.html">Logistic Regression via Association Rule</a></h2>
      <!--
      <p><h1 id="model-selection-for-lr-via-ar">Model selection for LR via AR</h1>

<p>Changpetch P, Lin DK. Model selection for logistic regression via association rules analysis. Journal of Statistical Computation and Simulation. 2013 Aug 1;83(8):1415-28.</p>

<p>一句话介绍，为了逻辑回归有更好的结果，使用关联规则产生新的特征以生成一个最优模型。</p>

<h3 id="motivation">Motivation</h3>

<p>某些变量间存在相互作用(interaction)，需要将变量和变量之间的相互作用同时加入LR模型。</p>

<p>使用关联规则来探索变量间的相互作用。</p>

<h3 id="算法">算法</h3>

<ol>
  <li>
    <p><strong>产生关联规则</strong></p>

    <p>CBA算法，mini_support = 0.1，mini_confidence = 0.8</p>

    <p>rule example: $X_1=0 \&amp; X_2=1\rightarrow Y=0$</p>

    <p>​</p>
  </li>
  <li>
    <p><strong>规则筛选</strong></p>

    <p>选择30~50个置信度最高的规则。</p>

    <p>​</p>
  </li>
  <li>
    <p><strong>产生特征</strong></p>

    <p>将关联规则后项去除，即为新的特征。</p>

    <p>feature example: $X_1(0)X_2(1)$</p>

    <p>​</p>
  </li>
  <li>
    <p><strong>模型搜索</strong></p>

    <p>根据上面产生的特征生成最终的模型，使用AIC(Akaike information criterion; Akaike 1974)作为模型选择的损失函数进行搜索。</p>

    <p>或者使用lasso(least absolute shrinkage and selection operator; Tibshirani 1996)、SCAD(smoothly clipped absolute deviation; Fan 1997)、BIC(Bayesian information criterion; Schwarz 1978)等方法。</p>
  </li>
</ol>

<h3 id="实验">实验</h3>

<p>论文所使用的数据集是CMU于1991年提出的MONK数据集，二分类问题，每个样本具有6个离散值的属性，数据集共包含432个样本，即包含所有可能出现的组合情况（$3\times3\times2\times3\times4\times2=432$）。</p>

<table>
  <thead>
    <tr>
      <th>attribute</th>
      <th>values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>head_shape</td>
      <td>{round, square, octagon}</td>
    </tr>
    <tr>
      <td>body_shape</td>
      <td>{round, square, octagon}</td>
    </tr>
    <tr>
      <td>is_smiling</td>
      <td>{yes, no}</td>
    </tr>
    <tr>
      <td>holding</td>
      <td>{sword, balloon, flag}</td>
    </tr>
    <tr>
      <td>jacket_color</td>
      <td>{red, yellow, green, blue}</td>
    </tr>
    <tr>
      <td>has_tie</td>
      <td>{yes, no}</td>
    </tr>
  </tbody>
</table>

<p>MONK数据集由独立的三个数据集构成，每个数据集的标签分类规则不同。论文仅使用第一个数据集monks-1，此数据集标签按照如下标准划分：</p>

<p>For a sample, if <em>head_shape = body_shape</em> <strong>or</strong> <em>jacket_color = red</em>,  it’s in Class 1; otherwise it’s in Class 0.</p>

<p>测试集即为有标签的全部432个样本，训练集为从中随机选取的124个样本，无噪声。</p>

<p><strong>这篇论文对离散变量的编码方式有点奇怪。</strong></p>

<p>以上数据集为例，其使用11个二值变量编码6个原始变量。</p>

<p>若目的是将多值离散变量进行二值化，直接使用 One-Hot 编码也仅需要17个编码位，为什么不直接使用One-Hot？</p>

<table>
  <thead>
    <tr>
      <th>attribute</th>
      <th>variables {values}</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>head_shape</td>
      <td>$X_1,X_2$ {1*, 01, 00}</td>
    </tr>
    <tr>
      <td>body_shape</td>
      <td>$X_3,X_4$ {1*, 01, 00}</td>
    </tr>
    <tr>
      <td>is_smiling</td>
      <td>$X_5$ {1, 0}</td>
    </tr>
    <tr>
      <td>holding</td>
      <td>$X_6,X_7$ {1*, 01 , 00}</td>
    </tr>
    <tr>
      <td>jacket_color</td>
      <td>$X_8,X_9,X_{10}$ {1**, 01*, 001, 000}</td>
    </tr>
    <tr>
      <td>has_tie</td>
      <td>$X_{11}$ {1, 0}</td>
    </tr>
  </tbody>
</table>

<h3 id="实验结果">实验结果</h3>

<p>模型</p>

<p><img src="/img/monk.model.PNG" alt="模型" /></p>

<p>复现结果</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># LR.AR model 4</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">^</span><span class="n">e</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="n">e</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="n">e</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">11</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/img/monk.result.PNG" alt="复现结果" /></p>

<h3 id="reference">Reference</h3>

<p>[1] <a href="https://archive.ics.uci.edu/ml/datasets/MONK's+Problems">UCI上MONK数据集主页</a></p>

<p>[2] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">scikit-learn OneHotEncoder</a></p>

<p>[2] <a href="https://amoko.github.io/2018/03/28/Logistic-Regression.html">Yonji’ Blog Logistic Regression</a></p>
</p>
      -->
      <span class="post-meta">Apr 27, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/04/24/Pareto-Distribution.html">Pareto Distribution</a></h2>
      <!--
      <p><h1 id="pareto-distribution">Pareto Distribution</h1>

<p>高斯分布 Gaussian Distribution 与帕累托分布 Pareto Distribution 并列为两大主导自然和人类世界的概率分布。</p>

<h3 id="高斯分布">高斯分布</h3>

<p>根据中心极限定理，大量独立随机变量的均值收敛于高斯分布。</p>

<p>比如男性（或女性）的身高分布、等等。高斯分布的概率密度函数及钟形函数曲线如下
<script type="math/tex">f(x|\mu,\sigma^2)=\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\tag{1}</script></p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/525px-Normal_Distribution_PDF.svg.png" alt="" /></p>

<h3 id="帕累托分布">帕累托分布</h3>

<p>高斯分布的本质是独立性，而对于以社会性为本质的人类，某些属性在个体之间的独立性是不存在的。</p>

<p>所以在人类群体中，出现最多的往往是帕累托分布，比如个人财富、期刊引用量、社交媒体上的KOL，图书销售情况等。</p>

<p>帕累托分布，也称幂律分布，由 <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/38px-Flag_of_Italy.svg.png" alt="" /> 意大利人Vilfredo Pareto于19世纪末提出。</p>

<p>其概率密度函数及函数曲线（J型）如下，$x_m$是$x$所能取到的最小值
<script type="math/tex">f(x|\alpha)=\frac {\alpha x_m^\alpha}{x^{\alpha +1}}\tag{2}</script></p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Probability_density_function_of_Pareto_distribution.svg/488px-Probability_density_function_of_Pareto_distribution.svg.png" alt="" /></p>

<p>对于帕累托分布的出现，美国社会学家 Robert Merton 于1968年提出了马太效应 Matthew Effect进行解释。</p>

<p>Robert认为，在正反馈机制作用下，个体所获得的初始优势会不断滚雪球，最终强者愈强，弱者愈弱，出现帕累托分布。</p>

<p>马太效应的名称源自圣经新约。</p>

<blockquote>
  <p>For to every one who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away.</p>

  <p>—Matthew 25:29</p>

  <p>因为凡有的，还要加给他，叫他有余；没有的，连他所有的也要夺过来。</p>

  <p>—马太福音 25:29</p>
</blockquote>

<h3 id="结合lingam中的非高斯性">结合LiNGAM中的非高斯性?</h3>

<p>LiNGAM中的非高斯性指的是变量还是噪声？体现在哪里。</p>

<h3 id="路径依赖-vs-马尔科夫链">路径依赖 v.s. 马尔科夫链</h3>

<p>以社畜の作息为例，我们有三个变量 $S$: sleep early，$G$: get up early，$A$: arrive at office on time。</p>

<p>马尔科夫链如下
<script type="math/tex">S\rightarrow G\rightarrow A</script></p>

<p>虽然$A$的状态是由$G$直接决定的，但从路径依赖的考虑，从$S$开始干预所付出的成本是最小的，更容易达成目标。</p>

<p>如果说做不到从$S$开始干预，也只能从$G$下手了。</p>
</p>
      -->
      <span class="post-meta">Apr 24, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/03/28/Logistic-Regression.html">Logistic Regression</a></h2>
      <!--
      <p><h1 id="logistic-regression">Logistic Regression</h1>

<blockquote>
  <p>Updated in March 06, 2019</p>
</blockquote>

<p>逻辑回归（LR）是机器学习中的经典分类方法。</p>

<p>简单理解就是在线性回归+sigmoid激活函数，从而把分类问题转变为回归问题。</p>

<h2 id="1-logistic-回归模型">1 Logistic 回归模型</h2>

<p>首先介绍Logistic 函数，由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/38px-Flag_of_Belgium_%28civil%29.svg.png" alt="Belgium" />比利时人 Pierre Verhulst 于1845年研究模仿人口增长的曲线时发现并命名。其数学形式如下：</p>

<script type="math/tex; mode=display">y=\frac{L}{1+e^{-k(x-x_0)}}</script>

<p>机器学习中常用的激活函数 sigmoid 函数（Sigmoid 意为S型），一般特指 logistic 函数的简化形式。其数学形式及函数曲线如下，值域为 $(0, 1)$。</p>

<script type="math/tex; mode=display">y=\frac{1}{1+e^{-x}}\tag{1}</script>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png" alt="sigmod函数曲线" /></p>

<p>进入正题，Logistic 回归模型的数学形式如下：</p>

<script type="math/tex; mode=display">p(y=1|x,\theta)=\frac{1}{1+e^{-\theta^Tx}}\tag{2}</script>

<p>Logistic 回归的输出是一个概率值，根据此概率值与阈值 $μ$ 的大小进行分类，$μ$ 一般取值0.5。</p>

<p>
若 $p(y=1|x,\theta)&gt;μ$，则 $y^*=1$。函数具有对称性质，$p(y=0|x,\theta)=1-p(y=1|x,\theta)$。
</p>

<p>因为线性回归残差服从高斯分布，所以可以使用最小二乘法进行参数求解；而 Logistic 回归的因变量、残差均为二项分布，不满足正态性，所以使用MLE为目标函数来进行参数$\theta$的求解。</p>

<h2 id="2-最大似然估计mle">2 最大似然估计（MLE）</h2>

<p>最大似然估计Maximum Likelihood Estimation，是一个用来估计概率模型参数的方法，由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/38px-Flag_of_the_United_Kingdom.svg.png" alt="Britain" />英国人 Ronald Fisher 于上世纪20年代推广。MLE的两个条件是已知数据和概率分布假设（似然函数），其思想是求得这个分布具有什么样的参数值才能使我们观测到这组数据的可能性最大。</p>

<p>假设有一组独立同分布(i.i.d.)的随机变量 $X={x_1,x_2,\cdots,x_n}$，给定一个概率分布 $D$，假设其概率密度函数为 $f$，那么通过参数为 $\theta$ 的模型 $f$ 产生上面这组样本的概率为:</p>

<script type="math/tex; mode=display">f(x_1,x_2,\cdots,x_n|\theta)=\prod_{i=1}^n f(x_i|\theta)</script>

<p>MLE寻找使得这组样本出现的概率最大的参数 $\theta​$。也就是根据样本估计参数$\theta​$，定义<strong>似然函数</strong>为：</p>

<script type="math/tex; mode=display">L(\theta|x_1,x_2,\cdots,x_n)=f(x_1,x_2,\cdots,x_n|\theta)=\prod_{i=1}^n f(x_i|\theta)</script>

<p>取对数后，<strong>对数似然函数</strong>如下：</p>

<script type="math/tex; mode=display">l(\theta)=\sum_{i=1}^n \ln f(x_i|\theta)\tag{3}</script>

<p>最大似然估计即最大化对数似然函数 $l(\theta)$，$\theta^*=\mathop{\arg \max}_\theta l(\theta)$。</p>

<h2 id="3-损失函数">3 损失函数</h2>

<p>那么具体到 Logistic 回归，样本标签 $y_i$ 取值为0或1，预测概率值为 $h_{\theta}(x)=\cfrac{1}{1+e^{-\theta^Tx}}$。</p>

<p>则似然函数为如下形式：</p>

<script type="math/tex; mode=display">L(\theta|x_1,x_2,\cdots,x_n)=\prod_{i=1}^n h(x_i)^{y_i}(1-h(x_i))^{1-y_i}</script>

<p>取对数后，<strong>对数似然函数</strong>为：</p>

<script type="math/tex; mode=display">l(\theta)= \sum_{i=1}^n [y_i\ln h(x_i)+(1-y_i)\ln (1-h(x_i))]\tag{4}</script>

<p>在不引入正则项的情况下，Logistic 回归的<strong>目标函数</strong>是最小化负的平均对数似然函数，如下：</p>

<script type="math/tex; mode=display">J(\theta)=-\frac{1}{n}l(\theta)</script>

<p>由于多元变量很难求得解析解 $\theta^<em>=\mathop{\arg \min}_\theta J(\theta)$，一般使用梯度下降法逼近 $\theta^</em>$ 的最优值。</p>

<h2 id="4-梯度下降法">4 梯度下降法</h2>

<p>梯度下降法 Gradient Descent 是最优化算法的一种，据说由<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/38px-Flag_of_France.svg.png" alt="France" />法国人 Augustin Cauchy 于1847年提出。从函数当前点对应梯度的反方向按一定的步长$\alpha$进行迭代搜索，寻找函数$J(\theta)$的一个局部极小值点。若目标函数为凸函数，则得到的一定是全局最小值点。</p>

<p>GD 算法如下：</p>

<p>1）随机指定初始值 $\theta^k​$，$k=0​$</p>

<p>2) 以全部数据为样本计算当前点 $\theta^k$ 的梯度，即对每个 $\theta_i$ 求偏导</p>

<script type="math/tex; mode=display">g^k_i=\frac{\partial J(\theta)}{\partial\theta_i}\tag{5}</script>

<p>3) 更新$\theta$值，迭代公式如下</p>

<script type="math/tex; mode=display">\theta^{k+1}_i=\theta^k_i-\alpha g^k_i\tag{6}</script>

<p>4) 若函数 $J(\theta)$ 未收敛到极小值点，则令 $k=k+1$，重复步骤2、3；若已收敛，则停止迭代。</p>

<h2 id="5-调包淆">5 调包淆</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="k">def</span> <span class="nf">LR_demo</span><span class="p">():</span>
    <span class="c"># load datasets</span>
    <span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
	<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>  <span class="c"># use the first two features</span>
	<span class="n">Y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c"># fit model</span>
	<span class="n">lr_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e3</span><span class="p">)</span>
	<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    
    <span class="c"># predict</span>
	<span class="n">acc</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">)</span>
	<span class="n">prepro</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">acc</span><span class="p">,</span> <span class="n">prepro</span>
</code></pre></div></div>

<h2 id="reference">Reference</h2>

<p>[1] 李航 (2012) 统计学习方法. 清华大学出版社, 北京.</p>

<p>[2] <a href="https://tech.meituan.com/intro_to_logistic_regression.html">美团点评技术团队 logistic regression</a></p>

<p>[3] <a href="https://en.wikipedia.org/wiki/Logistic_function">Wikipedia Logistic function</a></p>

<p>[4] <a href="https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A">Wikipedia 最尤推定</a></p>

<p>[5] <a href="https://blog.csdn.net/bitcarmanlee/article/details/52201858">最大似然估计与贝叶斯估计</a></p>

<p>[6] <a href="http://www.hanlongfei.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/07/29/gradient/">梯度下降算法 理论基础</a></p>

<p>[7] <a href="https://ctmakro.github.io/site/on_learning/gd.html">梯度下降算法 python实现</a></p>

<p>[8] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">scikit-learn Logistic Regression</a></p>

</p>
      -->
      <span class="post-meta">Mar 28, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/03/15/mokuyoubi.html">木曜日</a></h2>
      <!--
      <p><h1 id="木曜日">木曜日</h1>

</p>
      -->
      <span class="post-meta">Mar 15, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/03/15/Hiragana.html">平仮名</a></h2>
      <!--
      <p><h1 id="平仮名">平仮名</h1>

</p>
      -->
      <span class="post-meta">Mar 15, 2018</span>
    </article>
    <hr />
  
    <article class="post">
      <h2><a href="/2018/03/15/1984.html">1984</a></h2>
      <!--
      <p><h1 id="down-with-big-brother">Down with Big Brother</h1>

</p>
      -->
      <span class="post-meta">Mar 15, 2018</span>
    </article>
    <hr />
  
</div>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Amoko/amoko.github.io">amoko.github.io</a> is maintained by <a href="https://github.com/Amoko">Amoko</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
