<!DOCTYPE html>
<html lang="en-US">

<head>

    <meta charset="UTF-8">
    <link rel="icon" type="image/png"  href="/img/favicon.jinja.png">
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Logistic Regression via Association Rule | Yonji’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Logistic Regression via Association Rule" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Model selection for LR via AR Changpetch P, Lin DK. Model selection for logistic regression via association rules analysis. Journal of Statistical Computation and Simulation. 2013 Aug 1;83(8):1415-28. 一句话介绍，为了逻辑回归有更好的结果，使用关联规则产生新的特征以生成一个最优模型。 Motivation 某些变量间存在相互作用(interaction)，需要将变量和变量之间的相互作用同时加入LR模型。 使用关联规则来探索变量间的相互作用。 算法 产生关联规则 CBA算法，mini_support = 0.1，mini_confidence = 0.8 rule example: $X_1=0 \&amp; X_2=1\rightarrow Y=0$ ​ 规则筛选 选择30~50个置信度最高的规则。 ​ 产生特征 将关联规则后项去除，即为新的特征。 feature example: $X_1(0)X_2(1)$ ​ 模型搜索 根据上面产生的特征生成最终的模型，使用AIC(Akaike information criterion; Akaike 1974)作为模型选择的损失函数进行搜索。 或者使用lasso(least absolute shrinkage and selection operator; Tibshirani 1996)、SCAD(smoothly clipped absolute deviation; Fan 1997)、BIC(Bayesian information criterion; Schwarz 1978)等方法。 实验 论文所使用的数据集是CMU于1991年提出的MONK数据集，二分类问题，每个样本具有6个离散值的属性，数据集共包含432个样本，即包含所有可能出现的组合情况（$3\times3\times2\times3\times4\times2=432$）。 attribute values head_shape {round, square, octagon} body_shape {round, square, octagon} is_smiling {yes, no} holding {sword, balloon, flag} jacket_color {red, yellow, green, blue} has_tie {yes, no} MONK数据集由独立的三个数据集构成，每个数据集的标签分类规则不同。论文仅使用第一个数据集monks-1，此数据集标签按照如下标准划分： For a sample, if head_shape = body_shape or jacket_color = red, it’s in Class 1; otherwise it’s in Class 0. 测试集即为有标签的全部432个样本，训练集为从中随机选取的124个样本，无噪声。 这篇论文对离散变量的编码方式有点奇怪。 以上数据集为例，其使用11个二值变量编码6个原始变量。 若目的是将多值离散变量进行二值化，直接使用 One-Hot 编码也仅需要17个编码位，为什么不直接使用One-Hot？ attribute variables {values} head_shape $X_1,X_2$ {1*, 01, 00} body_shape $X_3,X_4$ {1*, 01, 00} is_smiling $X_5$ {1, 0} holding $X_6,X_7$ {1*, 01 , 00} jacket_color $X_8,X_9,X_{10}$ {1**, 01*, 001, 000} has_tie $X_{11}$ {1, 0} 实验结果 模型 复现结果 # LR.AR model 4 line = np.array([e[0]^e[3], e[1]^e[4], e[2]^e[5], e[11]]) Reference [1] UCI上MONK数据集主页 [2] scikit-learn OneHotEncoder [2] Yonji’ Blog Logistic Regression" />
<meta property="og:description" content="Model selection for LR via AR Changpetch P, Lin DK. Model selection for logistic regression via association rules analysis. Journal of Statistical Computation and Simulation. 2013 Aug 1;83(8):1415-28. 一句话介绍，为了逻辑回归有更好的结果，使用关联规则产生新的特征以生成一个最优模型。 Motivation 某些变量间存在相互作用(interaction)，需要将变量和变量之间的相互作用同时加入LR模型。 使用关联规则来探索变量间的相互作用。 算法 产生关联规则 CBA算法，mini_support = 0.1，mini_confidence = 0.8 rule example: $X_1=0 \&amp; X_2=1\rightarrow Y=0$ ​ 规则筛选 选择30~50个置信度最高的规则。 ​ 产生特征 将关联规则后项去除，即为新的特征。 feature example: $X_1(0)X_2(1)$ ​ 模型搜索 根据上面产生的特征生成最终的模型，使用AIC(Akaike information criterion; Akaike 1974)作为模型选择的损失函数进行搜索。 或者使用lasso(least absolute shrinkage and selection operator; Tibshirani 1996)、SCAD(smoothly clipped absolute deviation; Fan 1997)、BIC(Bayesian information criterion; Schwarz 1978)等方法。 实验 论文所使用的数据集是CMU于1991年提出的MONK数据集，二分类问题，每个样本具有6个离散值的属性，数据集共包含432个样本，即包含所有可能出现的组合情况（$3\times3\times2\times3\times4\times2=432$）。 attribute values head_shape {round, square, octagon} body_shape {round, square, octagon} is_smiling {yes, no} holding {sword, balloon, flag} jacket_color {red, yellow, green, blue} has_tie {yes, no} MONK数据集由独立的三个数据集构成，每个数据集的标签分类规则不同。论文仅使用第一个数据集monks-1，此数据集标签按照如下标准划分： For a sample, if head_shape = body_shape or jacket_color = red, it’s in Class 1; otherwise it’s in Class 0. 测试集即为有标签的全部432个样本，训练集为从中随机选取的124个样本，无噪声。 这篇论文对离散变量的编码方式有点奇怪。 以上数据集为例，其使用11个二值变量编码6个原始变量。 若目的是将多值离散变量进行二值化，直接使用 One-Hot 编码也仅需要17个编码位，为什么不直接使用One-Hot？ attribute variables {values} head_shape $X_1,X_2$ {1*, 01, 00} body_shape $X_3,X_4$ {1*, 01, 00} is_smiling $X_5$ {1, 0} holding $X_6,X_7$ {1*, 01 , 00} jacket_color $X_8,X_9,X_{10}$ {1**, 01*, 001, 000} has_tie $X_{11}$ {1, 0} 实验结果 模型 复现结果 # LR.AR model 4 line = np.array([e[0]^e[3], e[1]^e[4], e[2]^e[5], e[11]]) Reference [1] UCI上MONK数据集主页 [2] scikit-learn OneHotEncoder [2] Yonji’ Blog Logistic Regression" />
<link rel="canonical" href="http://localhost:4000/2018/04/27/LR-AR.html" />
<meta property="og:url" content="http://localhost:4000/2018/04/27/LR-AR.html" />
<meta property="og:site_name" content="Yonji’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-27T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"Model selection for LR via AR Changpetch P, Lin DK. Model selection for logistic regression via association rules analysis. Journal of Statistical Computation and Simulation. 2013 Aug 1;83(8):1415-28. 一句话介绍，为了逻辑回归有更好的结果，使用关联规则产生新的特征以生成一个最优模型。 Motivation 某些变量间存在相互作用(interaction)，需要将变量和变量之间的相互作用同时加入LR模型。 使用关联规则来探索变量间的相互作用。 算法 产生关联规则 CBA算法，mini_support = 0.1，mini_confidence = 0.8 rule example: $X_1=0 \\&amp; X_2=1\\rightarrow Y=0$ ​ 规则筛选 选择30~50个置信度最高的规则。 ​ 产生特征 将关联规则后项去除，即为新的特征。 feature example: $X_1(0)X_2(1)$ ​ 模型搜索 根据上面产生的特征生成最终的模型，使用AIC(Akaike information criterion; Akaike 1974)作为模型选择的损失函数进行搜索。 或者使用lasso(least absolute shrinkage and selection operator; Tibshirani 1996)、SCAD(smoothly clipped absolute deviation; Fan 1997)、BIC(Bayesian information criterion; Schwarz 1978)等方法。 实验 论文所使用的数据集是CMU于1991年提出的MONK数据集，二分类问题，每个样本具有6个离散值的属性，数据集共包含432个样本，即包含所有可能出现的组合情况（$3\\times3\\times2\\times3\\times4\\times2=432$）。 attribute values head_shape {round, square, octagon} body_shape {round, square, octagon} is_smiling {yes, no} holding {sword, balloon, flag} jacket_color {red, yellow, green, blue} has_tie {yes, no} MONK数据集由独立的三个数据集构成，每个数据集的标签分类规则不同。论文仅使用第一个数据集monks-1，此数据集标签按照如下标准划分： For a sample, if head_shape = body_shape or jacket_color = red, it’s in Class 1; otherwise it’s in Class 0. 测试集即为有标签的全部432个样本，训练集为从中随机选取的124个样本，无噪声。 这篇论文对离散变量的编码方式有点奇怪。 以上数据集为例，其使用11个二值变量编码6个原始变量。 若目的是将多值离散变量进行二值化，直接使用 One-Hot 编码也仅需要17个编码位，为什么不直接使用One-Hot？ attribute variables {values} head_shape $X_1,X_2$ {1*, 01, 00} body_shape $X_3,X_4$ {1*, 01, 00} is_smiling $X_5$ {1, 0} holding $X_6,X_7$ {1*, 01 , 00} jacket_color $X_8,X_9,X_{10}$ {1**, 01*, 001, 000} has_tie $X_{11}$ {1, 0} 实验结果 模型 复现结果 # LR.AR model 4 line = np.array([e[0]^e[3], e[1]^e[4], e[2]^e[5], e[11]]) Reference [1] UCI上MONK数据集主页 [2] scikit-learn OneHotEncoder [2] Yonji’ Blog Logistic Regression","@type":"BlogPosting","url":"http://localhost:4000/2018/04/27/LR-AR.html","headline":"Logistic Regression via Association Rule","dateModified":"2018-04-27T00:00:00-07:00","datePublished":"2018-04-27T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/04/27/LR-AR.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=b0ca346b5b38b55e1001eb0920382c9d2e5aa454">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
</head>


  <body>
    <section class="page-header">
      <h1 class="project-name">Yonji's Blog|
      <a href="http://localhost:4000">Home</a></h1>
      <!--
      <h2 class="project-tagline">OMG! They killed Kenny!</h2>
      -->
      <a href="https://github.com/Amoko/amoko.github.io" class="btn">View on GitHub</a>
    
      
    </section>

    <section class="main-content">
      <h1 id="model-selection-for-lr-via-ar">Model selection for LR via AR</h1>

<p>Changpetch P, Lin DK. Model selection for logistic regression via association rules analysis. Journal of Statistical Computation and Simulation. 2013 Aug 1;83(8):1415-28.</p>

<p>一句话介绍，为了逻辑回归有更好的结果，使用关联规则产生新的特征以生成一个最优模型。</p>

<h3 id="motivation">Motivation</h3>

<p>某些变量间存在相互作用(interaction)，需要将变量和变量之间的相互作用同时加入LR模型。</p>

<p>使用关联规则来探索变量间的相互作用。</p>

<h3 id="算法">算法</h3>

<ol>
  <li>
    <p><strong>产生关联规则</strong></p>

    <p>CBA算法，mini_support = 0.1，mini_confidence = 0.8</p>

    <p>rule example: $X_1=0 \&amp; X_2=1\rightarrow Y=0$</p>

    <p>​</p>
  </li>
  <li>
    <p><strong>规则筛选</strong></p>

    <p>选择30~50个置信度最高的规则。</p>

    <p>​</p>
  </li>
  <li>
    <p><strong>产生特征</strong></p>

    <p>将关联规则后项去除，即为新的特征。</p>

    <p>feature example: $X_1(0)X_2(1)$</p>

    <p>​</p>
  </li>
  <li>
    <p><strong>模型搜索</strong></p>

    <p>根据上面产生的特征生成最终的模型，使用AIC(Akaike information criterion; Akaike 1974)作为模型选择的损失函数进行搜索。</p>

    <p>或者使用lasso(least absolute shrinkage and selection operator; Tibshirani 1996)、SCAD(smoothly clipped absolute deviation; Fan 1997)、BIC(Bayesian information criterion; Schwarz 1978)等方法。</p>
  </li>
</ol>

<h3 id="实验">实验</h3>

<p>论文所使用的数据集是CMU于1991年提出的MONK数据集，二分类问题，每个样本具有6个离散值的属性，数据集共包含432个样本，即包含所有可能出现的组合情况（$3\times3\times2\times3\times4\times2=432$）。</p>

<table>
  <thead>
    <tr>
      <th>attribute</th>
      <th>values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>head_shape</td>
      <td>{round, square, octagon}</td>
    </tr>
    <tr>
      <td>body_shape</td>
      <td>{round, square, octagon}</td>
    </tr>
    <tr>
      <td>is_smiling</td>
      <td>{yes, no}</td>
    </tr>
    <tr>
      <td>holding</td>
      <td>{sword, balloon, flag}</td>
    </tr>
    <tr>
      <td>jacket_color</td>
      <td>{red, yellow, green, blue}</td>
    </tr>
    <tr>
      <td>has_tie</td>
      <td>{yes, no}</td>
    </tr>
  </tbody>
</table>

<p>MONK数据集由独立的三个数据集构成，每个数据集的标签分类规则不同。论文仅使用第一个数据集monks-1，此数据集标签按照如下标准划分：</p>

<p>For a sample, if <em>head_shape = body_shape</em> <strong>or</strong> <em>jacket_color = red</em>,  it’s in Class 1; otherwise it’s in Class 0.</p>

<p>测试集即为有标签的全部432个样本，训练集为从中随机选取的124个样本，无噪声。</p>

<p><strong>这篇论文对离散变量的编码方式有点奇怪。</strong></p>

<p>以上数据集为例，其使用11个二值变量编码6个原始变量。</p>

<p>若目的是将多值离散变量进行二值化，直接使用 One-Hot 编码也仅需要17个编码位，为什么不直接使用One-Hot？</p>

<table>
  <thead>
    <tr>
      <th>attribute</th>
      <th>variables {values}</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>head_shape</td>
      <td>$X_1,X_2$ {1*, 01, 00}</td>
    </tr>
    <tr>
      <td>body_shape</td>
      <td>$X_3,X_4$ {1*, 01, 00}</td>
    </tr>
    <tr>
      <td>is_smiling</td>
      <td>$X_5$ {1, 0}</td>
    </tr>
    <tr>
      <td>holding</td>
      <td>$X_6,X_7$ {1*, 01 , 00}</td>
    </tr>
    <tr>
      <td>jacket_color</td>
      <td>$X_8,X_9,X_{10}$ {1**, 01*, 001, 000}</td>
    </tr>
    <tr>
      <td>has_tie</td>
      <td>$X_{11}$ {1, 0}</td>
    </tr>
  </tbody>
</table>

<h3 id="实验结果">实验结果</h3>

<p>模型</p>

<p><img src="/img/monk.model.PNG" alt="模型" /></p>

<p>复现结果</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># LR.AR model 4</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">^</span><span class="n">e</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="n">e</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="n">e</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">e</span><span class="p">[</span><span class="mi">11</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/img/monk.result.PNG" alt="复现结果" /></p>

<h3 id="reference">Reference</h3>

<p>[1] <a href="https://archive.ics.uci.edu/ml/datasets/MONK's+Problems">UCI上MONK数据集主页</a></p>

<p>[2] <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">scikit-learn OneHotEncoder</a></p>

<p>[2] <a href="https://amoko.github.io/2018/03/28/Logistic-Regression.html">Yonji’ Blog Logistic Regression</a></p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Amoko/amoko.github.io">amoko.github.io</a> is maintained by <a href="https://github.com/Amoko">Amoko</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
