<!DOCTYPE html>
<html lang="en-US">

<head>

    <meta charset="UTF-8">
    <link rel="icon" type="image/png"  href="/img/favicon.jinja.png">
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>PyTorch Notes | Yonji’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="PyTorch Notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="PyTorch Notes 一些PyTorch笔记 ，Updated on Mar 22，2019 1 CrossEntropyLoss &amp; Softmax 因为PyTorch在CrossEntropyLoss损失函数中整合了Softmax激活函数，所以对于多分类神经网络，最后一层不需要添加激活函数，只要设定神经元个数为类别个数即可。 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(3 * 32 * 32, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) # the last layer needs no activation function x = self.fc3(x) return x criterion = nn.CrossEntropyLoss() net = Net() outputs = net(inputs) # calculate loss loss = criterion(outputs, labels) # predict predicted = torch.max(outputs.data, 1)[1] 2 数据归一化 from torchvision import transforms transform = transforms.Compose( [transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]) PyTorch在数据预处理时使用参数 transform 来定义对数据的归一化方式，将原始值域为 $[0,255]$ 的numpy数据矩阵转换为值域为 $[0,1]$ 的Tensor。 这里面有两个操作，第一个transforms.ToTensor()是 Min-Max 归一化； 第二个transforms.Normalize()则是 Z-score 归一化，两组参数均值与标准差需要计算后赋值，因为定义的是单通道图像所以均值与标准差各只有1个。 参见下面这段代码： img = np.array([[0, 32], [128, 255]], np.uint8) plt.imshow(img, cmap=&#39;gray&#39;) plt.show() # x&#39; = x / 255 tensor = transforms.functional.to_tensor(img.reshape(2,2,1)) print(tensor) &#39;&#39;&#39; tensor([[[ 0.0000, 0.1255], [ 0.5020, 1.0000]]]) &#39;&#39;&#39; # x&#39; = (x - mean) / std tensor = transforms.functional.normalize(tensor, (0.407,), (0.389,)) print(tensor) &#39;&#39;&#39; tensor([[[-1.0463, -0.7237], [ 0.2441, 1.5244]]]) &#39;&#39;&#39; 3 参数dim PyTorch中的参数dim，就是NumPy中的参数axis，参考下面两个函数定义： # pytorch torch.max(input, dim, keepdim=False, out=None) torch.squeeze(input, dim=None, out=None) # numpy numpy.max(a, axis=None, out=None) numpy.squeeze(a, axis=None) 对于二维矩阵，dim = 0 则以行（0维度）为轴，对各列进行“压缩”；dim = 1 则以列（1维度）为轴对各行进行“压缩”。参见下面这段代码： &gt;&gt;&gt; a = torch.randint(0, 10, (2, 3)) &gt;&gt;&gt; a tensor([[ 5., 2., 9.], [ 7., 3., 2.]]) &gt;&gt;&gt; b = torch.max(a, 0) &gt;&gt;&gt; b (tensor([ 7., 3., 9.]), tensor([ 1, 1, 0])) &gt;&gt;&gt; c = torch.max(a, 1) &gt;&gt;&gt; c (tensor([ 9., 7.]), tensor([ 2, 0])) 4 TBC 人は運命にはさからえませんから。" />
<meta property="og:description" content="PyTorch Notes 一些PyTorch笔记 ，Updated on Mar 22，2019 1 CrossEntropyLoss &amp; Softmax 因为PyTorch在CrossEntropyLoss损失函数中整合了Softmax激活函数，所以对于多分类神经网络，最后一层不需要添加激活函数，只要设定神经元个数为类别个数即可。 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(3 * 32 * 32, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) # the last layer needs no activation function x = self.fc3(x) return x criterion = nn.CrossEntropyLoss() net = Net() outputs = net(inputs) # calculate loss loss = criterion(outputs, labels) # predict predicted = torch.max(outputs.data, 1)[1] 2 数据归一化 from torchvision import transforms transform = transforms.Compose( [transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]) PyTorch在数据预处理时使用参数 transform 来定义对数据的归一化方式，将原始值域为 $[0,255]$ 的numpy数据矩阵转换为值域为 $[0,1]$ 的Tensor。 这里面有两个操作，第一个transforms.ToTensor()是 Min-Max 归一化； 第二个transforms.Normalize()则是 Z-score 归一化，两组参数均值与标准差需要计算后赋值，因为定义的是单通道图像所以均值与标准差各只有1个。 参见下面这段代码： img = np.array([[0, 32], [128, 255]], np.uint8) plt.imshow(img, cmap=&#39;gray&#39;) plt.show() # x&#39; = x / 255 tensor = transforms.functional.to_tensor(img.reshape(2,2,1)) print(tensor) &#39;&#39;&#39; tensor([[[ 0.0000, 0.1255], [ 0.5020, 1.0000]]]) &#39;&#39;&#39; # x&#39; = (x - mean) / std tensor = transforms.functional.normalize(tensor, (0.407,), (0.389,)) print(tensor) &#39;&#39;&#39; tensor([[[-1.0463, -0.7237], [ 0.2441, 1.5244]]]) &#39;&#39;&#39; 3 参数dim PyTorch中的参数dim，就是NumPy中的参数axis，参考下面两个函数定义： # pytorch torch.max(input, dim, keepdim=False, out=None) torch.squeeze(input, dim=None, out=None) # numpy numpy.max(a, axis=None, out=None) numpy.squeeze(a, axis=None) 对于二维矩阵，dim = 0 则以行（0维度）为轴，对各列进行“压缩”；dim = 1 则以列（1维度）为轴对各行进行“压缩”。参见下面这段代码： &gt;&gt;&gt; a = torch.randint(0, 10, (2, 3)) &gt;&gt;&gt; a tensor([[ 5., 2., 9.], [ 7., 3., 2.]]) &gt;&gt;&gt; b = torch.max(a, 0) &gt;&gt;&gt; b (tensor([ 7., 3., 9.]), tensor([ 1, 1, 0])) &gt;&gt;&gt; c = torch.max(a, 1) &gt;&gt;&gt; c (tensor([ 9., 7.]), tensor([ 2, 0])) 4 TBC 人は運命にはさからえませんから。" />
<link rel="canonical" href="http://localhost:4000/2018/08/01/PyTorch-Notes.html" />
<meta property="og:url" content="http://localhost:4000/2018/08/01/PyTorch-Notes.html" />
<meta property="og:site_name" content="Yonji’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-01T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"PyTorch Notes 一些PyTorch笔记 ，Updated on Mar 22，2019 1 CrossEntropyLoss &amp; Softmax 因为PyTorch在CrossEntropyLoss损失函数中整合了Softmax激活函数，所以对于多分类神经网络，最后一层不需要添加激活函数，只要设定神经元个数为类别个数即可。 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(3 * 32 * 32, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) # the last layer needs no activation function x = self.fc3(x) return x criterion = nn.CrossEntropyLoss() net = Net() outputs = net(inputs) # calculate loss loss = criterion(outputs, labels) # predict predicted = torch.max(outputs.data, 1)[1] 2 数据归一化 from torchvision import transforms transform = transforms.Compose( [transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]) PyTorch在数据预处理时使用参数 transform 来定义对数据的归一化方式，将原始值域为 $[0,255]$ 的numpy数据矩阵转换为值域为 $[0,1]$ 的Tensor。 这里面有两个操作，第一个transforms.ToTensor()是 Min-Max 归一化； 第二个transforms.Normalize()则是 Z-score 归一化，两组参数均值与标准差需要计算后赋值，因为定义的是单通道图像所以均值与标准差各只有1个。 参见下面这段代码： img = np.array([[0, 32], [128, 255]], np.uint8) plt.imshow(img, cmap=&#39;gray&#39;) plt.show() # x&#39; = x / 255 tensor = transforms.functional.to_tensor(img.reshape(2,2,1)) print(tensor) &#39;&#39;&#39; tensor([[[ 0.0000, 0.1255], [ 0.5020, 1.0000]]]) &#39;&#39;&#39; # x&#39; = (x - mean) / std tensor = transforms.functional.normalize(tensor, (0.407,), (0.389,)) print(tensor) &#39;&#39;&#39; tensor([[[-1.0463, -0.7237], [ 0.2441, 1.5244]]]) &#39;&#39;&#39; 3 参数dim PyTorch中的参数dim，就是NumPy中的参数axis，参考下面两个函数定义： # pytorch torch.max(input, dim, keepdim=False, out=None) torch.squeeze(input, dim=None, out=None) # numpy numpy.max(a, axis=None, out=None) numpy.squeeze(a, axis=None) 对于二维矩阵，dim = 0 则以行（0维度）为轴，对各列进行“压缩”；dim = 1 则以列（1维度）为轴对各行进行“压缩”。参见下面这段代码： &gt;&gt;&gt; a = torch.randint(0, 10, (2, 3)) &gt;&gt;&gt; a tensor([[ 5., 2., 9.], [ 7., 3., 2.]]) &gt;&gt;&gt; b = torch.max(a, 0) &gt;&gt;&gt; b (tensor([ 7., 3., 9.]), tensor([ 1, 1, 0])) &gt;&gt;&gt; c = torch.max(a, 1) &gt;&gt;&gt; c (tensor([ 9., 7.]), tensor([ 2, 0])) 4 TBC 人は運命にはさからえませんから。","@type":"BlogPosting","url":"http://localhost:4000/2018/08/01/PyTorch-Notes.html","headline":"PyTorch Notes","dateModified":"2018-08-01T00:00:00-07:00","datePublished":"2018-08-01T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/08/01/PyTorch-Notes.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=b0ca346b5b38b55e1001eb0920382c9d2e5aa454">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
</head>


  <body>
    <section class="page-header">
      <h1 class="project-name">Yonji's Blog|
      <a href="http://localhost:4000">Home</a></h1>
      <!--
      <h2 class="project-tagline">OMG! They killed Kenny!</h2>
      -->
      <a href="https://github.com/Amoko/amoko.github.io" class="btn">View on GitHub</a>
    
      
    </section>

    <section class="main-content">
      <h1 id="pytorch-notes">PyTorch Notes</h1>

<blockquote>
  <p>一些PyTorch笔记 ，Updated on Mar 22，2019</p>
</blockquote>

<h2 id="1-crossentropyloss--softmax">1 CrossEntropyLoss &amp; Softmax</h2>

<p>因为PyTorch在CrossEntropyLoss损失函数中整合了Softmax激活函数，所以对于多分类神经网络，最后一层不需要添加激活函数，只要设定神经元个数为类别个数即可。</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c"># the last layer needs no activation function</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
        
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c"># calculate loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="c"># predict</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="2-数据归一化">2 数据归一化</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
            <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])</span>
</code></pre></div></div>

<p>PyTorch在数据预处理时使用参数 transform 来定义对数据的归一化方式，将原始值域为 $[0,255]$ 的numpy数据矩阵转换为值域为 $[0,1]$ 的Tensor。</p>

<p>这里面有两个操作，第一个<code>transforms.ToTensor()</code>是 Min-Max 归一化；</p>

<p>第二个<code>transforms.Normalize()</code>则是 Z-score 归一化，两组参数均值与标准差需要计算后赋值，因为定义的是单通道图像所以均值与标准差各只有1个。</p>

<p>参见下面这段代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">255</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c"># x' = x / 255</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="s">'''
tensor([[[ 0.0000,  0.1255],
         [ 0.5020,  1.0000]]])
'''</span>
<span class="c"># x' = (x - mean) / std</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.407</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.389</span><span class="p">,))</span> 
<span class="k">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="s">'''
tensor([[[-1.0463, -0.7237],
         [ 0.2441,  1.5244]]])
'''</span>
</code></pre></div></div>

<h2 id="3-参数dim">3 参数dim</h2>

<p>PyTorch中的参数dim，就是NumPy中的参数axis，参考下面两个函数定义：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pytorch</span>
<span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># numpy</span>
<span class="n">numpy</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>
<p>对于二维矩阵，dim = 0 则以行（0维度）为轴，对各列进行“压缩”；dim = 1 则以列（1维度）为轴对各行进行“压缩”。参见下面这段代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span>
<span class="p">(</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">9.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>

<h2 id="4-tbc">4 TBC</h2>

<p>人は運命にはさからえませんから。</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Amoko/amoko.github.io">amoko.github.io</a> is maintained by <a href="https://github.com/Amoko">Amoko</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
