---
title: How Batch Normalization works?
layout: default
tags: [machine learning,]
---

# How Batch Normalization works?

机器学习根据训练集得到模型在测试集上的结果通常会出现一定程度的下降，下降的多少取决于你对训练集的过拟合程度。

这是因为训练集的数据分布 $p(x)​$ 与测试集数据分布 $q(x)​$ 并不完全相同，那么根据训练集得到的参数模型 $p(y\vert x,\theta)​$ 与在测试集上所期望的的模型 $q(y\vert x)​$ 也会存在差异。对训练集进行过多的拟合导致这种差异性被无限放大。

这种由于数据集的分布差异 $p(x)\neq q(x)$，而造成的参数模型差异 $p(y\vert x,\theta) \neq q(y\vert x)$ 被 <ruby>下平<rt>しもだいら</rt></ruby> <ruby>英寿<rt>ひでとし</rt></ruby> 定义为 covariate shift，协变量偏移，并使用KL散度来衡量这种分布差异。

如何最大限度地消除不同数据集之间的差异是 domain adaption 所要解决的问题。

$\star$ 然而，covariate shift 不只是模型面对不同数据集时会出现。

对于多层神经网络，在训练阶段参数优化时，前层网络的参数更新会造成的后层网络的输入分布变化。

Batch Normalization 这篇文章将训练神经网络时所出现的这种，不同网络层之间数据分布差异定义为 internal covariate shift。

## 1 作用机制

### 1.1 Normalization

如何处理不同网络层对数据改变所造成的 internal covariate shift？

答案是对数据做归一化，归一化即 scale + shift 两个操作的组合，一般常用的Z-score归一化是 $\hat{x}=\cfrac{x-\mu}{\sigma}$。

归一化可以减小 internal covariate shift 的影响，而且可以避免输入数据 $x​$ 进入饱和区（saturated regime）从而加速模型的收敛。

但这样做的结果是抹去了前层网络所学习到的信息。

因此为了给归一化打补丁，BN在归一化的基础上另外定义了两个可学习参数：再缩放参数 $\gamma​$ 与再平移参数 $\beta​$，来对 $\hat{x}​$ 做进一步的变换。

$$
y=\gamma \hat{x}+\beta
$$


### 1.2 Regularization

与SGD求梯度时使用子样本代替全体样本引入随机噪声类似，BN使用mini-batch代替全体样本求均值与方差同样引入了随机噪声，一定程度上也对模型起到了正则化作用。从而可以舍弃掉为了防止过拟合而采用的dropout操作。

## 2 参数更新

$\gamma\leftarrow ?$

$\beta\leftarrow ?​$

## 3 TBC

これで今夜もくつろいで熟睡できるな。

## 4 Reference

\[1] Improving predictive inference under covariate shift by weighting the log-likelihood function, JSPI2000

\[2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv2015