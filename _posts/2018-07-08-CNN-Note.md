# CNN Note

卷积神经网络（Convolutional Neural Networks），深度学习大佬![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/33px-Flag_of_France.svg.png)法国人 Yann LeCun 的大作，在图像识别领域具有统治级的优秀表现。CNN与普通神经网络的区别在于两个定制的隐层结构，卷积层和池化层。



### 卷积层（Convolution Layer）

在卷积层，一个神经元的权值就是一个卷积核的参数。卷积核本质是一个滤波（filter），因为以一个卷积核对整个图像的不同部分进行扫描，因此说图像的不同部分之间是**权值共享**的。再加上ReLU等非线性激活函数来达到提取图像高维特征的目的。



以下以 Stanford CS231n中的例子来说明卷积层的计算过程：

![](/img/conv.PNG)

输入为$5\times5\times3$的RBG三通道图像矩阵$x$。卷积层有两个神经元，其卷积核分别为$w_0$和$w_1$，都是$3\times3\times3$的滤波矩阵。（卷积核最后一个维度值和输入图片通道数相等，相当于给每个通道$5\times5$的图片，配一个$3\times3$的滤波。）

对卷积核$w_0$来说，此神经元对应位置的输出值是3个输入信道的卷积值之和。

对图像进行扫描需要额外定义的两个参数：

- padding，边缘填充，在原始图像边缘填充的个数。
- stride，卷积核在原始图像上扫描时的步长，即移动的位数。



总结：

输入图像为$W_1\times H_1\times D_1$；

定义卷积层神经元个数为$K​$，每个神经元卷积核尺寸为$F\times F\times D_1​$，填充位数padding为$P​$，步长stride为$S​$；

则此卷积层的输出维度为$W_2\times H_2\times K$，其中$W_2=(W_1-F+2P)/S+1$，是$H_2=(H_1-F+2P)/S+1$。

上例中，输入数据$W_1=H_1=5,D_1=3$，卷积层$K=2,F=3,P=1,S=2$，则最终的输出结果$W_2=H_2=(5-3+2\times1)/2+1=3$。



#### PyTorch中的卷积层

``` python
class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
```

以上是PyTorch中一个2D的卷积层定义，必须指定的参数有三个：in_channels, out_channels, kernel_size。

- in_channels，输入信道，上一层神经元的个数。若为第一层，则对于RGB三通道图像数据，这个值为3。

- out_channels，输出信道，此卷积层神经元的个数，即卷积核的数目。

- kernel_size，卷积核尺寸。



###  池化层（Pooling Layer）

池化层的作用是降维，提升CNN的泛化能力。

池化的方式有很多，常用的是 max pooling，即选择区域内最大的元素值作为对应位置的输出。

![](/img/pool.PNG)



### Reference

\[1] 李航 (2012) 统计学习方法. 清华大学出版社, 北京.


\[2] [CS231n: CNN for Visual Recognition ](http://cs231n.github.io/convolutional-networks/)
